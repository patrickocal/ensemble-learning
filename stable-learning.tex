% !TEX TS-program = pdflatexmk
%\RequirePackage[l2tabu, orthodox]{nag}
   \documentclass[12pt,a4paper,twoside]{article}
   % \documentclass[12pt]{article}
\listfiles
\usepackage{./Paper_preamble}
\usepackage{bm}
\addbibresource{stable-learning-jabref.bib}
%includeonly 
%\includeonly{./introduction.tex}

  \linespread{1.41}
%  \linespread{1}

 % \geometry{showframe=true}
   \geometry{a4paper,margin=1.25in}%,footskip=.25in}
   % left=1.25in,right=1.25in,top=1.25in,bottom=1.25in}
% \addtolength{\oddsidemargin}{-.875in}
% \addtolength{\evensidemargin}{-.875in}
% \addtolength{\textwidth}{1.75in}


%\newwatermark[allpages,color=gray!50,angle=45,scale=3,xpos=0,ypos=0]{DRAFT}

%\renewcommand\Affilfont{\itshape\small}
 \usepackage{sectsty}
% \allsectionsfont{\mdseries\itshape}
 \paragraphfont{\mdseries\scshape}
 \makeatletter
\def\@seccntformat#1{\csname the#1\endcsname.\quad}
\makeatother
 \sectionfont{\normalsize\centering\mdseries\scshape}
%\usepackage[right]{lineno}
%\nolinenumbers
%\preto{\newpage}{\resetlinenumber}

%\def\pagewiselinenumbers{\linenumbers\setpagewiselinenumbers}
\linenumbers
\modulolinenumbers[5]
\renewcommand\thelinenumber{\color{lightgray}\arabic{linenumber}}
   % \makeatletter \newsavebox{\@linebox}
   % \savebox{\@linebox}[3em][t]{\parbox[t]{3em}{%
   %     \@tempcnta\@ne\relax
   %     \loop{\scriptsize\the\@tempcnta}\\
   %     \advance\@tempcnta by \@ne\ifnum\@tempcnta<47\repeat}}
   % \usepackage{fancyhdr} \pagestyle{fancy} \fancyhead{} \fancyfoot{}1]
% \fancyhead[CO]{\scriptsize How to Count Lines}
% \fancyhead[RO,LE]{\footnotesize\thepage}
%% insert this block within a conditional
% \fancyhead[RE]{\footnotesize\thepage\begin{picture}(0,0)%
%      \put(-26,-25){\usebox{\@linebox}}%
%    \end{picture}}  
% \fancyhead[RO]{%
% \begin{picture}(0,0)%
%   \put(12,-38){\usebox{\@linebox}}%
%  \end{picture}}
% %\fancyfoot[C]{\scriptsize Draft copy}
% \makeatother
%\medmuskip=4mu plus 2mu minus 2mu
%\newcounter{nameOfYourChoice}

\makeatletter
\newcommand{\srcsize}{\@setfontsize{\srcsize}{3pt}{3pt}}
\makeatother
\makeatletter
\newcommand{\srcsizetwo}{\@setfontsize{\srcsizetwo}{2pt}{2pt}}
\makeatother
% \newtheorem{taggedaxiom}{Axiom}
% \newenvironment{taggedaxiom}[1]
%  {\renewcommand\thetaggedaxiom{#1}\taggedaxiom}
%  {\endtaggedaxiom}

\newcommand{\gsii}{$\textup{GS03}$}
\DeclareMathOperator{\bd}{Bd}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\interior}{Int}
\DeclareMathOperator{\extrema}{Extrema}
\DeclareMathOperator{\ess}{ess}
\newcommand{\relations}{\operatorname{rel}}
\newcommand{\novel}{\mathfrak f}
\newcommand*\underdot[1]{%
  \underaccent{\dot#1}}
\newcommand{\overcirc}{\accentset{\ccirc}}
\newcommand*\undercirc[1]{%
  \underaccent{\ring#1}}
\newcommand{\smallgeq}{\mbox{\larger[-4]$\geq$}}
\newcommand{\smallleq}{\mbox{\larger[-4]$\leq$}}
\newcommand{\smallstrictlygr}{\mbox{\larger[-4]$>$}}
\newcommand{\smallstrictlyless}{\mbox{\larger[-4]$<$}}
\newcommand{\smallzero}{\mbox{\larger[-5]$0$}}
\newcommand{\smallasterix}{\mbox{\larger[-4]$*$}}
\newcommand{\smallplus}{\mbox{\larger[-4]$+$}}
\newcommand{\nneg}{\smallgeq}
\newcommand{\pos}{{\mdoubleplus}}
\newcommand{\posint}{\mbb Z_{\mdoubleplus}}
\newcommand{\nnint}{{\mbb Z}_{\mplus}}
\newcommand{\posrat}{\mbb Q_{\mdoubleplus}}
\newcommand{\posreal}{\mbb R_{\mdoubleplus}}
\newcommand{\negreal}{\mbb R_{-}}
\newcommand{\nnrat}{{\mbb Q}_{\mplus}}
\newcommand{\nnreal}{{\mbb R}_{\mplus}}

\newcommand\mdoubleplus{\text{\srcsize$+\mkern-2mu+$}}
%\newcommand\mdoubleplus{\text{\srcsize$> 0$}}
\newcommand\mplus{\text{\srcsize$+$}}
%\newcommand\mplus{\text{\srcsize$\geq 0$}}
\newcommand{\lightercolor}[3]{% Reference Color, Percentage, New Color Name
    \colorlet{#3}{#1!#2!white}
}
\lightercolor{gray}{50}{lightgray}
%\newcommand\mdoubleplus{\text{\tiny$*$\srcsize$+$}}
%\newcommand\doubleplus{+\kern-1.3ex+\kern0.8ex}
%\newcommand\mdoubleplus{\text{\tiny$*$\srcsize$+$}}
%\newcommand\mdoubleplus{\text{\srcsize$*+$}}
%\newcommand\mdoubleplus{\ensuremath{\mathbin{\bm{+}}}}
%\newcommand\mdoubleplus{\ensuremath{\mathbin{\circ\mkern-12mu+}}}
%\usepackage{cancel}
\usepackage{accents}
\newcommand*{\dt}[1]{%
  \accentset{\mbox{\bfseries .}}{#1}}
%\newcommand*{\ddt}[1]{%
% \accentset{\mbox{\large\bfseries .\hspace{-0.25ex}.}}{#1}}
\usepackage{upgreek}

%\renewcommand{\smallbot}{\text{\small$\bot$}}
\newcommand{\smallbot}{\text{\small$\hat{0}$}}
% \newcommand{\upriota}{\reflectbox{$\upiota$}}
% \newcommand{\upi}{\reflectbox{$\upriota$}}








\DeclareMathOperator{\dis}{d}
\newcommand{\spann}{\operatorname{span}}
\newcommand{\reg}{\operatorname{reg}}
\newcommand{\nov}{\operatorname{nov}}
\newcommand{\test}{\operatorname{test}}
\newcommand{\proj}{\operatorname{Proj}}
\newcommand{\Ext}{\operatorname{ext}}

\newcommand{\diam}{\operatorname{Diam}}
\newcommand{\ev}{\operatorname{Prop}}%\epsilon}
\DeclareMathOperator{\chamb}{ch}
\DeclareMathOperator{\ech}{ech}
\DeclareMathOperator{\conv}{conv}

\newcommand{\hyp}{\operatorname{Hyp}}

\newcommand*{\boldsymb}[1]{%
  \textpdfrender{%
    TextRenderingMode=FillStroke,%
    LineWidth=.4pt,%
  }{#1}%
CCM}

%\newcommand{\precsimb}{\precsim}
\newcommand{\precsimb}{\mathbin{\preceq}}
\newcommand{\precb}{\mathbin{\prec}}
\newcommand{\approxb}{\mathbin{\simeq}}
\newcommand{\simb}{\mathbin{\sim}}
%\newcommand{\precsimb}{\precsim}
\newcommand{\preceqb}{\mathbin{\preceq}}
\newcommand{\simeqb}{\mathbin{\simeq}}

%\newcommand{\precsimb}{\boldsymb{\precsim\hskip-5pt}}
%\newcommand{\precb}{\boldsymb{\prec\hskip-5pt}}
%\newcommand{\approxb}{\boldsymb{\approx\hskip-5pt}}
%\newcommand{\simb}{\boldsymb{\sim\hskip-5pt}}
%\newcommand{\sext}{P}
%\newcommand{\ext}{R}
\newcommand{\countof}{\mathbin{\#}\hskip1pt}

\newcommand{\supsext}{{\sext}}

\newcommand{\extr}{\mathrel{\mc R}}
\newcommand{\sextr}{\mathrel{\mc P}}

\newcommand{\ext}{\mathrel{\mc R}}
\newcommand{\sext}{\mathrel{\mc P}}
\newcommand{\next}{\mathrel{\mc I}}
\newcommand{\supext}{{\ext}}

\newcommand{\extb}{\mathbin{\mc R}}
\newcommand{\sextb}{\mathbin{\mc P}}
\newcommand{\nextb}{\mathbin{\mc I}}

\newcommand{\nextr}{\mathrel{\mc I}}



\newcommand{\dext}{\mathrel{\tilde{\mathrel{\mathcal R}}}}
\newcommand{\dsext}{\mathrel{\tilde{\mathrel{\mathcal P}}}}
\newcommand{\dnext}{\mathrel{\tilde{\mathrel{\mathcal I}}}}

\newcommand{\dextb}{\mathbin{\tilde{\mathbin{\mathcal R}}}}
\newcommand{\dsextb}{\mathbin{\tilde{\mathbin{\mathcal P}}}}
\newcommand{\dnextb}{\mathbin{\tilde{\mathbin{\mathcal I}}}}

\newcommand{\hext}{\mathrel{\hat{\mathrel{\mathcal R}}}}
\newcommand{\hsext}{\mathrel{\hat{\mathrel{\mathcal P}}}}
\newcommand{\hnext}{\mathrel{\hat{\mathrel{\mathcal I}}}}

\newcommand{\hextb}{\mathbin{\hat{\mathbin{\mathcal R}}}}
\newcommand{\hsextb}{\mathbin{\hat{\mathbin{\mathcal P}}}}
\newcommand{\hnextb}{\mathbin{\hat{\mathbin{\mathcal I}}}}

\newcommand{\aext}{\mathrel{\acute{\mathrel{\mathcal R}}}}
\newcommand{\asext}{\mathrel{\acute{\mathrel{\mathcal P}}}}
\newcommand{\anext}{\mathrel{\acute{\mathrel{\mathcal I}}}}

\newcommand{\aextb}{\mathbin{\acute{\mathbin{\mathcal R}}}}
\newcommand{\asextb}{\mathbin{\acute{\mathbin{\mathcal P}}}}
\newcommand{\anextb}{\mathbin{\acute{\mathbin{\mathcal I}}}}

\newcommand{\gext}{\mathrel{\grave{\mathrel{\mathcal R}}}}
\newcommand{\gsext}{\mathrel{\grave{\mathrel{\mathcal P}}}}
\newcommand{\gnext}{\mathrel{\grave{\mathrel{\mathcal I}}}}

\newcommand{\gextb}{\mathbin{\grave{\mathbin{\mathcal R}}}}
\newcommand{\gsextb}{\mathbin{\grave{\mathbin{\mathcal P}}}}
\newcommand{\gnextb}{\mathbin{\grave{\mathbin{\mathcal I}}}}

\newcommand{\cext}{\mathrel{\check{\mathrel{\mathcal R}}}}
\newcommand{\csext}{\mathrel{\check{\mathrel{\mathcal P}}}}
\newcommand{\cnext}{\mathrel{\check{\mathrel{\mathcal I}}}}

\newcommand{\cextb}{\mathbin{\check{\mathbin{\mathcal R}}}}
\newcommand{\csextb}{\mathbin{\check{\mathbin{\mathcal P}}}}
\newcommand{\cnextb}{\mathbin{\check{\mathbin{\mathcal I}}}}



\newcommand{\tran}{\textup{total}}
\newcommand{\total}{\textup{total}}
\DeclareMathOperator{\ran}{\textup{rank}}
\newcommand{\intran}{\textup{intran}}
\newcommand{\preferences}{\preceqb}
\newcommand{\trelations}{{\bm{\mc T}}}
\newcommand{\jac}{\textup{jac}}
\newcommand{\rep}{\textup{rep}}

\newcommand{\unaware}{\mathbb U}
\newcommand{\ememories}{\mathbb E}
\newcommand{\icases}{\mathbb F}
\newcommand{\ccases}{\mathbb G}


\newcommand{\mbbd}{{\mathds D}}
\newcommand{\mbbdp}{{\mathds D^{\novel}}}
\newcommand{\dpp}{{\mathfrak D}}


\newcommand{\mbbc}{{\mathds C}}
\newcommand{\mbbcp}{{\mathds C^{\novel}}}
\newcommand{\cpp}{{\mathfrak C}}

\newcommand{\mbbt}{{\mathds {T}}}
\newcommand{\mbbtp}{{\mathds{T} ^ \novel }}
\newcommand{\mbbtpp}{{\mathfrak{T}}}


\newcommand{\mbbi}{{\mathds L}}
\newcommand{\mbbip}{{\mathds{L}^{\novel}}}
\newcommand{\mbbipp}{{\mathfrak L}}

\newcommand{\mbbj}{\mathds J}
\newcommand{\mbbjp}{{\mathds {J}^{\novel}}}
\newcommand{\mbbjpp}{\mathfrak I}

\newcommand{\mem}{A}

\newcommand{\acase}{c}

\newcommand{\variable}{\mathcal C}
\newcommand{\var}{C}
\newcommand{\current}{{C^\star}}
\newcommand{\evcurrent}{{E^\star}}
%\newcommand{\ext}{\mathcal E}
\newcommand{\emories}{{\mathbb M^\star}}
\newcommand{\lamories}{\mathbb L}
\newcommand{\emory}{{C^\star}}
\newcommand{\past}{{D^\star}}

 \newcommand{\lbc}{\left\{}
\newcommand{\rbc}{\right\}}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}


\newcommand{\lbk}{\left(}
  \newcommand{\rbk}{\right)}

\newcommand{\ndash}{\textendash}


\newcommand{\indet}{\mathbb I}

\newcommand{\types}{{\textup T ^ { \novel }}}
\newcommand{\stypes}{{\textup T  }}

\newcommand{\indices}{\mathbb R}
\newcommand{\rel}{r}
\newcommand{\ind}{r}

\newcommand{\xp}{{x'}}
\newcommand{\xpp}{{x''}}
\newcommand{\xppp}{{x'''}}

\newcommand{\xxp}{{(0,1)}}
\newcommand{\xpx}{{(1,0)}}
\newcommand{\xxpp}{{(0,2)}}
\newcommand{\xpxpp}{{(1,2)}}
\newcommand{\xppxp}{{(2,1)}}

\renewcommand{\ij}{{(i, j)}}
\newcommand{\ji}{{(j, i)}}


\newcommand{\xx}{(x,x)}
\newcommand{\xy}{{(x, y)}}
\newcommand{\yx}{{(y, x)}}

\newcommand{\yz}{{(y,z)}}
\newcommand{\zy}{{(z,y)}}

\newcommand{\xz}{{(x,z)}}
\newcommand{\zx}{{(z,x)}}

\newcommand{\xw}{{(x,w)}}
\newcommand{\wx}{{(w,x)}}

\newcommand{\yw}{{(y,w)}}
\newcommand{\wy}{{(w,y)}}

\newcommand{\zw}{(z,w)}
\newcommand{\wz}{(w,z)}

\newcommand{\xpw}{(x^{\prime},w)}
\newcommand{\wxp}{(x,x^{\prime})}

\newcommand{\xpy}{(x^{\prime},y)}
\newcommand{\yxp}{(y,x^{\prime})}

\newcommand{\xpz}{(x^{\prime},z)}
\newcommand{\zxp}{(z,x^{\prime})}

\newcommand{\xyz}{{ (x,y,z) }}

\newcommand{\dd}{{(\cdot,\cdot)}}
\newcommand{\justdd}{\cdot\cdot}

\newcommand{\ab}{{(a,b)}}
\newcommand{\ba}{{(b,a)}}
\newcommand{\Dtypes}{{\Delta _ \types}}
\newcommand{\Dstypes}{{\Delta _ \stypes}}

\renewcommand{\j}{{J}}
\newcommand{\jp}{{J'}}
\newcommand{\jpp}{{J''}}
\newcommand{\jppp}{{J'''}}
\newcommand{\jstar}{{ \hat J  }}
%\newcommand{\jsstar}{{ \hat{\hat{ J }}  }}
\newcommand{\jhash}{{ J ^ \hash }}

\newcommand{\kstar}{{ \hat  K }}
%\newcommand{\ksstar}{{ \hat{\hat{K}} }}

\renewcommand{\l}{{L}}
\newcommand{\lp}{{L'}}
\newcommand{\lpp}{{L''}}
\newcommand{\lppp}{{L'''}}
\newcommand{\lstar}{{ \hat L  }}


\newcommand{\lhash}{{ L ^ \hash }}

\renewcommand{\mp}{{ M '}}
\newcommand{\mpp}{{ M ''}}
\newcommand{\mstar}{{ M ^ \star }}

\newcommand{\uh}{\hat {\mathbf{ u }}}
\renewcommand{\u}{{\mathbf{ u }}}
\newcommand{\up}{{\mathbf{ u }'}}
\newcommand{\upp}{{\mathbf{ u }''}}
\newcommand{\uppp}{{\mathbf{ u }'''}}

\newcommand{\vh}{ \hat {\mathbf{ v }}}
\renewcommand{\v}{{\mathbf{ v }}}
\newcommand{\vp}{{\mathbf{ v }'}}
\newcommand{\vpp}{{\mathbf{ v }''}}
\newcommand{\vppp}{{\mathbf{ v }'''}}

\newcommand{\bmu}{\bm{\upmu}}
\newcommand{\eh}{\hat {\varepsilon}}
\newcommand{\ep}{{\epsilon'}}
\newcommand{\epp}{{\epsilon''}}
\newcommand{\eppp}{{\epsilon'''}}

\newcommand{\ih}{\hat { I }}
\newcommand{\ip}{{ I '}}
\newcommand{\ipp}{{ I ''}}
\newcommand{\ippp}{{ I '''}}
\newcommand{\stability}{\textit{4}-\textup{{stability}}}
\newcommand{\threepru}{\textit{3}-\textup{{stability}}}
\newcommand{\parthreediv}{\textup{partial-\textit{3}-diversity}}
\newcommand{\Parthreediv}{\textup{Partial-\textit{3}-diversity}}
\newcommand{\condtwodiv}{\textup{conditional-\textit{2}-diversity}}
\newcommand{\Condtwodiv}{\textup{Conditional-\textit{2}-diversity}}
\newcommand{\twodiv}{\textit{2}-\textup{diversity}}
\newcommand{\twodivhash}{\textit{2}-\textup{Div}$^{\hash}$}
\newcommand{\twodivstar}{\textit{2}-\textup{Div}$^{*}$}

\newcommand{\fourdiv}{\textit{4}-\textup{diversity}}
\newcommand{\fourdivhash}{\textit{4}-\textup{Div}$^{\hash}$}
\newcommand{\fourdivstar}{\textit{4}-\textup{Div}$^{*}$}

\newcommand{\threediv}{\textit{3}-\textup{diversity}}

\newcommand{\fourjac}{\textup{\textit{4}-Jac}}
\newcommand{\threejac}{\textup{\textit{3}-Jac}}
\newcommand{\mrpropto}{\mathrel{\propto}}
%\newcommand{\notpropto}{\mathrel{\bcancel{\propto}}}
\newcommand{\half}{\frac{1}{2}}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\rowrank}{rowrank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\closure}{closure}
\DeclareMathOperator{\strict}{strict}
\DeclareMathOperator{\equivset}{equiv}
\DeclareMathOperator{\image}{image}
%\DeclareMathOperator{\ess}{ess}
% \usepackage{scalerel}
% \newcommand\reallywideparen[1]{%
% \begin{array}{c}
% \stretchto{
%   \scaleto{
%     \scalerel*[\widthof{#1}]{\frown}
%     {\rule[-\textheight/2]{1ex}{\textheight}} %WIDTH-LIMITED BIG WEDGE
%   }{1.25\textheight} % THIS STRETCHES THE WEDGE A LITTLE EXTRA WIDE
% }{0.5ex}\\           % THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
% #1\\                   % THIS STACKS THE WEDGE ATOP THE ARGUMENT
% \rule{0ex}{.01ex}
% \end{array}
% }

% \title{
% %Avoiding exploitation in the context of a game with potential unawareness
% %Robustness to exploitation i\cos{\cos{}}n the theory of case-based decisions
% %Future-proof legislation within the theory of case-based decisions
% %Robust case-based decision making via awareness of unawareness
% %Reasoning about unawareness in the theory of case-based decisions\footnoteLast
% %update of this D
% %when the data is limited % when  experience and knowledge are bounded
% Prudent case-based prediction\footnote{The time
%   stamp for this version is   \currenttime\ (AEST),  \today.
%   The following link will bring you to the latest version:
%   \url{https://www.dropbox.com/s/vme2a8y7ei5wisu/}.}
% }
% %Part of the present work was completed whilst working towards my PhD at the
%University of Warwick.}}
 

% \author{Patrick
%   H. O'Callaghan\footnote{\href{http://orcid.org/0000-0003-0606-5465}{
%       \texttt{orcid.org/000-0003-0606-5465}},
%     \href{mailto:p.ocallaghan@uq.edu.au}{\texttt{p.ocallaghan@uq.edu.au}}
% }
% \footnote{I thank Itzhak Gilboa for encouraging this project at an early stage.}
% }

% \affil{Australian Institute for Business and Economics \\ University of Queensland}
% \date{}

% \usepackage{titling}% the wheel somebody else kindly made for us earlier
% \pretitle{% add some rules
% \begin{center}
% \Large\scshape
% }%, make the fonts bigger, make the title (only) bold
% \posttitle{%
% \end{center}%
% \noindent%\vrule height 2.5pt width \textwidth
% \vskip .75em plus .25em minus .25em% increase the vertical spacing a bit,
% make this particular glue stretchier
% }
% \preauthor{%
% \begin{center}
% \Large \lineskip 0.75em%
% % \vrule height 0.4pt width .25\textwidth\par
% \begin{tabular}[t]{@{}l@{}}%
% }
% \postauthor{%
% \end{tabular}
% \vskip -.5em
% \par
% % \vrule height 0.4pt width .25\textwidth\par
% \end{center}%
% }
% \pagestyle{fancy}
% \fancyhf{}
% \fancyhf[lh]{\itshape Immanuel Kant}
% \fancyhf[ch]{\itshape\thetitle{}}
% \fancyhf[rh]{\itshape trans.\ by \theauthor}
% \fancyhf[cf]{--- \thepage ---}
%includeonly
\usepackage{blindtext} \makeatletter \renewcommand\maketitle
  {\begin{center}\mdseries\large
    {\@title}%
    \par\medskip\medskip
    {\normalsize\@author}%
    \par\medskip\medskip\normalfont
    \begin{small}
\emph{Australian Institute for Business and Economics}
\end{small}
\par
\begin{small}
\emph{University of Queensland}
\end{small}
   \end{center}
  } \makeatother \usepackage{fancyhdr} \fancyhf{}
  \fancyhead[LE,RO]{\small\thepage} \fancyhead[CO]{\small\scshape 
  Stable learning and no-arbitrage pricing based on sentiments}

% odd page header 
\fancyhead[CE]{\small\scshape P.H. O'Callaghan} \fancyfoot[L,R,C]{}
\renewcommand{\headrulewidth}{0pt}% disable the underline of the header part
%\includeonly{}
%\includeonly{./introduction/introduction}

\title{\MakeUppercase{Stable learning and no-arbitrage pricing\\ based
on sentiments} \footnote{I thank Itzhak Gilboa for encouraging this project at
an early stage. I thank Khoa Hoang for helpful discussions and pointers to the
data.

    This version has time stamp \currenttime~(AEST), \today. Past versions can
    be found at \url{https://arxiv.org/abs/1904.02934} and
    \url{https://github.com/patrickocal/stable-learner}.}}

\author{\large\textsc{By Patrick H. O'Callaghan} \footnote{Email address
    \href{mailto:p.ocallaghan@uq.edu.au}{\texttt{p.ocallaghan@uq.edu.au}} and
    ORCID iD \href{http://orcid.org/0000-0003-0606-5465}{
      \texttt{0000-0003-0606-5465}}}}


  \date{}
    
  \begin{document}
%  \pagenumbering{gobble}
  \maketitle
  \pagestyle{fancy}
\renewcommand{\abstractname}{\vspace{-\baselineskip}} \thispagestyle{plain}
%\input{frontmatter}
%\maketitle
%\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
 % abstract
%\resetlinenumber 
\begin{abstract}
  \input{abstract/abstract-for-circ.tex}
\end{abstract}
  \setlength{\epigraphwidth}{11.5cm} \epigraph{From the past, the
  present acts prudently, lest it spoil future action.}{\emph{Titian:  Allegory
  of Prudence} }
  %     \newpage
  %\tableofcontents introduction
  %\renewcommand*{\thefootnote}{\arabic{footnote}}

  \section{Introduction} \label{sec-introduction} \textsc{In general,
  prediction requires imagination}: the capacity to go beyond the current
  training set of past cases. To paraphrase \citep[pages 9 and
  10]{hume1896treatise}: \emph{the memory is tied down, by original impressions
  \emph{[past cases]}, without any power of variation; in contrast, the
  imagination is free to transpose and change its ideas.} Well-established
  neuroscientific evidence points to an important role for the imagination and
  the degrees of freedom or flexibility that it provides in improving
  prediction and thus the chances of survival
  \citep{bartlett1932remembering,suddendorf2007evolution,mullally2013memory}.
  In machine learning, where learners (prediction models) are artificial, the
  practice of splitting the data into a training set and a test set is a
  standard \citep{hastie2009elements}. Cases in the training set resemble
  memories whereas cases in the test set resemble instances of the imagination
  in the sense that they allow the learner to explore unseen and potentially
  novel data.

  The goal of external validation is generalization: the ability to adapt
  appropriately to previously unseen data.  The key to generalization is
  stability
  \citep{bousquet2002stability,poggio2004general,mukherjee2006learning} Loosely
  speaking, a learner is stable if small changes in the data yield small
  changes in its predictions: continuity of the learning map from training sets
  to weighting functions (kernels).  \cite{mccloskey1989catastrophic} provides
  famous examples of unstable learning where training on new cases causes
  current kernels to unravel in what is called catastrophic interference.

  In this paper, we derive the learner's kernel on basis of {sentiments}.  We
  formalise \emph{sentiments} as a mapping from samples of past cases to
  rankings of underlying objects of interest.  To account for all kinds of
  intelligence, we provide a framework that captures the behaviour of learners
  with the ability to externally validate their model. The axioms we provide
  are agnostic in relation to the method of external validation. The goal is
  instead to allow an observer to determine whether or not the learner is able
  to generalise to richer training sets of past cases in a consistent manner.
  %In our framework, the prudent learner takes an within-sample perspective
  %\citep[page 7, experiment 1]{chervonenkis2015recollections} and considers
  %the potential for generalizing to richer samples of past cases.  Whereas in
  %classical learning theory, the goal is to minimize the empirical error as
  %measured on the training set, the present framework allows for more abstract
  %goals.
  
  Stability is the key axiom in the present derivation of a learner's kernel on
  the basis of qualitative rankings, given any finite resampling of past
  cases.\footnote{Here a \emph{kernel} is simply a real-valued weighting
  function of two variables.} The learner's kernel is a subjective assessment
  of how similar past cases are to the current prediction problem.  Stable
  learners generalize to novel case types without needing to: re-evaluate past
  rankings; generate intransitive rankings; or suppress novel (transitive)
  rankings. In essence, a stable learner's kernel is not only stable in a
  literal sense, it also allows for meaningful, unbiased generalisations.
 
  %What sort of prediction problem might force an unstable learner to modify
  %its kernel?  generalizes to novel case types without revealing intransitive
  %rankings.  Informally, a kernel is stable whenever there is no reason to
  %modify it.

  %More formally, a kernel is stable provided that, as a function of past
  %cases,
  %it forms a groupoid over eventualities (in \citet[henceforth \gsii
  %]{gilboa2003inductive} kernels with this property satisfy the Jacobi
  %equations).
  %%In all settings, novel types may reveal intransitivities in
  %%unstable kernels thus providing a reason to modify or re-evaluate past
  %%cases.
  %%In market settings, arbitrage may provide another.  It turns out that our
  %%notion of stability implies the canonical one. 
  %We refer to the subset of stable learners that are actually using their
  %imagination to explore novel cases (either literally or in the sense of
  %external validation) as prudent. 

% %One reason why the learner might be forced into modifying their
% %kernel is that it provides others with a free lunch at the learner's
% %expense.

Stable learners stand in contrast to those that learn by doing. Children are
encouraged to learn by making (noncatastrophic) mistakes. They are free to
modify their beliefs and behaviour after the event: the intransitive rankings
that novel case types may reveal \citep{weinstein} are of little consequence.
But what of a market maker that exposes their pricing kernel either directly or
indirectly (by buying and selling securities) on a financial market? We show
that a market maker fails to be stable if, and only if, the returns on
investment that they offer provide a free lunch to other agents today (\ie\
before any novel case types arrive and before they have a chance to
re-evaluate).  This equivalence between stability (when novel data arrives
tomorrow) and no-arbitrage (today) is at the heart of the present paper and it
has a number of interesting implications.\footnote{In general, the learner is
stable if, and only if, its kernel forms a groupoid over eventualities. This
concept corresponds to the Jacobi equations in \gsii.}

First, at the individual level, unstable learners are unlikely to survive.

Second, we can express stable pricing kernels as an empirical (geometric) mean.
This means that, in stable markets (\ie\ those with only stable learners),
prices are efficient stores of information. A stable market price formation
process is modular in cases and separable in securities. This implies that, if
we are also given the order that past cases arrived, we can identify the stable
market pricing kernel (\ie\ the weighting function implied by market prices).
 
Third, what if, in advance, learners ``massage'' their kernels to ensure they
are arbitrage-free (and thus stable)? This alternative method of external
validation is reminscent of De Finetti's Dutch books argument for the formula
of conditional probabilities: isn't this what bookies do?  But then how then
should an observer distinguish the stable learners that are prudent (\ie\ ones
that actually use their imagination or cross-validates their model)?  This
observational ambivolence carries over to the aggregate level.  The market
pricing kernel might be stable even if none of the individual agents in the
market are prudent.  This is a Deus-ex-machina or invisible-hand
property: the market process provides benefits independently of the wisdom of
its agents.  All other things being equal, markets where arbitrage is easier to
exploit should have more stable market pricing kernels.
 
 
The second implication speaks directly to the timeless topic of market
efficiency \citet{fama1970efficient,malkiel2003efficient}.  Stable pricing
kernels provide an explanation for why we might expect a passive (\ie\
buy-and-hold) strategy to perform at least as well as any other strategy.  (The
semi-martingale hypothesis of \citet{fama1970efficient}.) This explanation is
as follows. If the market pricing kernel is stable, then the success of an
active strategy depends solely on its ability to predict the impact of the next
case type.  Whilst this does simplify the task, it also means that the impact
of the next case is unlikely to be large. This is the nature of the empirical
mean: the most likely cases are those that have occurred most frequently in the
past. The impact of a new case is likely to be crowded out by the frequency of
those that it resembles.  On the flip side, unstable pricing kernels come with
arbitrage opportunities and learners modifying their kernels.  Moreover, by
virtue of the fact that unstable market pricing kernels cannot be expressed as
an empirical mean: price movements are likely to be larger; passive strategies
are less attractive; active strategies may indeed be less volatile.

The third implication motivates the present {generalization} of \gsii. Similar to
\gsii, the learner is endowed with a mapping from finite resamplings of past
cases to rankings of eventualities (sentiments).  Via the basic axioms, all
learners (stable or not) are characterised by a kernel that, for every pair of
eventualities, gives rise to a linear function on samples of past cases.
%\citet[henceforth \gsii]{gilboa2003inductive}.  
%When is prudence required to achieve stability? It turns out that, when the
%data is suitably rich, a learner is guaranteed to be stable.
%Of course what is rich
%data for one learner might be poor for another and this is why we choose
%\gsii\
%as our starting point. The main contribution of \gsii\ is to introduce an
%endogenous notion of case types.
With sufficiently rich data, prudence (or the threat of arbitrage) is
superfluous. Who needs an imagination when raw memory will do? To be more
precise, a translation of the main result of \gsii\ to the present context
tells us that a learner is guaranteed to be stable whenever resampling of past
cases generates a \fourdiv\ of rankings: for every subset of $k \leq 4$
eventualities, all $k!$ total orderings (of those $k$ eventualities) feature in
{sentiments}.  The \fourdiv\ axiom is sufficient but not necessary for
a kernel that, for every single eventuality, gives rise to a linear function on
samples of past cases. The stability axiom allows us to substantially weaken
\fourdiv\ to \parthreediv: for every subset of $k \leq 3$ eventualities, at
least $k$ total orderings feature in {sentiments}. Our main theorem then
leads to the following hypothesis: we expect unstable learning to arise in
settings where \fourdiv\ fails to hold and where prudence is required to
achieve stability: \ie\ where the learners are inexperienced (short of
relevant data) and where arbitrage opportunities are difficult to exploit.

%\gsii\ provide a foundation
%for nonparametric kernel-based inductive inference in the same way that
%\citet{Savage-Foundations_of_statistics} does for ``small-world'' Bayesian
%decision problems.  In both \gsii\ and the present framework, learners are
%endowed with qualitative rankings over observable eventualities such as future
%events, outcomes or causal effects.
%%(Statements of willingness to buy one
%%security instead of another would suffice to generate a ranking.)  
%
%\gsii\ makes no distinction between the memory and the imagination.  Given
%that
%neuroscience tells us of a common mechanism for the memory, imagination and
%prediction \citep[for a recent review]{mullally2013memory}, one
%could argue that a single notion of ``conceivable'' cases is all we need.  But
%whereas past cases are often observable, the imagination is not.
%%Hume paraphrase was here
%Moreover, in terms of external validation, the
%same questions arise. For what is outside of the current sample is, by
%construction, unobservable to the learner. One form of external validation is
%leave-one-out cross-validation which \citep{argenziano2019second}
%interpret as a form of second-order induction. Leave-one-out is a special case
%of $k$-fold cross-validation with $k$ equal to the sample size.  It also gives
%rise to a notion of stability in learning theory (also known as
%pointwise-hypothesis stability).  The learner is trained on all the data
%except
%one case and a prediction is made for that case.  In contrast with $k$-fold
%cross validation, our axiomatic framework leaves past cases in tact and
%considers the impact of a novel case type.

Before turning to the model, let us consider some examples. These examples
feature two kinds of learner: one who only looks back at past cases
(within-sample, Inny), and another who also considers the potential impact of
novel types of case (the prudent, Pru).

%Our
%derivation of the market pricing kernel as the empirical mean over past cases
%is what distinguishes the present approach from the forward-looking, states-of
%the-world approach that is most common in microeconomics. There, frequencies
%play less of a role since the likelihood is subsumed into the posterior
%together with the prior.  Our formal starting point is the axiomatic framework
%of \citet[henceforth \gsii]{gilboa2003inductive}.
% 
%  
%
% Consider the arrival of the next case. When the market pricing kernel is
% stable, a learner with an active strategy can only benefit if one of the
% following two occur:
% \begin{itemize}
% \item the next case is of some past type (\ie\ resembles some past case)
% and the learner correctly anticipates the type.
% \item the case type is novel and the learner correctly anticipates the
% impact.
% \end{itemize}
% If we assume that most cases are of past type

  
% ***
% Since external
% validation is costly, we would like to know the answer to the question: under
% what conditions is it worthwhile?  Or, how does the imagination add value to
% memory?

% Inductive inference, together with the tools that enable it, is the standard
% in empirical finance
% \citep{cowles1944stock,fama1970efficient,malkiel2003efficient,white2000reality,fama2015five,harvey2021lucky,gu2020empirical}.
% A market maker adopts an active trading strategy provided it performs well
% out of sample relative to a passive (\ie\ buy-and-hold) strategy. On the
% other side of this coin, the theory of efficient markets dictates that, at
% any point in time, market prices should ``fully reflect'' available
% information.  The empirical implementation of this theory expresses the
% market equilibrium conditions in terms of expected returns.  
% % The canonical theoretical
% % framework for modelling expected returns is of course Bayesian. (With
% % expectations defined in terms of probability distributions over the future
% % states of the world.)  Since the future states of the world are
% % unobservable,
% Whereas in theory expected returns are defined in terms of future states of
% the world, empirical finance works with samples of observations (\ie\ past
% cases).  Just like the market maker, we run a horse race between any strategy
% that seeks to exploit patterns in past cases against a passive strategy.  By
% running this race repeatedly at different points in time, we conclude that
% markets are efficient if passive beats every active strategy sufficiently
% often, out-of-sample \citep{martin2022market,guo2006out}.  
% 
%
% The point of \gsii\ is to explain how priors are formed, rather
% than take them as given: resampling of past cases determines beliefs over
% eventualities.
%The basic ingredients of beliefs are past cases (\ie\
%observations or experiences) and not future states of the world.
 
 % By refining
 % their notion of cases to capture the role of the imagination and
 % forward-looking behavior, we extend their model and apply it to the setting
 % of empirical asset pricing.
 %We provide: a robust test of forward-looking behavior; a 
 %perspective on survival in financial markets without Bayesian agents; and an
 %axiomatic approach to empirical no-arbitrage asset pricing.

%\footnote{In the sequel, we will see that
%{Pru} need not be as ``rational'' as a Bayesian in the sense of modelling all
%future states of the world.} 

\begin{example}[A setting where \fourdiv\ holds]\label{eg-tali}
  A tali (a primitive four-sided die) is rolled over and over
  again.\footnote{Similar to a modern tetrahedral die, the winning number lands
  face down.} So far, it yields only ones, twos and threes, so that the
  empirical distribution, conditional upon the current data, assigns zero
  probability to fours. By resampling (with replacement), we obtain the
  empirical distribution conditional on potential samples of past cases. This
  conditional empirical distribution induces a single ranking of the set
  $X=\{1, 2, 3\}$ of eventualities (outcomes) for each sample. The resulting
  rankings map generates the arrangement of hyperplanes in \cref{fig-tali}.  To
  each region of this arrangement, {sentiments} assigns a distinct
  total ranking.  
  %At each sample $D$ of past cases, we obtain a
  %ranking of $X$ by comparing the empirical frequencies in $D$.
  In dice-like problems, the inherent symmetry between outcomes and past cases
  ensures all $3!=6$ total rankings of $X$ arise.

 
  Our learners, Inny and {Pru}, forecast the outcome of the next roll.  Both
  reveal plausibility rankings that agree with \cref{fig-tali}. By virtue of
  \fourdiv, their ranking maps generalise to higher dimensions (four case
  types) without any need for external validation. The data is sufficiently
  rich for making predictions about $X$.
\end{example}
\begin{figure}
  \centering \input{./figures/fig-tali.tex}

  \caption{\label{fig-tali} Each rational point in this simplex corresponds to
  a sample of past case types: $t_1, t_2$ and $t_3$.  The total
  ranking $(1, 2, 3)$ arises at $D$ (one is least likely and three is most
  likely). This reflects the relative frequencies of past cases in $D$.  The
  inverse ranking $(3, 2, 1)$ arises at $D'$. Within each hyperplane $H^{\{x,
  y\}}$, faces $x$ and $y$ are equally plausible.  The three hyperplanes form a
  centered arrangement.}
\end{figure}

\begin{example}[A non-market setting where \fourdiv\ fails to hold]\label{eg-rates}
  Each month, the federal reserve announces its target overnight rate.  Inny
  and {Pru} wish to forecast the next announcement. The consensus is that there
  will be a rate rise of ${0}, {25}$ or ${50}$ basis points.
  %\footnote{Recall that 100 basis points is equal to $1\%$.}
  Inny and {Pru} have observed the same past announcements.  Current
  circumstances are such that they both find that past cases generate just
  three distinct case types $t_1$, $t_2$ and $t_3$ where the indices capture
  the (increasing) degree to which the fed is ``behind the curve''.  At
  (samples containing only cases of type) $t_3$, Inny and {Pru} agree with the
  plausibility ranking $\left({0}, {25}, {50}\right)$, so that larger rate
  rises are more likely.  At $t_1$, the opposite ranking holds: both find that
  smaller rate rises are more likely. At $t_2$, they agree ranking is
  $\left(\{{0}, {50}\}, {25}\right)$, so that a ${25}$ rise is most likely, and
  the others are equally plausible. 
 
  \Cref{fig-rates} shows some subtle differences between their current (bold)
  rankings maps.  These subtle differences turn out to be substantial once we
  extend each learner's rankings map to a hypothetical fourth case type.  To
  test for stability, the observer assigns $t_4$ the novel ranking
  $\left({25}, {0}, {50}\right)$ and then extends both learners' current
  rankings maps to samples that include cases of type $t_4$.  {Pru}'s extended
  rankings map can be chosen to form a congruent arrangement with six regions.
  In contrast, Inny's extended rankings map forms an incongruent arrangement
  with seven regions.  Samples in the seventh region give rise to the
  intransitive ranking $\left({25}, {0}, {50}, {25}\right)$.
 % \Cref{fig-rates} reveals intransitivities in Inny's extended rankings map,
 % but not {Pru}'s. {Pru} satisfies ``prudence'' whereas Inny does not.
  We may not be able to observe {Pru}'s imagination (cross-validation method),
  but since incongruence is a generic property for triples of hyperplanes, we
  can prove that he is almost certainly using one.
 % This is an example of a
 % non-market setting where \threediv\ (and hence \fourdiv) fails to hold for
 % the current rankings map.
  % Yet the two arrangements
% differ substantially and in ways that are are only revealed once a fourth
% type of case arrives.  By prudently going beyond memory to imagine novel
% cases, we find that {Pru}'s current rankings map {generalizes} to a congruent
% arrangement of hyperplanes in the higher dimensional simplex.
\end{example}
\begin{figure}[h]
  \centering
  \begin{subfigure}{.5\textwidth}
    \def \congruent{1} \centering \input{./figures/fig-rates.tex}
    \caption{\label{fig-rates-prati} {Pru}'s rankings map}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \def \congruent{0} \centering \input{./figures/fig-rates.tex}
    \caption{\label{fig-rates-sempi} Inny's rankings map}
  \end{subfigure}
  \caption{\label{fig-rates} In contrast with \cref{fig-tali}, the current
  (bold) rankings maps features four regions and the current hyperplane
  arrangement is uncentered.  }
\end{figure}
\begin{example}[a bond-market setting that we develop in
  \cref{sec-fin-app}]\label{eg-zeros} Inny and {Pru} are now fair market makers
  of \emph{Zeros} (zero-coupon treasury bonds).\footnote{Similar to a fair
  insurer, a fair market maker sets the market spread to zero.} The
  compound-interest formula for the accumulation process of such bonds is
  \begin{linenomath*}
    \begin{equation*}a^{\xy}= \left(1 + r^{\{x, y\}}\right)^{-x + y},
    \end{equation*}
  \end{linenomath*}
  where $r^{\{x,y\}}$ is the yield to maturity on a forward contract that
  accrues interest between dates $x$ and $y$. If $x$ is later than $y$, then
  the contract is to sell, and the market maker pays this yield, so that
  $r^{\{y,x\}} = r^{\{x,y\}}$.  Let $X \subseteq \R_{\mplus}$ index a suitable
  sequence of trading dates with $0 \in X$ being the spot date.  The
  log-accumulation process has no arbitrages provided it satisfies
  \begin{linenomath*}\begin{equation}\label{eq-no-arbitrage}
    \text{for every distinct $x, y, z \in X$,}\quad \log a^{\xy} = \log a^{\xz}
    + \log a^{\zy} \,.\footnote{To see this, suppose that, for $x = 0 < y < z$,
    Inny sets $a^{\xy} < a^{\xz}\cdot a^{\zy}$.  {Pru} would do well to buy the
    spot contract $\xy$ and sell both the spot contract $\xz$ and the forward
    contract $\zy$. This yields a risk-free profit of $a^\xz \cdot a^\zy -
    a^\xy > 0$.}
  \end{equation} \end{linenomath*}

  In \cref{eq-no-arbitrage}, reference to the data that drives the
  market-makers' prices is suppressed. When $\log a^\xy$ depends on past cases,
  it forms a vector that is normal (\ie\ orthogonal) to a hyperplane such as
  those that separate regions in ranking maps such as \cref{fig-tali} or
  \cref{fig-rates}.  When the data is rich enough to generate the full
  diversity rankings (as in \cref{eg-tali}), both Inny and {Pru} set prices
  that are currently arbitrage-free.  Otherwise, only {Pru} satisfies this
  property. This is true even though they both observe the same collection of
  past cases.  Equivalently, only {Pru} sets prices with the
  canonical form of an empirical (geometric) mean
  \begin{linenomath*}
    \begin{equation} B(x, D) = \prod_{c\, \in
    D}\left( 1 + r^{\{0,x\}}_{c} \right)^{-{x}/{n}}
  \end{equation} \end{linenomath*} for every date $x$ and finite sample $D$
  that we can generate by resampling (with replacement) from past cases.

\end{example}

  \begin{example}[A multi-sectoral investment setting]\label{eg-multi-sector}
  Suppose Inny and {Pru} are potential representative agents in a
    multi-sectoral macroeconomy
    \citep{long1983real,atalay2017how}.
    %(See
    %\citet{gilboa2006empirical} for a related axiomatic derivation for
    %representative agents.)
    Instead of maturity dates, as in \cref{eg-zeros}, $X$ is a variety of
    sectors with $x_0$ being the household (\ie\ consumption sector).  The
    following no-arbitrage equations may be derived from first-order conditions
    of the social planner's constrained optimisation problem provided we
    specify an explicit functional form for the instantaneous flow of utility.
    But here we take a nonparametric approach: we avoid the specification of a
    utility function and proceed from past cases to beliefs and decisions.
 
  For every pair of sectors $x$ and $y$ and nonempty set $D$ of past cases, let
    $S^{\xy}_{D}$ denote a suitably normalised quantity of sector $x$ inputs
    that the social planner invests in order to produce sector $y$ outputs,
    conditional on $D$$;$ then, for every sector $z$, the corresponding
    no-arbitrage equations are 
\begin{equation}
  \log S^{\xy}_{D} = \log S^{\xz}_{D} + \log S^{\zy}_{D}\,.\footnote{ We can
  confirm that, for rather general model specifications, these equations hold
  in our traditional simulations with explicit utility functions.}
\end{equation}
  Just as in \cref{eg-zeros}, when the data is sufficiently rich to generate a
    sufficient diversity of rankings, Inny and {Pru} are indistinguishable.
    Otherwise, only the prudent social planner, {Pru}, has a rankings map that
    satisfies the above no-arbitrage conditions; only {Pru} sets the required
    rate of return on investment (with the gross return $R(x_0, \cdot)$ to
    consumption normalised to one) according to the canonical form of an
    empirical mean
  \begin{linenomath*}
    \begin{equation}
      R(x, D) = \prod_{c \,\in D}\left(1 + r^{\{x_0,x\}}_{c}\right)^{1 / \lvert
      D \rvert}
  \end{equation} \end{linenomath*} for every sector $x$ and finite sample $D$
    that we can generate by resampling (with replacement) from past cases;
    finally, only {Pru} satisfies the inter-sectoral (and inter-temporal)
    consumption Euler equations
\begin{equation}
  \log S^{\xy}_{D} = \log{R(y, D)} - \log {R(x, D)}.
\end{equation}
\end{example}

\begin{example}[Supervised learning]\label{eg-supervised}
  We are given a training set $D \subset X\times Y$ consisting of $n$ i.i.d
  cases (\ie\ examples) drawn from some unknown probability distribution
  on $(X\times Y)^n$. That is $ D = \{(x_i, y_i): i = 1, \dots, n\}$. The basic
  goal of supervised learning is to use the training set $D$ to ``learn'' a
  function $f_D: X \rightarrow Y$ such that, for any given new value
  $x_{\textup{new}}$, $f_D$ provides an accurate prediction of the associated
  value $y_\textup{new}$. That is, we hope that $f_D(x_\textup{new})$ is
  suitably close to $y_\textup{new}$.
\end{example}

\begin{example}[Choosing factors]\label{eg-factors}
  In \cref{eg-tali,eg-rates,eg-zeros,eg-multi-sector}, $X$ respectively
  consists of a set of indices for faces of a tali, rate movements, maturity
  dates and sectors. A different, causal interpretation is possible if we let
  each $x \in X$ denote distinct combination of risk factors such as the market
  factor, firm size and book-to-market equity \citep{fama2015five}. Thus each
  $x$ forms a model with which to forecast stock or bond market returns
  \citep{fama2015five,harvey2021lucky,gu2020empirical}.  Such an interpretation
  brings our notation closer to the standard form that one finds in the
  statistical learning literature
  \citep{cucker2001mathematical,poggio2003mathematics,mukherjee2006learning}.


  Let $(Y, \leq^*)$ denote a suitably ordered criterion space for evaluating
  models.\footnote{We might induce $\leq^*$ via a norm on $Y$ so that $y \leq^*
  y'$ if, and only if, for the true value $y^*$, $\lVert y - y^*\rVert \leq
  \lVert y' - y^*\rVert$.}  The statement $x \preceq_D x'$ is interpreted as
  ``At $D$, $x'$ is a more plausible model than $x$''. The learner subscribes
  to this view if, and only if, the function $f_D: X \rightarrow Y$, $f_D(x')
  \leq^* f_D(x)$.

  Our goal is to derive a stable learning map $D \mapsto f_D$ on the basis of
  {sentiments} $D \mapsto \preceqb_D$ where $D$ is any finite sample of past
  cases.\footnote{The term learning map is introduced in
  \citet{mukherjee2006learning}. There stability is defined in terms of continuity
  of the learning map, something that we prove in \cref{prop-stability},
  below.}
\end{example}
    The rest of the paper is organised as follows. In \cref{sec-model} we
    present the model and key definitions. In \cref{sec-main} we present the
    axioms and main representation theorem. In \cref{sec-fin-app} we develop
    example \cref{eg-zeros} by deriving a representation result for a
    bond-market setting. In \cref{sec-fin-app} we also discuss the implications
    for market efficiency identification of pricing kernels in more detail. In
    \cref{sec-discussion} we provide a more detailed comparison of our main
    theorem with that of \gsii\ as well as matters such as second-order
    induction \citep{argenziano2019second}.
 
 
%In market settings, agents with intransitive rankings do not survive the
% exploitation by others who take advantage of risk-free arbitrage
% opportunities.  But what about an agent that presents transitive rankings
% given any resampling of past cases? One might well expect that such an agent
% would be safe from exploitation.  It turns out that, when resampling of past
% cases fails to generate a sufficiently diverse collection of (transitive)
% rankings, purely empirical agents (who only consider past cases), will
% generically find themselves offering free lunches to others.  This is where
% an imagination should come in handy. 
%% But how is it to be used?  Moreover, how can we provide
%%a model with testable implications given that the imagination is unobservable?
%
% We aim to provide a testable, axiomatic foundation
% for such behavior. We demonstrate the model's efficacy through applications
% to no-arbitrage asset pricing.  The axiomatic framework for case-based
% prediction and inductive inference of \citet[henceforth
% \gsii]{gilboa2003inductive} is our starting point. 
% 
%%By compensating for inexperience or insufficiently rich data,
%%prudent use of the imagination to conjure up novel types of case can ensure an
%%agent's survival and collectively lead to market efficiency.
%%De Finetti's Dutch book arguments for the
%%formula of conditional probabilities provide a hint that this need not be
%%true.
%%
%%In this paper, we provide new, formal insights into the
%%this nexus between intransitive rankings, no-arbitrage asset pricing and the
%%use of the imagination to assist with prediction when resampling the data
%%fails
%%to generate a diversity of rankings. 
%% 
% 
%
%The original idea from which the present paper evolved is a criticism of
    %\gsii.
%In practice, resampling past cases will probably fail to always generate a
    %full
%\fourdiv\ of rankings. (This holds when every subset of four eventualities
%generates $4!=24$ distinct total rankings.) \fourdiv\ is likely to fail when
%the market maker is inexperienced or when the available data is insufficiently
%rich \emph{for the present prediction problem}.  (\gsii\ allow for the
%possibility that the same data may be rich for some prediction problems and
    %yet
%poor in others by developing an endogenous notion case types.)
%
%We weaken \fourdiv\ to partial \threediv\ (every subset of three eventualities
%generates (at least) three distinct total rankings).  This is the weakest
%condition for which there exists a suitably unique functional representation
    %of
%rankings conditional upon resamplings of past data of the same form as \gsii.
%But alone this weaker form of diversity will not do.  Market makers that
%satisfy the other (reasonable axioms) of \gsii\ will find themselves
%offering free-lunches to other market participants, unless they are prudent
%enough to use their imagination \emph{when past cases fall short of \fourdiv}.
%In such settings, prudent market makers peer into the future and rank
%eventualities conditional, not only upon past cases, but also upon imagined,
%novel types of cases.\footnote{Case types are the dimensions of the model just
%as future states are for Bayesian models.}
%%
%%The reason is
%%that they only consider past cases. Market makers that use their imagination
%%to
%%explore (unobservable) novel case types find themselves at an advantage.
%
%Market makers gather (traditionally in trading pits) and provide liquidity by
%filling buy and sell orders that brokers bring to market.  In over-the-counter
%bond markets, market makers buy and sell forwards across different horizons.
%Predictions are made on-the-fly and split-second decisions become contracts
%with implications for the months ahead.  A market maker with openly
%intransitive rankings over existing securities is unlikely to survive the
%inevitable exploitation by others.  But what about intransitivities that
%surface only once new information arrives? That is, what if intransitivities
%are uncovered by novel case types (in a higher dimensional {generalization} of
    %the
%market maker's current model)?  Eradicating such intransitivities \emph{ex
%post} requires a re-evaluation or pricing of the informational content of past
%cases and their role in determining current yields.  A market pricing process
%of this form would be inefficient. 
%
%A key insight of the present paper is that potential intransitivities generate
%arbitrage opportunities even before the arrival of new information. This
%insight has a number of implications. First,
%this opens up a channel for market activity to weed out such
%intransitivities \emph{even in the absence of market makers that prudently use
%their imagination}. 
%Second, market makers such as Inny will not survive. Other agents will
%exploit their lack of imagination today.
%Third, a market maker who avoids using their imagination may
%appear prudent (in the sense of using their imagination) provided they are
%careful enough to set prices that are free of arbitrages.
%
%In our model, no-arbitrage pricing is equivalent to prudent pricing.  That is,
%equivalent to the existence of a suitably unique market pricing function
%(conditional on any resampling of past cases) that might belong to a market
%maker that is prudent.  That is to a market maker whose model {generalizes} to
    %higher
%dimensions (novel case types) without generating intransivities or other
%violations of the basic axioms of \gsii.  This invisible hand/``Deus ex
%machina'' style result is similar in spirit, yet quite different from notions
%of market efficiency that are surveyed in
%\citet{fama1970efficient,malkiel2003efficient}. Those notions of
%market efficiency are expressed in terms of a ``fair game'' or expected
    %return.
%Markets are efficient if the expected return of a portfolio is no higher than
%that which is possible with a buy-and-hold strategy. Or perhaps that markets
%are efficient whenever price increments follow a random walk.  These notions
%are somewhat more forward-looking than ours. Here, markets are efficient when
%the arrival of new information does not cause a re-evaluation of past cases.
%This is indeed efficiency in an operational sense because it ensures
    %modularity
%of the market model: similarity weights that have been generated in the past
%remain unchanged when new information arrives.
%
%We would however expect to find differences in repeated experiments. Whether
    %in
%the field or the lab, we would expect that the actual arrival of novel case
%types would have a different impact on Inny than on {Pru}.
%
%
%%
%%This demonstrates that shape associations and beliefs even in the absence of
%%market makers that are endowed with a rich imagination and that are prudent
%%enough to explore the impact of new information.  In other words, to borrow an
%%analogy with machine learning by eradicating current arbitrage opportunities,
%%market makers inadvertently make markets efficient. 
%
%% 
%%Intransitivities in preferences are known to open
%%the door to arbitrage opportunities.  Our main result is less obvious: in
%%information-poor situations, prudently avoiding future intransitivities can
%%ensure a no-arbitrage pricing strategy and hence survival \emph{today}. That
%%is, within the current prediction problem and before any novel case types
%%arrive.
%
%\paragraph{Robustness of these examples} On the one hand, there are far fewer
%maps of current rankings that are symmetric (in the sense that the
    %intersection
%of hyperplanes occurs inside the simplex as in \cref{fig-tali}) than there are
%asymmetric (in the sense of \cref{fig-rates}). Simply consider the fact that
%the simplex of \cref{fig-tali} is of measure zero in the plane.
%% On the other,
%%once the prediction problem consists of four or more eventualities (members of
%%$X$), symmetric rankings maps may also expose arbitrage opportunities.
%
%\paragraph{A robust test of prudent behavior} In an experimental setting, if
%one finds that a subject sets prices that are arbitrage free, then the current
%rankings map generates congruent hyperplanes as is the case for {Pru} in
%\cref{fig-rates-prati}. Recall that three hyperplanes are said to be in
%general (\ie\ generic) position if they are incongruent. There is therefore a
    %zero
%probability that a type-{Pru} subject is prudent by chance. 
%
%\paragraph{Machine learning application I} Consider a machine that is trained
%to set prices on the basis of past data. We would like to know whether the
%machine is capable of learning to avoid providing arbitrage opportunities
%without explicitly encoding the no-arbitrage condition in its objective. To do
%so it will have to be intelligent enough to ``imagine'' a future that is
%different to the present and perform well out-of-sample.
%
%\paragraph{Machine learning application II} Consider a machine that is trained
%to set prices on the basis of past data, only this time the no-arbitrage
%condition is explicitly encoded into its objective. Our results show that,
%regardless of what the future throws at the machine, the current rankings map
%will not need to be suddenly reconfigured to correct errors made today. This
%may substantially reduce the cost of checking {generalization}s of the current
%rankings map.
%
%
%%\begin{example}\label{eg-full-model}
%%
%% A canonical large-world setting is that of the global financial markets.  In
%% 2019, all models ommited details of COVID-19. Similarly, in 2007, a clear
%% description of the sub-prime mortgage crisis was beyond reach.  The central
%% role of simulation and bootstrap methods in empirical finance points to a
%% prevalence of inductive reasoning.\footnote{Consider
%% \citet{cowles1944stock}, \citet{white2000reality},
%% \citet{fama2015five} and \citet{harvey2021lucky}.} That is, to
%% forecasting on the basis of past cases as opposed to a full description of
%% future states. At the same time, market makers need to set prices that are
%% robust to changes that open the door to exploitation via arbitrage.
%%
%%\end{example} 
%%
%% This basic distinction between goes to the heart of the present paper. Our
%% notion of cases captures this distinction and
%% 
%%
%%Whilst imagining novel cases is easy to do in \cref{eg-tali}, in other settings
%%it will be hard.  Moreover, many ``real world'' settings feature much less
%%symmetry than the outcomes of a die (or even a tali). Asymmetry is where
%%intransitivities are more likely to arise.
%%
%%\pagenumbering{arabic} \setcounter{page}{2} Incomplete models are a key
%%motivation for recent axiomatic updates to the standard Bayesian framework such
%%as ``reverse Bayesianism'' of \citet{KV_Reverse_Bayes}. This and other work
%%(\citet{KV-Awareness_of_U}, \citet{HR-Knowledge_of_U} and
%%\citet{GKMQT_Robust_experiments}) on unawareness and robustness in the
%%state-space sense provide part of our inspiration for the present upgrade to
%%the axiomatic foundations of inductive inference in \citet[henceforth
%%\gsii]{gilboa2003inductive}. By taking rankings as primitive, we extend
%%\gsii\ to accommodate a forward-looking version of the second-order induction
%%of \citet{argenziano2019second}. 
%
%In the present framework, the basic building blocks of the model are
%observations or (synonymously) past cases. A given past case may be empirical
%or theoretical, and the learner's model is naturally bounded in size and
%scope by the learner's experience. Our contribution is to extend the
%framework of \gsii\ to model less experienced learners that are prudent. That
%is to agents that are able to proactively engage in second-order induction: by
%``pulling themselves up by the bootstraps'' and looking at how their model
%{generalizes} to novel cases. Thus allowing them to survive their initial
    %phase of
%inexperience.
%
%In the remainder of this section, we informally introduce: the model of
%\cref{sec-model}; the axioms and matrix representation of
%\cref{sec-axioms-theorem}; and the applications to which we return in
%\cref{sec-discussion}. Proofs of main results appear in appendices
%\ref{sec-proof-main} to \ref{sec-proof-foureq}.
%
%
%
%The learner is endowed with a
%qualitative plausibility ranking of eventualities given her current database
%$D^{\star}$ of past cases. Moreover, the same is true for every finite
%resampling of $D^{\star}$. We identify conditions on the resulting family of
%(ordinal) rankings for the existence of a suitably unique
%real-valued matrix $\mathbf v$ on eventualities$\times$cases that
    %\emph{represents} the
%information in these rankings. The form of this representation is linear on
%databases (\ie\ additive over cases) and separable on eventualities, so that
    %for
%every database $D$, eventuality $y$ is
%more likely than $x$ if, and only if,
%\begin{linenomath*}
% \begin{equation}\label{eq-similarity}
% \sum_{c\,\in D} \mathbf v(x,c) \leq \sum_{c\,\in D} \mathbf v(y,c).
%\end{equation}
%\end{linenomath*}
%The similarity weight $\mathbf{v}(x,c)$ is the degree of support that case $c$
%lends to $x$.
%\begin{example}As a canonical example, consider predicting the slope
    %coefficient
% $\beta_{i}$ from the regression of asset $i$'s returns on the market
    % portfolio
% \citep[in the two-pass method of][]{FM-Two_pass}. Then values $\beta_{i}$ are
% eventualities and $D^{*}$ is the current sample of past returns. In this
% setting, $\mathbf v$ is an empirical log-likelihood function that we generate
% via a generalised notion of bootstrapping of cases in $D^{\star}$.
% % This empirical likelihood is robust to the car primitive rankings via an
    % initial round of least-squares
% % maximisations, but we only retain qualitative ordinal information $\mathbf
    % v$
% % is robust to the final is additive in the sample, maximum-likelihood
% % estimation conforms to maximising the the current class of prediction
% % rules. But the current . This can be seen by observing that the log Least
% % squares to generate a ranking given each resampling $D$; we retain only the
% % ordinal information objective function of a least squares minimisation
% % provides an ordinal ranking for every resampling of past observations. The
%\end{example}
%Key to the contribution of \gsii\ is an endogenous notion of case types: a
%partition of cases according to the marginal information they contribute to a
%given database. This marginal contribution is measured in terms of the impact
    %on
%the rankings of eventualities.\footnote{This notion of case type is therefore
% close to Quine's ``perceptual similarity'' (see \cref{sec-discussion}).}
    % Case
%types are analogous to states in the sense that they form the model's
%dimensions: the lower the dimension the less the experience.
%
%\begin{example}[Search Engine Results Page, SERP]\label{eg-search_engine}
% Advertising aside, when users conduct a web search, the search engine
    % compiles
% a ranking $\preceqb_{D^{\star}}$ of (web)pages $x, y, z, \dots$ on the basis
% of its database $D^{\star }$ of past cases: searches of past users plus
% feedback from subsequent clicks.  Resampling yields other databases $D$ and
% other rankings.  At one extreme, past cases may be so similar to one another
% that the same plausibility ranking arises regardless of how the data is
% resampled.  At the other, past cases may be sufficiently rich that resampling
% yields every feasible plausibility ranking of eventualities.
%\end{example}
%
%A rich set of past cases is at the heart of the \fourdiv\ axiom of \gsii. This
%restricts the model to learners whose current data is sufficiently rich that
%a resampling exercise generates \emph{all $4!=24$ strict (\ie\ total) rankings
%of every subset of four eventualities}. In this paper, we accommodate less
%inexperienced learners by only replacing \fourdiv\ with \condtwodiv. Given
%the other basic axioms of \gsii, \condtwodiv\ turns out to be equivalent to
%requiring that, for every three distinct eventualities $x$, $y$ and $z$,
%resampling generates at least $3$ of the $3! = 6$ possible distinct strict
%rankings of this triple. \Condtwodiv\ is minimal in the sense that, in its
%absence, we lose both existence and uniqueness of the similarity
    %representation
%(see \cref{eg-lexicographic}).  
%
%To compensate for a lack of experience, we introduce a subtle and more
    %flexible
%notion of cases that allows us to capture the learner's potential awareness
%of her limited experience.  Formally, related notions in the literature go by
%the name of unforeseen consequences in \citet{GQ-Surprises} and shadow
%propositions in the setting dynamic awareness in \citet{HP-Dynamic_awareness}.
%Via a content-free case $\novel$, the learner can explore the impact on her
%model of the arrival of a novel case type. In effect, this involves
%meta-analysis of how her similarity function will evolve over time. Hence the
%reference to second-order induction.
%
%The prudent learner ensures her model is robust to the arrival of novel case
%types. She ensures that, when a novel case arrives, she can accept the ranking
%it generates without finding herself in the potentially costly position of
%generating intransitive rankings when she combines past and novel cases.
%
%\begin{example}[Second-order inductive inference]\label{eg-second-order}
% Consider a search-engine startup seeking to establish itself in the face of
% incumbents with the experience of Google. The start-up engages in second
% order induction when it is learning the similarity function itself. This may
% include learning the values $\mathbf{v}(x,c)$ of \cref{eq-similarity}, but it
% may also involve costly updates of the model structure ``on the fly'', \eg\
% redefining case types, rankings, \etc. The startup is prudent if, \emph{ex
% ante}, it structures its model to ensure that it is relatively costless to
% extend to novel case types$:$ once they arrive.
%\end{example}
%
%Prudence is only worthwhile when revisions of the learner's model `on the
%fly',
%once the novel case arrives, is costly.  % Yet, in many settings,
%% revising a model on the fly is indeed costly. And in some settings, cases are
%% by
%% definition novel.
%Consider ``zero-day attacks'' in the setting of cyber security.
%\Citet{Hota_et_al-Cyber_security} highlight the essence of time when a novel
%attack on a computer network arrives; and that such attacks are novel
    %precisely
%because cyber-security experts have already built in solutions to known
%vulnerabilities. Also, tradeoffs between time, cost and learning are nowhere
%more important than in finance. For a bond-market setting, we are able to
%provide a formal equivalence between {stability} and arbitrage pricing in
%\cref{sec-discussion}.  In \cref{sec-discussion}, we also discuss: other
%applications; empirical evidence linking intransitivity, memories and novelty;
%and connections with the literature on second-order induction in more detail.
%
%
%
%% The exposition of the model appear in \cref{sec-model}, the axioms and main
%% theorem then follow in \cref{sec-axioms-theorem}. Proofs appear in
%% \cref{sec-proof-main} to \cref{sec-proof-foureq}.
%
%
%
%
%
%
%
%
% % \begin{example}\label{eg-Black_swan} Before the first European observation
    % of
%% black swans in modern day Australia in 1697, the typical European zoologist's
%% dataset would justify the plausibility ranking ``On his expedition to New
%% Holland, it is more plausible that Willem de Vlamingh will observe white
%% swans than black ones.'' It is only with a sufficiently rich knowledge of
%% the migratory behavior of Swans, a rich dataset documenting their absence in
%% tropical regions, and a phylogenetic theory of Swan evolution that a
%% zoologist could convincingly propose the reverse plausibility ranking.  The
%% fact that de Vlamingh's observation had the impact on zoology that it did
%% reveals that few if any zoologists were capable of such a deep and convincing
%% hypothesis.  Nonetheless, a prudent zoologist might well anticipate that,
%% since expeditions typically yield new data, it would be wise to explore the
%% potential impact of a novel case on her forecast. For instance, is it feasible
%% that resampling some combination of past data with novel case will generate a
%% forecast with intransitive rankings?
%% \end{example}
%
%
%
%
%
%% %Yet, in the model of \gsii, this is the kind of ability the forecaster must
%% %have.
%
%% We shall model a novel case as a variable with unknown domain and unknown range.
%% This is because cases are defined independently of the rankings that they
%% determine.  An alternative approach would be to take the rankings, given each
%% database as primitive
%
%% Although the learner can only observe past cases, she might also observe that
%% resampling them does not yield a forecast with a diverse family of plausibility
%% rankings.  And, in turn, she may observe that, if the present prediction problem
%% turns out to be unlike past cases, her future forecast may well be more diverse
%% than her current one.  She may then ask whether she will need to revise her
%% current forecast in order to accommodate a forecast generated by richer
%% databases?  In the present paper, we show that this line of observation and
%% questioning naturally leads to the following conclusion: if the inexperienced
%% learner restricts her resampling procedure to past cases, then she may be
%% omitting important information that is concealed by her lack of experience.  At
%% least, that is, if she accepts the basic axioms for case-based decision theory
%% (and prediction in particular).
%
%% The basic axioms are as follows.  The first and most basic is that, conditional
%% upon any feasible database of past cases, the plausibility ranking is both
%% transitive and complete.  The second axiom, Combination, is the requirement that
%% whenever the plausibility rankings at two disjoint databases coincide, then the
%% combined database (formed by taking the union) generates the same plausibility
%% ranking.  When the number of eventualities is finite, the third axiom,
%% Archimedeanity, requires that for any pair of disjoint databases, the
%% information associated with the first is eventually swamped by the combining it
%% with sufficiently many copies of the second.
%
%% The formal question we shall ask is the following: for a learner with a
%% current forecast that satisfies the basic rationality axioms of \gsii, will
%% these axioms apply to any {generalization} of her current forecast that is formed by
%% considering databases involving resampled copies of a novel case?  If not, then
%% the omitted information that we refer to in the penultimate paragraph above may
%% bias her forecast in such a way that intransitive plausibility rankings arise in
%% every {generalization} of her current forecast that satisfies Completeness (at each
%% database), Combination and Archimedeanity.  This is essence of the Stability
%% axiom that we appeal to instead of the Diversity axiom of \gsii.
%
%
% % The consequence of this bias (\ie~a violation of the Stability axiom) is to
% % preclude the representation of the forecast by a real-valued suitably
% % additive weighting function of the form
%% \begin{equation*}\text{$v:$ eventualities $\times$ past cases $\rightarrow \R$. }\end{equation*}
%% That is a matrix $v$ such that for any pair of eventualities $x$ and $x'$ and
%% any database of resampled past cases $D$,
%% \text{$x\preceqb_D x'$\quad if, and only if,\quad $\sum_{c\,\in D} v(x,c)\leq \sum_{c\,\in D} v(x',c)$},
%% where $\preceqb_D$ is the plausibility ranking that is determined by
%% $D$. (For instance, in a typical empirical setting, $\preceqb_D$ might be the
%% ordinal content of the empirical likelihood conditional on $D$.)
%
%
%% When states, events and probabilities summarise the learner's view of the
%% world and its impact on her predictions, each state specifies a distinct
%% dimension of the model and the probability distribution assigns a weight to each
%% dimension.  A key feature of \gsii, where cases are primitive, is that the
%% dimensions are endogenous to the problem at hand.  The idea is that, for one
%% prediction problem, two cases may may be so similar as to considered as
%% equivalent to one another, or of the same \emph{case type}.  For another
%% prediction problem, the same two cases may no longer be equivalent, each giving
%% rise to different weights or indeed different plausibility rankings.  It is the
%% set of case types that determines the notion of dimension in the present model.
%
%% Our first conceptual {generalization} of \gsii~is to consider case types as one measure
%% of experience.  For instance, in \cref{eg-Search_engine}, the (extremely)
%% inexperienced engine has a forecast of dimension one, whereas the experienced
%% engine has a forecast that is of dimension at least $n!$, where $n$ is the
%% number of plausible webpages.  The number of case types is always at least as
%% large as the number of rankings of eventualities %The scenarios we describe in
%% \cref{eg-Search_engine} support this interpretation: the search engine with one
%% case type is, in effect, inexperienced and any resampling of past cases will not
%% change this fact.  %It is clear that, the richer the data the search engine has
%% access to, the more diverse its conditional predictions will be.  %Conditional,
%% that is, upon a given database that arises from resampling or subsampling.  Yet
%% one forecast is more diverse than another if it contains a greater number of
%% distinct plausibility rankings.  Thus, the diversity of a forecast provides
%% another measure the learner's experience which is distinct from, but closely
%% related to, case types.  %The essential distinction is that the diversity of a
%% forecast is a lower bound on the number of case types.
%
%% The main contribution of the present work is to allow for forecasts that are
%% nondiverse in the sense that they do not satisfy the diversity axiom of \gsii.
%% The latter axiom requires that, for every strict ordering of four eventualities,
%% there exists a database, conditional upon which, the learner's ranking over
%% all eventualities contains that ordering.  In \gsii, as well as in the closely
%% related models of \citet{GS_Act_similarity,GS_CBDT_Book}, the diversity axiom is
%% presented as a purely technical requirement for the desired representation.
%% When diversity fails to hold, the remaining axioms do not suffice for a matrix
%% representation that is separable across eventualities.  Our answer to this
%% fundamental difficulty (which also arises in other settings such as
%% \citet{Azrieli_Valence_dimension}), is to ask that the learner is forward
%% looking.  Our learner is forward looking if her behavior can be explained by
%% adding a ``novel case'' to the model and studying the ``potential {generalization}s''
%% of her forecast.
% %% We conclude the introduction with a brief discussion these concepts and
    % the main
%% axiom to which they give rise.  In contrast with a standard case, which
%% determines a plausibility ranking, we define a \emph{novel case} to be one that
%% can be associated with any plausibility ranking.  %Indeed, it defines a free
%% variable on the set of feasible plausibility rankings.  Since a novel case is
%% intended to capture cases that lie beyond the experience of the learner, its
%% weight in a given a database should be indeterminate.  As such, the plausibility
%% ranking associated with any database that contains a copy of the novel case is
%% also indeterminate.
%
%% By assigning a plausibility ranking to this case and to its affiliated
%% databases, we obtain a \emph{(potential) {generalization}} of the (current) forecast.
%% It will suffice to study the {generalization}s that involve three or four
%% eventualities.  Two {generalization}s will be of the same type if they feature the same
%% degree of diversity or experience.  Our main axiom requires that every {generalization}
%% that satisfies all the necessary axioms of \gsii~(with the possible exception of
%% transitivity), has a modification that is of the same type and that also
%% satisfies transitivity.
%
%
%% The role of our main axiom, which we refer to as \emph{{stability}} is to exclude
%% the forecasts with {generalization}s that feature essential intransitivities. That is,
%% our learner fails to be prudent if she (behaves as if) she fails to foresee
%% that, should a certain case arise, the only way she can satisfy the \gsii~axioms
%% avoid specifying an intransitive plausibility ranking at some database is to
%% retrospectively change her current forecast or by being dogmatic and ruling out
%% certain plausibility rankings regardless of the data.  To borrow from our
%% epigraph, by being imprudent the learner may spoil her future action.
%
%% % The proof of our main theorem yields a technical condition that is equivalent
%% % to {stability} when there are at least two case types.
%% % This condition, that may be easier to verify in certain settings.
%
%
%% Through examples, we will highlight the ways in which {stability} is strictly
%% weaker than imposing the diversity axiom on the forecast.  We will also show
%% that {stability} is weaker than potential diversity (the forecast admits an
%% {generalization} that satisfies the diversity condition).
%
%% A casual comparison of our representation with that of \gsii~leads to the same
%% conclusion.
%%In essence, of the learners with an additive the only learners we
%%The representation of \gsii~characterises the experienced learner via a
%%matrix that is diversified.
%
%Hume establishes two basic principles. The first is that all \emph{ideas
%proceed mediately or immediately from their correspondent impressions}.
%Impressions are experiences such as sensations, passions and emotions. Ideas
%are faint images in thinking and reasoning. Before establishing his second
%principle he notes that there are two basic forms of idea, the first is a
%memory which ``retains to a considerable degree its vivacity and is between an
%impression and an idea''.  It is the faculty by which we repeat our
%impressions.  The second form of idea is the imagination where perception is
%faint and languid.
%\begin{quote} \dots yet the imagination is not restrain'd to the
% same order and form with the original impressions; while the memory is in a
% manner ty'd down in that respect, without any power of variation.
%\end{quote}
%The second principle concerns the imagination: \emph{of the liberty of the
%imagination to transpose and change its ideas.} In this way, both memories and
%our imagination form our experiences.  But how should we model this
%distinction? How should a learner use his limited constrained memories or the
%liberty of her imagination?
%%model
\section{Model}\label{sec-model}
%\paragraph{The primitives of our model}\hskip-5pt%\label{sec-primitives}
% consist of the nonempty sets $X$ and $\current$.
%For the current prediction problem, we interpret
%members of $X$ as \emph{eventualities} and members of $\current$ as
%\emph{current} cases.  Our first substantive departure from \gsii\ is to allow
%for two forms of current case.  That is we take $\current$ to be the union
%$\past \cup \{\novel\}$ of \emph{past} cases $\past$ and a \emph{free} case
%$\novel$.\footnote{
%We assume $\novel$ to be the only free case $\current$, not
%because we think learners are constrained in this way, but because one degree
%of freedom is sufficient for our purposes.  One may view the matter from an
%evolutionary perspective: {Pru} represents a minimal deviation from Inny.}
%As
%per Hume's distinction (second paragraph of our introduction), past cases are
%``tied down, original impressions'' \ie\ constant (of arity zero); and
%$\novel$
%is ``unrestrained'', and, for now, a variable (of positive arity).


\paragraph{The primitives of our model}\hskip-5pt consist of the nonempty sets
$X$ and $\mbbcp$.  For the current prediction problem, we interpret members of
$X$ as \emph{eventualities} and members of $\mbbcp$ as \emph{current} cases.
Our first departure from \gsii\ is to allow for two forms of current case.
That is we take $\mbbcp $ to be the union $\mbbc \cup [\novel]$ of a set
$\mbbc$ of constant, \emph{past} cases and a set $[\novel]$ of copies of the
variable, \emph{free} case $\novel$.
\begin{remark*}
  %Recalling Hume's distinction (second paragraph of our introduction), past
  %cases are indeed ``tied down, original impressions''. Moreover, the
  %imagination is indeed free to ``transpose and change its ideas'' via copies
  %of $\novel$, all of which are free of content and, in this sense, identical.
  By way of analogy with computer memory, a natural implementation is as
  follows.  Take any case $c \in \mbbcp$ to consist of a pair $p \times m:$ a
  pointer $p$ that references a memory location and the memory content $m$.
  For $c\in \mbbc$, just as in the setting of \gsii, content of $m$ is
  meaningful.  For $c \in [\novel]$, although $m$ is empty (assigned a
  ``garbage'' value), the allocation is itself valuable. Pairs $c, d\in
  [\novel]$ are indistinguishable in terms of content and are in this sense
  copies of $\novel$.

  The learner is free to assign novel, imagined content to $\novel$ and before
  going on to explore the prediction problem with this content in mind.  As we
  will show, an observer (or more sophisticated learner) who observes past
  cases, but not the imagination, can nonetheless use $\novel$ to analyse the
  learners' behaviour.
\end{remark*}

%Our choice of $\current$ as primitive ensures the learner's model is
%``pointed''. That is to say $\current$ is the current data. The learner is
%nonetheless endowed 
%Not all cases in $\current$ 
%By resampling (with replacement) cases in $\current$, we obtain a set
%$\mbbcp$. 

% \paragraph{Databases} %
With case \emph{resampling} from the literature on bootstrapping in mind, let
\begin{equation*}\mbbd\defeq \lbc D\subseteq \mbbc: \countof D < \infty \rbc \end{equation*} denote the set
of (finite) \emph{determinate or constant databases} or memories and let
$\mbbdp$ denote the corresponding set of all finite subsets of $\mbbcp$.
Finally, throughout the sequel, we take $\cpp$ to denote a member of
$\{\mbbc, \mbbcp\}$ without reference to the latter set. Similarly, 
\begin{linenomath*} \begin{equation*} \text{$\dpp$} =
  \left\{ \begin{array}{ll} \mbbd & \text{if, and only if, $\cpp = \mbbc$,
  and}\\ \mbbdp & \text{otherwise.} \end{array}\right.  \end{equation*}
\end{linenomath*}
%\vskip-5pt
\paragraph{Sentiments.}\hskip-5pt We now take a first
step towards formalising the ranking maps of \cref{fig-tali,fig-rates}.
(For the translation from databases to rational vectors, see
\cref{sec-proof-main}.)
For each $D$ in $\mbbd$, the learner is endowed with a well-defined
plausibility ranking $\preceqb_D$ in the set $\relations(X)$ of binary
relations on $X$.  Denote the symmetric and asymmetric parts of $\preceqb_D$ by
$\simeq _D$ and $\precb_D$ respectively. 

\emph{Sentiments} $D \mapsto \preceqb_{D}$ are thus a vector in
$\relations(X)^{\mbbd}$ of the form
\begin{linenomath*}
  \begin{equation*}\preceqb_{\mbbd} \defeq \langle\preceqb_{D} : D\in
  \mbbd\rangle\,.\end{equation*}
\end{linenomath*}
This subtle departure from the form $\{\preceqb_{D} : D \in \mbbd\}$ of \gsii\
is closer in structure to visual representations (\cref{fig-tali,fig-rates})
and the graph $\{D \times \preceqb_{D} : D \in \mbbd\}$.\footnote{I thank
Maxwell B.  Stinchcombe for bringing this point to my attention.}

%For any given $D$, the ranking $\preceq_D$ on $X$ is induced by the
%anticipated
%ranking of eventualities $Y$. That is, let $X = \times_i^m X_i$ and let $Y =
%\R^m$, then, for some function $f_D:X \rightarrow Y$, if $x\in X$ and $y =
%f_D(x)$ for some $D \in \mbbd$, then for $0 \leq k, l \leq m+1$, $x_k
%\preceq_D
%x_l $ if, and only if, it is more plausible that security $l$ will yield a
%higher return than security $k$ (\ie\ $y_k < y_l$).  Under this
%interpretation,
%$x_k$ is the data point generated through current research into security $k$.
%When the function $f: X \times \mbbd \rightarrow Y$ is unknown, the learner
%exploits information in the ranking map to estimate $f$ and in turn use the
%estimated function to generate a prediction of the vector $(y_0, \dots,
%y_{m+1})$. This
%information is revealed by varying the sample and generating new rankings of
%$X$ (and, implicitly, new rankings of the set $S$ of securities and $Y$).
%The {sentiments} are the mathematical object to which the axioms apply. It
%corresponds to what \citet{mukherjee2006learning} refer to as the learning
%algorithm which is a mapping $D \mapsto \mc H$. Where 

%For each $C$ in $\mbbdp \bs \mbbd$, the fact that for some $c \in [ \novel ]$,
%$c \in C$ means that $\preceqb_{C}$ is indeterminate, free variable in
%$\relations(X)$.
%\begin{remark}
%Although, in isolation each such $\preceqb_{C}$ is free, when the axioms we
%  introduce hold, the potential values of the variable $\preceqb _ \mbbdp
%  \defeq \langle \preceqb _ C : C \in \mbbdp \rangle$ are constrained by the
%  current values of the constant $\preceqb_\mbbd$.
%\end{remark}

% \begin{remark}

%   The fact that the empty database $\emptyset$ is absent from
% $\mbbd$ means that $\preceqb_\emptyset$ is undefined (just as
% $\preceqb_D$ is not defined when $D$ is infinite). We leave
% $\preceqb_\emptyset$ undefined since our axioms will accommodate the
% possibility that $x\precb_D y$ for every $D\in \mbbd$. Indeed the situation
% where our forecaster is without data and without a free  case would only seem to
% be justified when she is entirely unaware of the prediction problem. But in that
% case, we find it unreasonable to suppose that $\preceqb _ \emptyset$ is
% complete and transitive on $X$.
% \end{remark}

% At the same time, the learner that is aware of the prediction problem may
% find it useful to contemplate what she would do if she had
%Instead, we   accommodate this hypothetical scenario and simultaneously
%simplify our exposition by including $\emptyset$ in the set of {generalization}s of
%$\mbbd$, which we introduce shortly.



\paragraph{Case types.}\hskip-5pt As in \gsii, two past cases $c , d \in \mbbc$
are of the same \emph{case type} if, and only if, the marginal information of
$c$ is everywhere equal to the marginal information of $d$. Formally, $c \sim
^{ \star } d$ if, and only if, for every $D \in \mbbd$ such that $c , d \notin
D$, $\preceqb _ { D \cup \{ c \} } = \preceqb _ { D \cup \{ d \} }$.  By
observation 1 of \gsii, $\sim ^{ \star }$ is an equivalence relation on
$\mbbc$. The equivalence classes of $\sim^\star$ generate a partition $\mbbt$
of case types.

We extend $\sim ^{ \star }$ to $\mbbcp$ by taking $[ \novel ]$ to be an
equivalence class of its own, so that, for every $c \in \mbbc$, $c \nsim ^{
  \star } \novel$.  We let $\mbbtp$ denote the corresponding partition of
$\mbbcp$.  Like \gsii, we also extend $\sim^{\star}$ to $\mbbdp$ by treating
databases that contain the same number of each case type as equivalent. That
is, $C \sim ^{\star } D$ if, and only if, for every $t \in \mbbtp$, the numbers
$\countof (C \cap t)$ and $\countof (D \cap t)$ of that case type coincide. To
enable a surjection $D \mapsto \langle\countof (D \cap t): t \in \mbbtp \rangle$
from databases to counting vectors, we impose


% The set $\mbbt$ of case types in $\mbbc$ consists of the
% partition generated by $\sim ^{ \star }$ on $\mbbc$.
% We then denote the extended set
% of case types by $\mbbtp$.

\begin{assumption*}
For every  $t \in \mbbtp$, there are infinitely many cases in $t$.
%There are infinitely many cases in $[ \novel ]$ and in each equivalence class
%of $\sim ^ \star$ in $\mbbc$.
%and  there are at least two case
% types.
\end{assumption*}

% For each $D \in \mbbdp$, let
% $t \mapsto I _{ D } ( t ) = \countof ( D \cap t )$ denote the vector that
% counts the number of cases each case type in $D$.  As in \gsii, take
% $C, D \in \mbbdp$ to be \emph{equivalent}, written $C\sim ^ \star D$,
% if, and only if, $I _{ C } = I _{ D }$.

\paragraph{Generalizations}\hskip-5pt of sentiments $\precsimb_\mbbd$ to
$\mbbdp$ require a suitable structure. The following definition provides that
structure. It also simplifies the exposition by accommodating generalizations
that restrict attention to subsets $Y$ of $X$.
\begin{definition}\label{def-{generalization}} $\extb \defeq \langle \extb_{D}:
  D\in \dpp \rangle$ is a \emph{{generalization}} (or $Y$-{generalization}) of
  $\preceq_{\mbbd}$ if, for some nonempty $Y \subseteq X$, the following all
  hold$:$
  %such that $\hash Y = 3$ or $\hash Y = 4$,

\begin{enumerate}%[label=\textup{(\roman*)}]

\item \label{item-preserving} for every $D \in \mbbd$ and every $x,y\in Y$,
  $x \ext_{D} y$ if, and only if, $x \preceq_{D} y$,

\item\label{item-binary-rel} for every $D\in \dpp$, $\ext_{D}$ belongs
  to $\relations (Y)$,

\item \label{item-dimension} for every $D\in \dpp$ and %every $c,d \in [\novel]$,
%  $c, d \not \in D$ such that $D\cup \{c\}, D \cup \{d\} \in \dpp$, 
every $c,d \in \cpp\bs D$,  if $c \sim ^ \star d$ then
  $\extb _ { D \cup \{c\} } = \extb _ { D \cup \{d\}}$.

\end{enumerate}
A {generalization} $\ext_{\dpp}$ is \emph{proper} if $\dpp = \mbbdp$ and improper
  otherwise.
 % , for some $Y$ and
 % every $D \in \mbbd$, $\extb_{D} = \preceqb_{D}\cap Y^{2}$.
%Let $\ext ^ \star$ denote the unique (improper) {generalization}
\end{definition}
 Observe that $\preceq_\mbbd$ is an (improper)
{generalization} of itself: take $\dpp = \mbbd$ and $Y = X$.
Indeed part \ref{item-preserving} of the definition implies that, for every
$Y$-{generalization} $\ext$ and every $D\in \mbbd$, $\ext$ is simply the restriction
$\preceqb_{D}\cap Y^{2}$ of $\preceq_{D}$ to $Y$.  We therefore refer to part
\ref{item-preserving} of the definition of a {generalization} as the preservation or
\emph{nonrevision} condition.
%We will often suppress reference to $Y$ and $\preceqb _ \mbbd$ and
%refer to $\ext$ simply as a {generalization}.

Part \ref{item-binary-rel} of the definition of a {generalization} ensures that, for
every proper $Y$-{generalization} $\ext$ and every $D\in \mbbdp$, $\ext_{D}$ is a
well-defined binary relation on $Y$.  Let $\next_{D}$ and $\sext_{D}$
respectively denote the symmetric and asymmetric parts of $\ext_{D}$.


Part \ref{item-dimension} of the definition ensures that, for proper {generalization}s
$\ext$, the partition $\mbbtp$ of case types generated by $\sim^{\star}$ is at
least as fine the partition generated by the equivalence relation generated by
$\ext$. Two cases $c,d \in \mbbcp$ are \emph{equivalent with respect to
  $\ext$}, written $c \sim^{\extb} d$, if, for every $D \in \mbbd$ such
that $c,d \notin D$, $\extb _ { D \cup \{c\} } = \extb _ { D \cup\{d\} }$.
This notion allows us to partition the set of proper {generalization}s as follows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %novel vs regular

\begin{definition*}\label{def-novel}

  A proper {generalization} $\ext$ is either \emph{regular} or \emph{novel}.  It is
  novel whenever $[\novel]$ is a distinct equivalence class of $\sim^{\supext}$,
  so that, for every $c \in \mbbc$, $c \nsim ^ \supext \novel$.

\end{definition*}

% In the present model, the only novel {generalization}s that we need will involve $Y
% \subseteq X$ of cardinality $3$ or $4$.  Let $\textup{Nov} ( Y , \preceqb
% _ \mbbd )$ denote the subset of novel $Y$-{generalization}s and let its
% complement $\textup{Reg} ( Y , \preceqb _ \mbbd )$ in $\Ext ( Y ,
% \preceqb _ \mbbd )$ denote the set of regular {generalization}s.

% Spanning {generalization}s are the richest (in terms of the rankings they feature)
%feasible {generalization}s of the learner's current forecast $\preceqb _ \mbbd
%$ and as such they are the natural candidates for testing for inconsistencies.

%In the terminology of \citet{HR-Knowledge_of_unawareness}, $\novel$ is
%aplaceholder for experience that the learner cannot currently describe.
For any given novel {generalization} $\ext$, $[\novel]$ is the unique novel case type
that $\sim^{\extb}$ generates.  We impose this restriction, not because we
think the imagination of learners is constrained in this way, or because the
model would not work that way, but rather because one degree of freedom is
sufficient for our purposes. From an evolutionary perspective: {Pru} represents
a minimal deviation from Inny.

%For novel {generalization}s, $\novel$ mimmicks the potential arrival of new
%information. Yet novel {generalization}s need not feature qualitatively new rankings
%(\ie\ rankings that do not feature in $\preceq_{\mbbd}$). In \cref{lem-insep},
%we show that this is because a novel case type is characterised by the
%quantitative notion of a similarity weight.

For every regular {generalization} $\ext$, there exists $c \in \mbbc$ such that
$c \sim^{\extb} \novel$. Thus, there are as many regular {generalization}s as there are
past case types (\ie\ $\countof \mbbt$).  Yet every regular $Y$-{generalization} $\ext$
is \emph{equivalent} to the unique improper $Y$-{generalization}
$\langle \preceqb_{D} \cap Y^{2}: D \in \mbbd\rangle$ in the sense of 
\begin{theoremEnd}{observation}\label{obs-reg-eq}%
  For every regular $Y$-{generalization} $\ext$ and improper $Y$-{generalization} $\aext$, for
  every $C \in \mbbdp$, there exists $D \in \mbbd$ such that
  $C \sim^{\extb} D$ and $\extb_{C} = \aextb_{D}$.\footnote{This means that
    there is a canonical embedding of
    $\left\{C \times \extb_{C}: C \in \mbbdp\right\}$ in
    $\{D \times \aextb_{D}: D \in \mbbd\}$. The converse embedding follows from
    the nonrevision condition of \cref{def-{generalization}}.} % in the sense of
%We extend this notion of equivalence to pairs of {generalization}s.
\end{theoremEnd}

\begin{proofEnd}\label{proof-reg-eq} Fix $Y\subseteq
  X$ nonempty and $\aext$ regular.  \Wlog, take $C \in \mbbdp \bs \mbbd$, so
  that $C$ contains at least one copy of $\novel$.  For any $c \in C \cap [
    \novel ]$, the fact that $\aext$ is regular implies that $c \sim^{\aextb} c
  _ 1$ for some $c _ 1 \in \mbbc$.  The richness assumption ensures that we may
  choose $c _ 1$ from the complement of $C$.  Then, since neither $c$ nor
  $c_{1}$ belong to $C _ 1 \defeq C \bs \{c\}$, $c \sim^{\aextb} c_{1}$ implies
  $\aextb_{C} = \aextb _ { C _ 1 \cup \{c_{1}\} }$.  If $c$ is the unique
  member of $C \cap [ \novel ]$, then the proof is complete.  Otherwise, using
  the fact that $C$ is finite, we may proceed by induction until we obtain a
  set $C _ n$ such that $C _ n \cap [\novel ]$ is empty and $D \defeq C _ n
  \cup \{ c _ 1 , \dots , c _ n \}$ belongs to $\mbbd$.  Part
  \ref{item-preserving} of \cref{def-{generalization}} then implies $\aextb _ {
    D } = \preceqb _ { D } \cap Y^{2}$, so that, since $\ext$ is
  improper, $\aextb_{D}=\extb_{D}$.  Finally, since $C \sim ^{\aextb} D$,
$\aextb _ { C } = \aextb_{D}$, as required.
\end{proofEnd}
  % \Cref{obs-reg-eq} allows us to identify $\preceq_{\mbbd}$ with any regular
  % $Y$-{generalization} $\ext$ such that $Y=X$. This motivates our treatment
% of
  % $\preceq_{\mbbd}$ as an improper {generalization} of itself.

%axioms and main theorem
\section{Axioms and main results}\label{sec-axioms-theorem}
%basic axioms

\paragraph{The basic axioms\hskip-10pt}~of \gsii, which we rewrite in terms of
{generalization}s, are the following. In each of these axioms, $\ext$ is an arbitrary
$Y$-{generalization} of $\preceq_{\mbbd}$.% Via
% \cref{obs-reg-eq}, it is meaningful to say that $\preceqb _ \mbbd$
% satisfies one of the following basic axiom of \gsii\ if, and only if, some (and
% therefore every) regular $Y$-{generalization} with $Y = X$ satisfies that axiom.

\begin{taggedblank}{$\textup{A}0$}[Transitivity axiom for $\ext$]\label{T}

  For every $D \in \dpp$, $\ext_{D}$ is transitive.

  \end{taggedblank}

\begin{taggedblank}{$\textup{A}1$}[Completeness axiom  for $\ext$]\label{K}

  For every $D \in \dpp$, $\ext_{D}$ is complete.

\end{taggedblank}

\begin{taggedblank}{$\textup{A}2$}[Combination axiom  for $\ext$]\label{C}

  For every disjoint $C, D\in \dpp$ and every $x,y\in Y$, if $x \ext_{C} y$
  and $x \ext_{ D } y$, then $x \ext_{ C \cup D} y ;$ and  if $x \sext_{C}
  y$ and $x \ext_{D} y$, then $x\sext _{C \cup D } y$.

\end{taggedblank}

\begin{taggedblank}{$\textup{A}3$}[Archimedean axiom  for $\ext$]\label{A}

  For every disjoint $C,D \in \dpp$ and every $x,y\in Y$, if $x \sext_{D}
  y$, then there exists $k \in \posint$ such that, for every pairwise
  disjoint collection $\lb D _ j : \text{$ D _ j \sim ^ \supext D $ and $ C
  \cap D _ j  =\emptyset$}\rb _ 1 ^ k$ in $\dpp$, $x \sext_{ C \cup D_{1} \cup
  \cdots \cup D_{k} } y.$

\end{taggedblank}
% Moreover, if a {generalization} satisfies \ref{TQ}, we say that it is everywhere-transitive.

%We adopt this generalised form so as to accommodate our main axiom,
% {stability}.

% \footnote{Note that, in the case that $\mbbc$ is empty, the set
% $\reg ( X , \preceqb _ \mbbd )$ is also empty.}


 % diversity

\paragraph{The diversity axioms\hskip-10pt}~that now follow require that $\mbbd$
is sufficiently rich to support $Y$-{generalization}s $\ext$ with a variety of
\emph{total} orderings: \ie\ complete, transitive and antisymmetric
($x \ext_{D} y$ and $y \ext_{D} x$ implies $x = y$). %\label{sec-div-pru}
Let $\total ( \ext )$ denote the set $\lb R : \text{for some
 $ C \in \dpp $, $ R = \extb _{ C }$ is total}\rb$ of of total orders that
feature in $\ext$. For $k = 4$, the following axiom is a restatement of the
diversity axiom of \gsii.
 
\begin{taggeddiv}{\hskip-5pt}[$k$-Diversity axiom]
  For every $Y \subseteq X$ of cardinality $n= 2, \dots, k$, every regular
  $Y$-{generalization} $\ext$ of $\preceqb _{ \mbbd }$ is such that
  $\countof \total(\ext) = n!$\hskip.5pt.
\end{taggeddiv}
We say \emph{$k$-diversity holds on $Z$} if the axiom holds with $Z$ in the
place of $X$. By lowering the bar for the required number of total orders, the
following axiom substantively weakens \threediv\ and \emph{a fortiori} \fourdiv.

\begin{taggedblank}{$\textup{A}4'$}[Partial \threediv]\label{p3d}
  For every $Y\subseteq X$ with cardinality $n = 2$ or $3$, every regular
  $X$-{generalization} $\ext$ of $\preceq _{ \mbbd }$ is such that $\countof
  \total ( \ext ) \geq n$.

% ,   for some
% $C , D \in \mbbd$, $\ext _{ C }$ is total,  $\ext _{ D } = \ext _{ C }
% ^{ - 1}$. Moreover, for $\countof Y = 3$,  $\countof \total ( \ext ) \geq 3$.
\end{taggedblank}


%In \cref{sec-uniqueness}, we assume this axiom holds for $k = 2$ and derive a
%necessary and sufficient conditions for a representation that has the same
%uniqueness properties as the representation of \gsii.

\Cref{eg-rates} (and \cref{eg-zaslavski} of the appendix) provide examples of
$\preceq_{\mbbd}$ satisfying \ref{T}\ref{p3d}, but not \threediv.  For
settings where the basic axioms hold, the next observation shows that
\parthreediv\ is equivalent to

\begin{taggedblank}{\textup{A}$4$}[Conditional-\twodiv]\label{c2d}
  For every three distinct elements $x , y , z \in X$, one of the sets
  $\{ D' : x \prec _{D ' } y \}$ and $\{ D' : y \prec_{D ' } x \}$ contains 
  both $C$ and $D$ such that $z \prec _{ C } x$ and
  $x \prec _{ D } z$. If $\countof X = 2$, then \twodiv\ holds on $X$.

\end{taggedblank}


\begin{observation}\label{obs-c2d}
  For $\preceq_{\mbbd}$ satisfying \ref{T}--\ref{A}, \condtwodiv\ and
  \parthreediv\ are equivalent.
\end{observation}
\begin{proof}\label{proof-obs-c2d}
  This follows by virtue of \cref{prop-c2dQ} and the translation of
  \cref{sec-proof-main}.
\end{proof}
\Parthreediv\ is therefore stronger than \twodiv\ and, moreover, it plays the
dual role of guaranteeing the existence and uniqueness of the representation.
Via \cref{eg-lexicographic}, \parthreediv\ is weakest axiom with these
properties.
% \subsection{Stability}
\paragraph{The {stability} axiom \hskip-10pt} is our final requirement. It is
distinguished by the fact that it imposes structure on novel {generalization}s.  First
a definition that reduces the set of {generalization}s we test when establishing
{stability}.
%First two definitions that simplify both the statement of the axiom and the
%proof of the main theorem.
%As we will see in the proof of the main theorem (see
%\cref{lem-insep}), novel {generalization}s are characterised by a cardinal notion. In
%contrast, the notions of testworthiness and perturbation that we now introduce
%are ordinal in nature.
% def-testworthy

\begin{definition*}\label{def-testworthy}

  A proper {generalization} $\ext$ of $\preceqb_{\mbbd}$ is \emph{testworthy}
  if it satisfies \textup{\ref{K}--\ref{A}} and, for some $D\in \mbbd$ such
  that $\ext_{D}$ is total, $\extb _{ \novel }$ is the inverse of
  $\extb_{D}$.\footnote{Recall that the inverse $\ext _{ D } ^{ - 1 }$ of $\ext
  _{ D }$ satisfies $x \ext _{ D } ^{ - 1 } y$ if, and only if, $y \ext _{ D }
  x$.}
\end{definition*}
Testworthy {generalization}s are distinguished by the ranking at $\novel$ and
the fact that they need not satisfy \ref{T}.  Stability requires that for any
testworthy {generalization} we can find a suitable perturbation that also
satisfies \ref{T}.
\begin{definition*} Let $\ext$ and $\aext$ be {generalization}s of
  $\preceqb_{\mbbd}$. $\aext$ is a \emph{perturbation} of $\ext$ if
  $\aextb_{\novel } = \extb_{\novel}$ and, moreover, a \emph{{{diverse}}}
  perturbation if $\countof \total (\aextb) \geq \countof \total ( \extb)$.
  % \emph{perturbation} of another, $\ext$, if $\extb_{\novel }=
  % \hextb_{\novel}$. It is \emph{{{diverse}}} if, in addition, $\countof
  % \total (\extb) \leq \countof \total ( \hextb)$.
\end{definition*}
A diverse perturbation of $\ext$ does not suppress the novel, transitive
rankings that $\ext$ generates. If we allowed for nondiverse (\ie\ dogmatic)
perturbations, the learner could continue to ``hide'' intransitive rankings.
Our main axiom is then
\begin{stability*}\label{P}
  For every $Y\subseteq X$ with cardinality $3$ or $4$, every testworthy
  $Y$-{generalization} of $\preceqb _{ \mbbd }$ that is novel has a {{diverse}}
  perturbation that satisfies \ref{T}\ref{A}.
\end{stability*}
Since every proper {generalization} is either regular or novel, the next
observation confirms that \fourdiv\ holds whenever \stability\ holds vacuously;
that is, whenever there are no novel {generalization}s for us to test.
\begin{observation} \label{obs-testworthy}
  Let $\preceq_{\mbbd}$ satisfy \ref{T}\ref{c2d}. For
  every $Y\subseteq X$ of cardinality $3$ or $4$, the set of testworthy
  $Y$-{generalization}s is nonempty. If, for some $Y$, every testworthy
  $Y$-{generalization} is regular, then \fourdiv\ holds on $Y$.
\end{observation}
\begin{proof}\label{proof-obs-testworthy}
  Respectively, these two statements follow via \cref{prop-central-testworthy}
  and \cref{lem-test-empty-fourdiv}.
\end{proof}

It is natural to ask whether \stability\ is simply requiring that, on $Y$ such
that $\preceq_{\mbbj}$ fails to satisfy \fourdiv, there exists a testworthy
$Y$-{generalization} that is novel and satisfies \fourdiv. The proof of
\cref{thm-foureq} (see for instance \cref{fig-hasse-centerless-Y4}), shows
otherwise. When the basic axioms hold, \fourdiv\ implies that the current
ranking map features a centered arrangement of hyperplanes (for every
$Y\subseteq X$ of cardinality four). In contrast, \stability\ does not even
imply the existence of a centered extended ranking map.

\paragraph{Our main theorem\hskip-5pt} involves real-valued
function $\mathbf{v}$ on the product $X \times \mbbc$. We view $\mathbf{v}$ as
a matrix and $\mathbf{v}(x, \cdot)$ as one of its rows.  The matrix
$\mathbf{v}$ is a \emph{representation of $\preceq_{\mbbd}$} whenever it
satisfies \label{sec-main}
 \begin{linenomath*} 
\begin{equation}\notag\label{eq-rep-main}
  \left\{
  \begin{array}{l}
    \text{for every $x , y \in X$ and every $D \in \mbbd$,}\\
    x \preceq_{D} y \quad \text{if, and only if,} \quad \sum _ {\, c \,\in\, D}
    \mathbf{v}(x, c) \leq \sum_{\, c \,\in\, D} \mathbf{v}(y, c) .
  \end{array}\right.
\end{equation}
\end{linenomath*}
The matrix $\mathbf{v}$ \emph{respects case equivalence} (with respect to
$\preceq_{\mbbd}$) if, for every $c,d\in \mbbc$, $c \sim^{\star} d$ if, and
only if, the columns $\mathbf{v}(\cdot, c)$ and $\mathbf{v}(\cdot, d)$ are equal.
% whenever,
% it holds where, once more, $% I_{D}(t)$ counts the number of cases of type
% $t$ in
% $D$.  Finally, $v(x,\cdot),v(y,\cdot)$ and
% $v(z,\cdot)$ are \emph{affinely independent} if there is no $\lambda\in
% \R$ such that $v(x,\cdot)=\lambda v(y,\cdot)+(1-\lambda)
% v(z,\cdot)$. (Equivalently,
% $v(x,\cdot)-v(z,\cdot)$ and $v(y,\cdot)-v(z,\cdot)$ are noncollinear.)
 %existence theorem

\begin{theorem}[Part I, Existence]\label{thm-main}
  Let there be given $X$, $\mbbcp$, $\preceqb_ \mbbd$ and
  associated {generalization}s, as above, such that the richness condition holds. Then
  \ref{ax-main} and \ref{wrap-main} are equivalent.
\begin{enumerate}[label=\textup{(\ref{thm-main}.\roman*)}]
\item\label{ax-main}
  \ref{T}--\ref{c2d} and \stability\ hold for $\preceq_{\mbbd}$.
  
\item\label{wrap-main} There exists a matrix
  $\mathbf{v} : X \times \mbbc \rightarrow \R$ satisfying \ref{rep-main} and \ref{rows-main}$\,:$
  \begin{enumerate}[label=\textup{(\ref{thm-main}.\alph*)}]
  \item\label{rep-main} $\mathbf{v}$ is a representation of
    $\preceq_{\mbbd}$ that respects case equivalence$\,;$

  \item\label{rows-main} no row of $\mathbf{v}$ is dominated by
    any other row, and for every three distinct elements $x,y, z \in X$ and
    $\lambda \in \R$, $\mathbf{v} (x, \cdot) \neq \lambda
    \mathbf{v}(y,\cdot) + (1-\lambda) \mathbf{v}(z,\cdot)$\,.\footnote{Observe that
$\mathbf{v}(x,\cdot)- \mathbf{v}(z,\cdot)$ and
$\mathbf{v}(y,\cdot)-\mathbf{v}(z,\cdot)$ are noncollinear if, and only if,
the affine independence condition of \ref{rows-main} holds.}
\end{enumerate}
\end{enumerate}                 %
\end{theorem}


% uniqueness

%\Condtwodiv, \cref{c2d}, yields the following uniqueness result.
% It is the minimal condition that is not context-specific.
Our uniqueness result is identical to that of \gsii.  \setcounter{theorem}{0}
\begin{theorem}[Part II, Uniqueness]%\label{thm-uniqueness}
  If \ref{ax-main} $[$or \ref{wrap-main}$]$ holds, then the matrix
  $\mathbf{v}$ is unique in the following sense$\,:$ for every other matrix
  $\mathbf{u} : X \times \mbbc \rightarrow \R$ that represents
  $\preceqb_{\mbbd}$, there is a scalar $\lambda > 0$ and a matrix
  $\beta : X \times \mbbc \rightarrow \R$ with identical rows (\ie\ with
  constant columns) such that $\mathbf{u} = \lambda \mathbf{v} + \beta$.
  
\end{theorem}
% We discuss the counterpart to \cref{thm-main} part II of  \gsii\ in \cref{sec-reference-free}. 

%    and a matrix
%   $\beta : X \times \mbbt \rightarrow \R$ with identical rows (\ie~constant
%   columns) such that $u = \lambda v + \beta$.

% Alternatively, if there is some eventuality $x$ that ought to be independent of or 
% ``orthogonal'' to each and every past case, then we may choose $\beta$ such
% that, for every $y \in X$, $\beta ( y , \cdot ) = - v ( x , \cdot )$. Then
% we obtain a new matrix representation $u : X \times \mbbt \rightarrow \R$ such
% that $u \defeq v + \beta$. Then every matrix representation $u '$ of
% $\preceqb _{ \mbbd }$ with the property $u ' ( x , \cdot ) = 0$
% satisfies $u ' = \lambda u$ for some $\lambda > 0$.

%The proof of \cref{thm-main} appears in online appendix \ref{sec-proof-main}.
%It relies upon a translation from the abstract database/memory set up of the
%model to the setting of rational vectors similar to \gsii. We show that the
%translated \cref{thm-mainQ} (see \cref{sec-proof-main}) is equivalent to
%\cref{thm-main}. The proof of \cref{thm-mainQ} is then the subject of online
%appendix \ref{sec-proof-mainQ}.

\paragraph{A variety of statistical methods}\hskip-7pt that are related to
\cref{thm-main} are discussed in \gsii. In the next section, we provide a more
detailed application to no-arbitrage asset pricing. This application also
appeals to intermediate results in the proof of \cref{thm-main}, so let us
unpack the key steps.

The first step in the proof of \cref{thm-main} (see \cref{sec-proof-main} is to
translate all the above concepts to the setting of rational vectors indexed by
case types: as in \cref{fig-tali,fig-rates}, \ie\ vectors in
$\nnreal^\mbbt$.  \Cref{thm-mainQ} is the corresponding theorem for
rational vectors.  Definitions and intermediate results in the proof of
\cref{thm-mainQ} are the subject of \cref{sec-proof-mainQ}.  For now,
we maintain the discussion of these results in terms of samples (\ie\
databases).

In \cref{sec-proof-mainQ}, we show that \ref{K}--\ref{c2d} alone yield a more
general matrix representation than that of \cref{thm-main}. In particular, a
matrix $v^{\dd} : X^2 \times \mbbc \rightarrow \R$, $x \times y \times c
\mapsto v^{\xy}_{c}$.  For each pair $x, y \in X$, the row $v^{\xy}$ of this
matrix is a pairwise representation provided that, for every $x,y\in X$ and $D \in
\mbbd$,
\begin{equation*} \text{$x \preceqb_{D} y$ if, and only if, $\sum_{c \,\in D}v^{\xy}_{c}
\geq 0$.}\end{equation*}
(In the context of \cref{fig-tali}, the projection of each such $v^{\xy}$
onto case-type space is orthogonal to the hyperplane $H^{\{x,y\}}_{\mplus}
\subset \nnreal^\mbbt$.)

\ref{c2d} is characterised by the property that no row of $v^{\dd}$ dominates
any another; nor is any pair collinear.  The final key to the proof of
\cref{thm-main}, {stability}, and the connection with no-arbitrage pricing is
% Then, via an example, we introduce
  % some key concepts from the literature on hyperplane
% arrangements.\footnote{A
  % more detailed introduction can be found in the classic text
  % \citet{orlik1992arrangements}, the recent \citet{dimca2017hyperplane} and the
  % lecture notes \citet{stanley2007introduction}.}
\begin{definition*} %We will use the following notation.
  For $Y \in 2 ^ { X }$, $v^{ \dd } : Y^{2}\times \mbbc \rightarrow \R$
  satisfies the \emph{groupoid property} (Jacobi identity) whenever, for every
  $x , y , z \in Y$, $v ^{ \xz } = v ^{ \xy } + v ^{ \yz }$.
\end{definition*}
When the Jacobi identity holds on $X$, instead of the $\binom{\countof X}{2}$
rows of $v^{\dd}$, we only need $\countof X$ vectors in $\R^\mbbc$ to summarise
$\preceqb_\mbbd$. This is the crux of \cref{thm-main}.
  % If in addition, $v^{\dd}$ satisfies
% the Jacobi identity \emph{Jacobi identity holds for $\ext$} and $v^{\dd}$ is
% a \emph{Jacobi representation}. For $\ext$ improper, we say that the
% \emph{Jacobi identity holds on $Y$}.  In the results that follow, we work
% with
% \begin{k-jac*}

%   For every $Y \subseteq X$ such that $3 \leq \countof Y \leq k$, the Jacobi
%   identity holds on $Y$.
   
% \end{k-jac*}
%\setcounter{theorem}{2}
\begin{theoremEnd}[link to proof]{corollary}[a characterisation of
  {stability}]\label{cor-foureq} Let the number of case types be finite and let
  $\preceq_{\mbbd}$ satisfy \ref{c2d}. Then $\preceq_{\mbbd}$ satisfies
  \stability\ if, and only if, $\preceq_{\mbbd}$ has a pairwise representation
  $v^{\dd}$ that satisfies the Jacobi identity. Moreover, for every other
  pairwise representation $u^{\dd}$, there exists $\lambda >0$, such that
  $u^{\dd} = \lambda v^{\dd}$.
 \end{theoremEnd}
 \begin{proofEnd}%[Proof of \cref{cor-foureq}]
   This follows from \cref{thm-foureq}, \cref{lem-induction} and the fact that,
   via \cref{lem-test-empty-fourdiv}, \stability\ implies \ref{T}--\ref{A} when
   the number of case types is finite. \label{proof-cor-foureq}
\end{proofEnd}

This suffices for us to formalise the log-accumulation process of
\cref{eg-zeros}.

\section{Application to no-arbitrage asset pricing}\label{sec-fin-app} Fixed
income securities (henceforth \emph{bonds}) are commonly traded over the
counter with certain parties (typically investment banks) acting as market
makers. Similar to bookmakers in betting markets, market makers profit from the
bid-ask spread. Unlike bookmakers, however, they do sieze on perceived
opportunities to make money by trading with other market makers. To simplify
the exposition, we assume the market makers are \emph{fair}, so that the
bid-ask spread is zero.

Recall that a (fixed income) \emph{forward} is a contract between two parties
to exchange a bond at a given date, price and amount in the future.  Building
on \cref{eg-zeros}, let $X$ denote the set of settlement/maturity dates for
forwards associated with given zero-coupon bond (a \emph{Zero}) issued by the
same entity. In particular, take $X = \{x_0, x_1, \dots\}$ such that $x_0 = 0$
and $x_k < x_{k + 1}$ for all feasible $k$. 
\begin{remark*}[on zero-coupon bonds]
  Although coupon-paying bonds are far more common, Zeros provide a
  natural asset for studying no-arbitrage pricing
  \citep{barillas2019speculation}.  Zeros are generated by ``stripping''
  coupon-paying bonds of their coupons enroute to deriving benchmark
  zero-coupon yield curves.\footnote{Recall that Zeros are a common way to
  derive the implied yield curve. They are generated by taking a coupon-paying
  bond price and subtracting of the present value of each of its coupons. These
  present values are calculated using price of Zeros with shorter maturities
  \citep{brealey2020principles}.} Since coupon stripping is reversible,
  restricting attention to Zeros is without loss of generality.  Indeed, modulo
  surmountable complications relating to the uncertainty of the cashflows, the
  arguments that follow extend to dividend-paying stocks.
  % (In this case, via options markets, we may identify suitable benchmarks for
  % calculating the present value of dividends.)
\end{remark*}

Let $\past$ denote the (unique) current history of time-series data that are
relevant to this market for Zeros.  We generate the set $\mbbc$ of past cases
by taking each case $c \in \mbbc$ to consist of market-relevant data from a
given time interval (a block of time periods) in the past. Block resampling of
time series data was originally developed by
\citet{kunsch1989jackknife,politis1994stationary}. (For applications to
finance problems see \citet{white2000reality,harvey2021lucky}.) For the
present purposes, blocks need to be chosen so that, for any $k > 0$ and
resampling $D = \{c_1, \dots, c_k\}$, what matters is the number of repetitions
of a given case type, not the order of the cases that form the pseudo-time
series. In other words, $D \sim^\star D'$ for any permutation $D'$ of the cases
in $D$.  It is straightforward to verify that this is indeed the case for the
stationary bootstrap of \citet[section 2]{politis1994stationary}.

The free case $\novel$ has no additional structure beyond that of
\cref{sec-model,sec-main}.

Before turning to the interpretation of $\preceq_\mbbd$ in the present setting,
we introduce current market prices. From the market maker's perspective,
current prices reflect the rest of the market's view on yields to maturity
conditional on $\past$. Since we take current market prices (and rates) to be
fixed, we suppress reference to $\past$. The market maker observes past cases,
and current prices and forms a view about how prices might change when the
information changes.  The compound-interest formula for the \emph{market
accumulation process} (at current market rates) is then
\begin{linenomath*}
  \begin{equation*}a^{\xy} = \left(1+r^{\{x,y\}}\right)^{-x + y},
  \end{equation*}
\end{linenomath*}
where $r^{\{x,y\}}$ is the market's implied forward yield for the contract that
accrues interest between dates $x$ and $y$. If $x$ is later than $y$, then the
contract is to sell, and the market maker pays this yield, so that $r^{\{y,x\}}
= r^{\{x,y\}}$. For each $x \in X$, the \emph{market price} of a Zero that pays
out one-dollar at date $x$ is defined as $b(x) \defeq a^{(x, 0)}$.

Let $X \subseteq \R_{\mplus}$ index a suitable sequence of trading dates with
$0 \in X$ being the spot date.  It is well known that a Zero that pays out one
dollar at time $x > 0$ is arbitrage-free if, and only if, the log-accumulation
process satisfies
\begin{linenomath*}
\begin{equation}\label{eq-no-arbitrage}
    \text{for every $x,y,z \in X$,}\quad \log a^{\xy} = \log a^{\xz} + \log
    a^{\zy} \,.\footnote{To see this, \withoutlog\ suppose that, for some $x <
    y < z$, $a^{\xz} < a^{\xy}a^{\yz}$.  The market maker would do well to sell
    the contract $\xz$ and buy the contracts $\xy$ and $\yz$. In the absence of
    counterparty risk, the difference between interest paid and received is 
    risk-free.}
  \end{equation}
\end{linenomath*}
This no-arbitrage condition is a special case of the Jacobi identity.

We now explain how a market maker might infer her own subjective accumulation
process from past cases. Our market maker thinks in terms of economic profits
(relative to the market price). For every finite resampling $D$ and every date
$x$ and $y$, let $x \preceq_{D} y$ if, and only if, at $D$, the market maker
finds $y$ (weakly) more plausible than $x$ in answer to the question
\begin{quote}
% ``At $D$, buying the forward contract that settles at date $x$ and matures at
% date $y$ will yield a higher return than selling.''
 ``Hold current market prices fixed and consider a one-dollar investment today.
  Given $D$, which maturity will yield a higher return?''
\end{quote}
% ``Would you rather buy or sell the forward that settles at date $x$ and
% matures at date $y$?

\begin{remark*}[alternative interpretation]
It is also possible to interpret $\preceq_\mbbd$ in terms of statements of the
  market maker's intention to buy or sell forwards. Let $(x, y)$ be shorthand
  for the forward contract where the buyer accumulates interest between dates
  $x$ and $y$. For $x < y$, 
%accumulating
%interest over the interval of time $(x, y)$ means buying $(x, y)$; since
%market
%making is fair,
accumulating interest over $(y, x)$ simply means selling $(x, y)$.  Holding
  current prices fixed, for each $D$ in $ \mbbd$, we have $x \preceq_{D} y$ if,
  and only if, given $D$, the market maker would buy $(x, y)$. Under this
  interpretation, if at $\past$ the market maker agrees with the market, then
  $x \simeq_{\past} y$ for every $x, y \in X$. Thus, if the market maker agrees
  with the market, then her {sentiments} are centered as in \cref{fig-tali}.
  Our point is that the market maker may not agree with the market and thus the
  rankings map may be uncentered as in \cref{fig-rates}.  To operationalise
  this interpretation, simply suppress reference to $a^{\dd}$ in
  what follows.%: that is, let $a^{\xy} = 1$ for every $x,y \in X$.
\end{remark*}

Next, we introduce the market maker's (possibly negative and subjective) markup
function. This is a markup relative to current rates $r^{\{\cdot, \cdot\}}$.  A
\emph{markup function} $\mu : X^2 \times \mbbc \rightarrow \R$, is
characterised by three conditions: for time intervals of length zero, the yield
is zero; fair pricing; and case equivalence.  These are, respectively,
formalised as follows: for every $x,y\in X$ and every $c , d\in \mbbc$,
$\mu^{\{x,x\}}_{c} = 0$; $\mu^{\{y,x\}}_{c} = \mu^{\{x,y\}}_{c}$; and $c
\sim^{\star} d$ if, and only if, $\mu^{\{x,y\}}_{c} = \mu^{\{x,y\}}_{d}$.  We
extend to $\mbbd$ by taking $\mu^{\{x, y\}}_{D}$ to be the (geometric)
\emph{mean markup conditional on $D$}
\begin{equation*} 1 + \mu^{\{x, y\}}_{D}
= \prod_{c\,\in D} \left(1 + \mu^{\{x, y\}}_{c}\right)^{\frac{1}{\lvert D
  \rvert}}.
\end{equation*}
The market maker's subjective forward yield conditional on $D$ is then
the following modification of the market yield $r^{\{\cdot,\cdot\}}$:
for every $x,y\in X$ and $D \in \mbbd$
\begin{equation*}
  1 + \rho^{\{x,y\}}_{D} = \left(1 + r^{\{x,y\}}\right)\cdot\left(1 +
  \mu^{\{x,y\}}_{D}\right).
\end{equation*}

In turn, the market maker's subjective accumulation process ${A^{\dd}}: X^2
\times \mbbd \rightarrow \R$ modifies the market accumulation process
$a^{\dd}$. For every $x, y \in X$ and $D \in\mbbd$,
\begin{equation*}
  {A}^{\xy}_{D} := \left(1 + \rho^{\{x,y\}}_{D}\right)^{-x + y} = a^{\xy} \cdot
  \left(1 + \mu^{\{x, y\}}_{D}\right)^{-x + y}.
\end{equation*}
Note that the market maker agrees with the current market accumulation process
whenever the cases in $\past$ are such that the positive markups countervail
those that are negative. That is, for every $x, y \in X$, ${\mu}^{\xy}_{\past}
= 0$.

The market maker's (subjective) empirical bond price function $B : X \times
\mbbd \rightarrow \R$ modifies the market price $b: X \rightarrow \R$. Thus,
for a Zero with a one-dollar face value the price at time $x$, conditional on
$D$ is 
\begin{equation*} B(x,D) := {A}^{(x, 0)}_{D} =
  b(x) \cdot \left( 1 + \mu^{\{0,x\}}_{D} \right)^{-x} .
\end{equation*}
This reflects the inverse relationship between bond prices and yields.  

When $D^{\star}$ belongs to $\mbbd$, the number of case types is finite, the
following corollary of \cref{thm-main} characterises \stability\ in the
Zero-market setting.
\begin{corollary}\label{cor-bond-rep}
  Let $\preceq_{\mbbd}$ satisfy \ref{c2d}. Then $\preceq_{\mbbd}$ satisfies
  \stability\ if, and only if, there exists empirical implied yield and empirical
  bond price functions, such that
  \begin{linenomath*} 
    \begin{equation}\tag{$*$}\label{eq-bond-rep}
 \left\{
  \begin{array}{l}
    \text{for every $x , y \in X$ and every $D \in \mbbd$,}\\
      x \preceq_{D} y \quad \text{if, and only if,}\quad B(x,D) \geq B(y,D).
  \end{array}\right.
      \end{equation}
    \end{linenomath*} 
    Moreover, for every other empirical bond price function $\acute{B}$ that
    satisfies \eqref{eq-bond-rep}, there exists a scalar $\lambda > 0$ such
    that $\log \acute{B} = \lambda \log B\,;$ and, for every $D\in \mbbd$, the
    associated accumulation process ${A}^{\dd}_{D}$ is arbitrage-free.

\end{corollary}
\begin{proof}[Proof of \cref{cor-bond-rep}]
  Via \cref{cor-foureq}, $\preceq_{\mbbd}$ satisfies \stability\ if, and only
  if, there exists a pairwise Jacobi representation $v^{\dd}: X^{2} \times
  \mbbc \rightarrow \R$.
% For each $x, y \in X$ and $c\in
% \mbbc$ let 
% \begin{equation*}\log {A}^{\xy}_{c} = v^{\xy}_c.\end{equation*}
  Recalling that $0 \in X$, for every $x\in X$ and $D \in \mbbd$, let
  \begin{linenomath*}
    \begin{equation}\label{eq-bond-vxy}
      \textstyle B(x,D) = \exp\left( -\frac{1}{\lvert D \rvert} \sum_{c\,\in D}
      v^{(0, x)}_{c} \right).
      \end{equation}
\end{linenomath*}
For the proof of \eqref{eq-bond-rep}, suppose that \withoutlog, $x \preceq_{D}
  y$, so that $\sum_{c\,\in D} v^{\yx}_{c} \leq 0$. Via the Jacobi identity we
  have $ \sum_{c\,\in D} \left\{ v^{(y,0)} + v^{(0,x)} \right\} \leq 0$.  Via
  the representation property $v^{(y, y)} = 0$.  Another application of the
  Jacobi identity yields $v^{(y, 0)} = -v^{(0, y)}$. Thus, 
\begin{linenomath*}
  \begin{equation*}\textstyle - \log B(x,D) + \log B(y,D) = \frac{1}{\lvert
    D\rvert}
  \sum_{c\,\in D} \left \{v^{(0, x)}_{c} - v^{(0, y)}_{c}\right \} \leq 0.
  \end{equation*}
\end{linenomath*}

We now show that the bond price is a suitable function of the empirical yield
  function.  For every $x,y \in X$ and $D \in \mbbd$, take $\log
  {A}^{\xy}_{D} = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D}v^{\xy}_{c}$. 
  That is,
  \begin{equation*}\log a^{\xy} +
  \tfrac{y - x}{\lvert D\rvert} \sum_{c\,\in D}\log
  \left(1+\mu^{\{x,y\}}_{c}\right) = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D} v^{\xy}_{c}. \end{equation*}
Take $D = \{c\}$ and note that the Jacobi identity implies $v^{\xy}_c = -
  v^{\yx}_c$. Thus
  \begin{equation*}\log a^{\xy} + (y - x) \log \left(1+\mu^{\{x,y\}}_{c}\right) = -\log
  a^{\yx} - (x - y) \log \left(1+\mu^{\{y,x\}}_{c}\right). \end{equation*}
Moreover, note
  that $\log a^{\xy} = -\log a^{\yx}$, so that, by cancelling terms, we obtain
  \begin{equation}\label{eq-mu-sym} \log \left(1+\mu^{\{x,y\}}_{c}\right)
    = \log \left(1+\mu^{\{y,x\}}_{c}\right),
  \end{equation}
so that $\mu^{\{x,y\}}_{c} = \mu^{\{y,x\}}_{c}$.
  Then $v^{\xx}_{c} = 0 = \log a^{\xx}_{c} $ implies that $\mu^{\{x,x\}}_{c} = 0$.
 Finally, note that for $c \sim^{\star} d$, the property $\mu^{\{x,y\}}_{c} =
 \mu^{\{x,y\}}_{d}$ is inherited from $v^{\xy}_{c} = v^{\xy}_{d}$, so that we
 have an empirical implied yield function. All of the above arguments are
  reversible, so that, given the existence of such empirical yield and bond
  price functions satisfying \eqref{eq-bond-rep}, it follows that
  $\preceq_{\mbbd}$ satisfies \stability.

If $\acute{B}$ is another empirical bond price function that satisfies
  \eqref{eq-bond-rep}, then via \cref{eq-bond-vxy} and \cref{cor-foureq}, for
  some $\lambda >0$, $\log \acute{B} = \lambda \log B$.  The fact that, for
  every $D$, ${A}^{\dd}_{D}$ is arbitrage-free follows by virtue of the fact
  that the Jacobi identity holds element-wise for $v^{\dd}$.  In particular,
  since, for every $x, y , z \in X$ and $c \in D$, $\log {A}^{\xy}_{c} =
  v^{\xy}_{c}$, 

  \begin{equation*}
    \log {A}^{\xy}_{D} = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D}\log
    {A}^{\xy}_c
%- \sum_{c\,\in
%D} v^{\xy}_c = - \sum_{c\,\in D} \left\{ v^{\xz}_c + v^{\zy}_c \right\} =
  = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D} \left\{ \log {A}^{\xz}_c + \log
    {A}^{\zy}_c \right\}.
  \end{equation*} 
Taking exponents, we obtain the no-arbitrage condition ${A}^{\xy}_{D} =
{A}^{\xz}_{D} \cdot {A}^{\zy}_{D}$.
\end{proof}

***prop-fourdiv-empty here ***

The uniqueness result in \cref{cor-bond-rep} is stronger than in the general
setting of part II of \cref{thm-main}. This is a consequence of the fact that
our empirical bond yield satisfies the property $r^{\xx} = 0 = \mu^{\xx}$, for
every $x \in X$. 

\paragraph{Our diversity axiom, \ref{c2d},}\hskip-6pt has a straightforward
interpretation in the present setting. Given \stability, \twodiv\ implies that,
for every distinct $x$ and $y$, there exist $c$ and $d$ such that $v^{\xy}_{c}
< 0 < v^{\xy}_{d}$.  This, via the arguments that lead to \cref{eq-mu-sym}, is
equivalent to both $\mu^{\{x,y\}}_{d} < 0 < \mu^{\{x,y\}}_{c}$ and
$\rho^{\{x,y\}}_{c} < r^{\{x,y\}} < \rho^{\{x,y\}}_{d}$.  In words, \twodiv\
requires that the market maker's data is rich enough to contain at least one
case where her markup between date $x$ and $y$ is positive, and at least one
where it is negative.
% \footnote{The affine independence part of \ref{}translates as: there is no
  % $\lambda \in \R$ such that, for every past case $c\in \mbbc$,
  % $B(x, \{c\}) \neq B(y,\{c\})^{\lambda} B(z,\{c\})^{1-\lambda}$.} 
\Condtwodiv\ {generalizes} this notion to require that for every distinct $x,
y, z$, $\mu^{\{x,y\}}_{C} < 0 < \mu^{\{x,y\}}_{D}$ for some $C$ and $D$ such
that $\mu^{\{x,z\}}_{C}\cdot \mu^{\{x,z\}}_{D} >0$. An equivalent
characterisation is also available in terms of the affine independence
condition \ref{rows-main} of \cref{thm-main} for the matrix $\beta: X \times
\mbbc \rightarrow \R$, $x\times c \mapsto \beta(x, c) = B(x, \{c\})$.

The considerably stronger conditional-\emph{3}-diversity arises (implicitly) in
the final step of our proof of \cref{thm-foureq}. When it holds for
$\preceq_{\mbbd}$, {stability} is unnecessary (provided \ref{T}--\ref{A} hold).
In terms of mark ups, we may characterise conditional-\emph{3}-diversity as,
for every distinct $x, y, z$ and $w$, one of the half spaces $\{D :
\mu^{\{x,w\}}_D > 0\}$ or $\{D: \mu^{\{x,w\}}_D < 0\}$ contains $D_1, \dots,
D_6$ and permutations $\pi_1, \dots, \pi_6$ of $x, y$ and $z$ such that for
$i=1,\dots,6$, 
\begin{equation*}
  \mu^{\{\pi_i(x),0\}}_{D_i} < \mu^{\{\pi_i(y),0\}}_{D_i} <
  \mu^{\{\pi_i(z),0\}}_{D_i}.
\end{equation*}
Yet \fourdiv\ is stronger still, requiring the above to hold on both half
spaces.

This disparity between \condtwodiv\ and \fourdiv\ reflects the value of
experience or of rich data. It also reflects the value of a prudent imagination
when the data fails to be rich.  Outside of market settings, this may be as far
as we can go, but in the present context we can say more.

\paragraph{Market efficiency} We begin with a proposition that justifies our
claim that when \stability\ holds, so will the efficient markets hypothesis in
the usual \citep{fama1970efficient} sense: passive (buy-and-hold)
strategies outperform active ones.  To this end, let $\beta : X \times \mbbdp
\rightarrow \nnreal$ be an empirical bond price function such that its
restriction to $X \times \mbbd$ coincides with $B$ of \cref{cor-bond-rep} and
let
\begin{equation*}
    %\frac{\lvert \Delta_c B(x, D)\rvert}{B(x, D)} 
  g_c(x, D) := \frac{\beta(x, D\cup \{c\}) - \beta(x, D)} {\beta(x, D)} 
\end{equation*}
 denote the \emph{proportional price increment} for the maturity date $x$ given
 the data $D$, following the arrival of a new case $c$.  By new case, we mean
 that $c$ can be regular or novel.  Under stable pricing, the proportional
 price increment converges to zero exponentially.
\begin{proposition}\label{prop-stability}
  For every $x \in X$, $D \in \mbbd$ of cardinality $n$ and $c \in \mbbcp \bs
  D$,
  \begin{equation*}
    1 + g_c(x, D) = \left(\left(1 + \rho_{c}^{\{0,x\}}\right)^{-x} / B(x,
    D)\right) ^ {\frac{1}{n+1}} \leq (1 + \rho)^{\frac{2 x}{n+1}}
  \end{equation*}
  where $\rho = \argmax \left\{\left(1 + \rho_d^{\{0, x\}}\right)^{-x} : d
  \in D \cup \{c\}\right\}$.% and $\xi= \max X$.
\end{proposition}
\begin{proof}
  Let $1, \dots, n$ be an enumeration of $D$ and identify $c$ with $n+1$.
  Moreover, let us suppress reference to $\{0, x\}$, so that $\rho_{n+1} =
  \rho_{c}^{\{0, x\}}$. We first derive the equality. By \cref{cor-bond-rep},
  and the fact that $\beta$ agrees with $B$ on $X \times \mbbd$, we obtain 
  \begin{equation*} \beta(x, D) = B(x, D) =
  \prod_{i}^{n} \left(1 + \rho_{i}\right)^\frac{-x}{n} .
  \end{equation*}
    and, by manipulating exponents, we obtain
  \begin{equation*}\beta(x, D \cup \{c\}) =
  \left(\prod_{i}^{n+1} \left(1 +
    \rho_{i}\right)^\frac{-x}{n}\right)^\frac{n}{n+1}.
  \end{equation*}
    Substituting for $B(x, D)$ and noting that $\frac{n}{n+1} = 1 -
    \frac{1}{n+1}$, we arrive at the expression
    \begin{equation*}
      1 + g_c(x, D) = \frac{B(x, D)^{1 - \frac{1}{n+1}} \cdot \left (1 +
      \rho_{n+1} \right) ^\frac{-x}{n+1}}{B(x, D)}.  
    \end{equation*}
  For the inequality, note that $B(x, D)^{-1} = \exp(\frac{1}{n}\sum_{1}^{n}
  \nu_i)$ where $\nu_i = x\log(1+\rho_i)$. Let $\nu_{n+1} = x \log(1 +
  \rho_{n+1})$.  Then, for $\nu := \max\{\lvert \nu_i \rvert : i = 1, \dots,
  n+1\}$, we have
  \begin{equation*}
 \frac{1}{n}\sum_{1}^{n} \nu_i \leq \left \lvert \frac{1}{n}\sum_{1}^{n}
    \nu_i\right \rvert \leq \frac{1}{n} \sum_1^n \lvert \nu_i \rvert \leq \nu =
    x \log (1 + \rho).
  \end{equation*}
 Then, since $\exp$ is an increasing function,
  \begin{equation*}
    B(x, D)^{-1} \leq \exp (\nu) = (1 + \rho) ^ {x}
  \end{equation*}
  Mutatis mutandis, the same argument yields $(1 + \rho_{n+1})^{-x} \leq (1 +
  \rho)^{x}$. Extending this pair of bounds extend to the product brings us to
  the desired inequality. 
\end{proof}

Key to the proof of \cref{prop-stability} is that the new pricing kernel
$\beta$ coincides with the old one $B$ on $X \times \mbbd$.
\Cref{prop-stability} thus demonstrates that \stability\ implies the usual
notion of stability of statistical learning
\citep{poggio2004general,mukherjee2006learning,bousquet2002stability}. That is
to say it implies continuity of the learning map (here induced by the ranking
map) $L:\bigsqcup_{i \geq 1} Z^n \rightarrow \mc H$ from the sample space to
the hypothesis space \citep{mukherjee2006learning}.  

The following corollary of \cref{prop-stability} confirms that, for stable
pricing kernels, the impact of the arrival of a given case type is decreasing
in its frequency.  This is a decreasing marginal impact of information
property: the information carried by a common case type is already baked into
the price.  We discuss the impact of this result on market efficiency following
the statement and proof.
\begin{corollary}\label{cor-efficiency}
  For $D \in \mbbd$ of cardinality $n$, let $\mbbt_D$ be the set of case types
  that feature in $D$ and let $c, d \in \mbbc \bs D$ be cases of type $s \in
  \mbbt_D$. % such that $D\cap s \neq \emptyset$.
  Then, for every $x \in X$,
  \begin{equation*}
    \lvert g_{d}(x, D\cup \{c\}) \rvert < \lvert g_c(x, D)\rvert
  \end{equation*}
  and, moreover, if $n_s$ is the frequency of $s$ in $D$, then $g_c(x, D)$
  tends to $0$ as $n_s \rightarrow n$. 
\end{corollary}
\begin{proof}
  %\Wlog\ suppose that, for every $t \in \mbbt$, $n_t = \countof (D \cap t) >
  %0$. 
  Take $\gamma_c = g_c(x, D)$ and $\gamma_d = g_d(x, D\cup\{c\})$.  For any
  $c'$ of type $t$, let $\rho_t = \rho_{c'}^{\{0, x\}}$.  We claim that
  \begin{equation}\label{eq-ppi-types}
    1 + \gamma_c = \left(\left(1 + \rho_s\right)^{n - n_s} / \prod_{t \neq s}
    \left(1 + \rho_t\right)^{n_t} \right)^\frac{-x}{n(n+1)}.
% \xrightarrow[n_s\to \,n]{} 1
  \end{equation}
  First note that, since, for each $t \in \mbbt_D$, $D$ contains $n_t > 0$
  cases of type $t$, we have 
\begin{equation*}
  B(x, D) = \left(\prod_{\,\,\, t \,\in\, \mbbt_D} \left(1 +
  \rho_t\right)^{n_t}\right)^\frac{-x}{n} = \left(\prod_{t \neq s} \left(1 +
  \rho_t\right)^{n_t} \cdot \left(1 + \rho_s\right) ^{n_s}\right)^\frac{-x}{n} 
\end{equation*}
  Then a substitution for $B(x, D)$ in the expression for $1 + \gamma_c$ of
  \cref{prop-stability} followed by a straightforward manipulation of exponents
  yields \cref{eq-ppi-types}.

  Let $\theta$ denote the main ratio (inside the brackets) of
  \cref{eq-ppi-types}.  For convergence of $\gamma_c$ to $0$ note that $n_s
  \rightarrow n$ implies that, for every $t \neq s$, $n_t \rightarrow 0$. Thus
  both the numerator and the denominator of $\theta$ tend to one and
  $\gamma_c \rightarrow 0$.

  For monotonicity, since $d$ also belongs to $s$, we extend
  \cref{eq-ppi-types} to obtain
  \begin{equation}\label{eq-ppi-types-2}
    1 + \gamma_d = \left(\left(1 + \rho_s\right)^{(n + 1) - (n_s + 1)} /
    \prod_{t \neq s} \left(1 + \rho_t\right)^{n_t}
    \right)^\frac{-x}{(n+1)(n+2)}.
  \end{equation}
  Note that the exponent of the main numerator in \cref{eq-ppi-types-2}
  simplifies to $n - n_s$. Thus, the main ratio in \cref{eq-ppi-types-2} is
  also equal to $\theta$.  In both \cref{eq-ppi-types} and
  \cref{eq-ppi-types-2}, the exponent involving $x$ is negative, so that, via
  the connection with $\theta$, 
  \begin{equation}\label{eq-theta-equiv}
    \text{$\gamma_c < 0$ \quad iff \quad $\theta > 1$ \quad iff \quad
    $\gamma_d < 0$.}
  \end{equation}
 
  Combining \cref{eq-ppi-types} and \cref{eq-ppi-types-2}, substituting
  $\theta$ and simplifying the exponent:
  \begin{equation}\label{eq-ppi-ratio}
    \frac{1 + \gamma_d}{1 + \gamma_c} = \theta^{2x \cdot \frac{n+1}{k}},
  \end{equation}
  where $k = n(n+1)^2(n+2)$. Since the exponent here is positive, we arrive at
  \begin{equation}\label{eq-theta-equiv-2}
    \text{$\frac{1 + \gamma_d}{1 + \gamma_c} < 1$ \quad iff \quad $\theta <
    1$.}
  \end{equation}
  Via \cref{eq-theta-equiv}, there are two cases to consider. The most obvious
  is $\gamma_d, \gamma_c > 0$, for then $\theta < 1$ and $\gamma_d < \gamma_c$
  follows from \cref{eq-theta-equiv-2}. The remaining case is $\gamma_c,
  \gamma_d < 0$ so that, via \cref{eq-theta-equiv}, $\theta > 1$. Then, since
  $1 + \gamma_d = 1 - \lvert \gamma_d \rvert$ (and, mutatis mutandis, the same
  is true of $\gamma_c$), via \cref{eq-theta-equiv-2}, we arrive at
  \begin{equation*}
    \text{$\frac{1 - \lvert \gamma_d \rvert}{1 - \lvert \gamma_c \rvert} > 1$
    \quad iff \quad $\theta > 1$.}
  \end{equation*}
A simple rearrangement then yields the desired inequality.
\end{proof}
In terms of market efficiency, implies that, if we assume that, more often than
not, the future resembles the past in the sense that past cases with a higher
frequency are more likely to continue to arrive more frequently,
\cref{cor-efficiency} means that the most likely cases are crowded out by the
volume other cases of the same type.  When the pricing kernel is stable, the
only way an active strategy can exploit the information in past cases is to
more accurately predict the type of the case. But, since the marginal return to
the most frequent cases is diminishing, this will be an uphill struggle.

Yet \cref{cor-efficiency} goes further. It tells us that there is value in
diversity, value to the information discovery process, for novel case types are
more rewarding. This explains why financial institutions spend vast resources
on research. When the price kernel is stable, the largest price movements come
out of ``left field''.  This frequentist idea of market efficiency allows
research and active strategies to co-exist with passive buy-and-hold. Most of
the time buy-and-hold is likely to be better, but if one can also discover the
nature of less frequent, novel cases prior to their arrival, one can improve by
actively researching the novel case generation process.  In this way, an active
imagination and hard work can help an entrepreneurial market maker to go beyond
prudence. (And also beyond the formal scope of this paper.) 

With unstable pricing kernels, the story is very different, whilst the yield
accumulation process $A = \{A^{\xy}_D : x, y \in X, D \in \mbbd\}$ is
well-defined, since the groupoid property fails to hold, the pricing kernel is
not. Whilst it is mathematically possible to derive equivalent statements to
\cref{prop-stability} and \cref{cor-stability} for $A$, it is somewhat
meaningless in the presence of the resulting arbitrage opportunities that are
present and likely to cause instability to $A$.  Instead, the value of $A$ is
that we can study out-of-equilibrium market activity in the absence of a
well-defined pricing kernel (in the sense of \cref{prop-bond-rep}). To our
knowledge, this feature is absent in the literature.


\Citet[p.60]{malkiel2003efficient} points out that ``Markets can be
efficient even if many market participants are quite irrational.'' Here, we can
show that markets can be efficient, in the sense that the pricing kernel is
stable (and thus acting as if it were guided by a prudent market maker), even
if no agent is prudent. Given the above discussion, this possibility should be
clear when the data is rich.  But what if the data fails to be
\emph{4}-diverse?

Throughout the following, we suppose that \condtwodiv\ holds. For in its
absence, uniqueness and existence are not guaranteed.

Recall that in our derivation above, we only assumed the market's accumulation
and bond price were available conditional on the current set $\past$. Suppose
the analyst is able to extend the market accumulation process and condition on
any finite sample and, moreover, is able to identify an arbitrage-free bond
price $b: X \times \mbbd \rightarrow \R$. (Below we provide a rudimentary
algorithm for this {generalization}.) Then our results say that the market bond
price induces a rankings map $\preceq_{\mbbd}$ that satisfies \stability.
Prudent pricing emerges from market activity even in the absence of prudent
agents.

This inductive notion of market efficiency differs from the more Bayesian and
forward-looking definitions of \citet{fama1970efficient} and
\citet{malkiel2003efficient}. It is not a form of weak efficiency, for we
may define cases to include more than historical data on prices. We may include
features such as firm size or price-earnings data or indeed any of the many
other factors that are extensively discussed in the literature
\citep{fama2015five,harvey2021lucky,gu2020empirical}.


%Prudent pricing in the absence of prudent market makers is on the one hand
%profound. Recall the neuroscientific evidence of a vital role for the
%imagination in prediction not only in humans (but also rats).  Prudent pricing
%represents a ``Deus ex machina'' result.  It says that markets and by
%{generalization} machines can produce behaviour that is indistinguishable from
%an imaginative, prudent market maker.
%

In our view, the fact that, in market settings, stable pricing can arise even
in the absence of both rich data (\fourdiv) and prudent agents is a tribute to
the power of markets: it is an instance of Adam Smith's invisible hand at its
best.  
While the threat of arbitrage may supplant the imagination and provide
an alternative form of external validation, stability also provides a way
to identify or ``memorize'' past cases.
This is a more operational, non-behavioural form of efficiency.

\paragraph{Structural breaks and second-order induction.} Of course some new
types of cases might for good reason require a re-evaluation of past ones.
Such cases lead to the formation of a new rankings map
$\grave{\preceq}_{\mbbd}$. We would expect such cases to be less common than
other forms of novel case since they represent structural breaks or regime
changes.  Agents that are sufficiently imaginative or privately informed about
such cases may be able to profit from the associated upheaval once they arrive.
But novel cases that are ``independent'' of past cases (in the sense that they
do not cause a re-evaluation) should not generate arbitrage opportunities.
This is the essence of stability.

\paragraph{Algorithm for identifying the pricing kernel from past cases} Under
stable pricing, cases combine in a modular way, markets assimilate the
information in new types of cases without re-pricing the information of old
case. This is the role of the nonrevision property of {generalization}s (see
\cref{def-{generalization}}).  Price movements reflect the marginal information
of new cases. 

We now describe a classification method for deciding the nature of a new case
and the implications for market maker(s) who themselves act as analysts.
Consider the arrival of $j$ new cases ${c}_1, \dots, {c}_j$.  Under stable
pricing, the analyst takes new price movements to reflect the market view on
the information value of new cases.  That is, for each $i = 1,\dots, j$, the
analyst estimates the markup associated with case $c_i$ to be
$\hat{\mu}^{\{x,y\}} = b(x, \past \cup \{c_i\}) - b(x, \past) $.  This is
reasonable provided she already has used the history of prices to estimate
$b:X\times \mbbd \rightarrow \R$. Moreover suppose that, for each $i = 1,
\dots, k-1$, there exists $d\in \past$ such that for every $x,y \in X$,
$\hat{\mu}^{\{x,y\}}_{c_i} = \mu^{\{x,y\}}_{d}$.  She concludes that each of
the cases $c_1, \dots, c_{k-1}$ is a ``copy'' of some past case type. Then
$\past \cup \{c_1, \dots, c_{k-1}\}$ belongs to $\mbbd$ and the analyst's
market model is unchanged: all that has changed is the location of the current
sample $\past$ in $\mbbd$.

Case $c_{k}$ is different. The analyst finds that price movements are such
that, for every $d \in \past$ (and hence every case in $\mbbc$),
$\hat{\mu}^{\{\cdot,\cdot\}}_{c_{k}} \neq \mu^{\{\cdot,\cdot\}}_{d}$.  This
reveals $c_{k}$ to be a new type of case.  The analyst then {generalizes} her model
of the market to let $\grave{\mbbc}$ and $\grave{\mbbd}$ to respectively be the
new sets of all cases and finite resamplings that she can generate from
$\grave{D} = \past \cup \{c_{k}\}$. Assuming no-arbitrage/prudent pricing,
there no need for her to update $\mu^{\{\cdot,\cdot\}}_{d}$ for $d \in \past$.
That is, the new market pricing function $\grave{b}: X \times \grave{\mbbd}
\rightarrow \R$ satisfies $\grave{b}(\cdot, D) = b(\cdot, D)$ for every $D \in
\mbbd$. It is in this sense that markets deliver efficiency via prudent
pricing.

Now consider two possibilities for $c_{k+1},\dots,c_{j}$. The first is that the
price movements associated with all of these cases are similar to cases in
$\grave{\mbbd}$. That is, for each $i=k+1,\dots,j$, there exists $d\in
\grave{D}$ such that $\hat{\mu}^{\{\cdot,\cdot\}}_{c_{i}} =
\grave{\mu}^{\{\cdot,\cdot\}}_{d}$. This is what we would expect if new case
types arrive infrequently. It would represent a consolidation of her updated
model $\grave{b}$ and $\grave{\preceq}_{\grave{\mbbd}}$.

The second possibility is where many of the cases $c_{k},\dots,c_{j}$ turn out
to be novel.  If new case types do indeed arrive infrequently, then it is
likely to reflect a re-evaluation of cases in $\past$ and a structural break
from the past. In this scenario, it may well be worth checking to see if a
re-evaluation or even re-specification of past cases reduces the number of case
types: thus generating a more parsimonious model.



%\begin{remark*}
% The same market structure also matches the profile of many growth stocks.
% Most famously, between its initial floatation in 1986 and early 2003,
% Microsoft paid no dividends and was, for many years, the most liquid options
% market on the Nasdaq Stock Exchange.
% % Until 2003, Microsoft options market
% % makers traded in an open-outcry ``pit'' on the Pacific Stock (and options)
% % Exchange in San Fransisco.
%\end{remark*}


\section{Discussion of the axioms and \cref{thm-main}}\label{sec-discussion} We
begin by restating the existence part of the main theorem of \gsii.
%\setcounter{theorem}{-1}
\begin{theorem*}[\gsii, existence]\label{thm-gsii}
  Let there be given $X$, $\mbbc$ and $\preceqb_ \mbbd$, as above, such that
  the richness condition holds. Then \ref{ax-gsii} and \ref{wrap-gsii} are
  equivalent.

\begin{enumerate}[label=\textup{(\roman*)}]

\item\label{ax-gsii}

  \textup{\ref{T}--\ref{A}} and \textup{\fourdiv} hold for $\preceq_{\mbbd}$.

\item\label{wrap-gsii} There exists a matrix
  $\mathbf{v} : X \times \mbbc \rightarrow \R$ satisfying \ref{rep-gsii} and \ref{rows-gsii}$\,:$
  \begin{enumerate}[label=\textup{(\alph*)}]
  \item\label{rep-gsii}
  $\mathbf{v}$ is a representation of $\preceq _ { \mbbd }$ that respects case equivalence$\,;$

\item\label{rows-gsii} For every four distinct elements $x,y,z,w \in X$ and
  every $\lambda , \mu, \theta \in \R$ such that $\lambda +\mu + \theta = 1$,
      $\mathbf{v}(x,\cdot ) \not \leq \lambda \mathbf{v}(y,\cdot )+\mu
      \mathbf{v}(z,\cdot)+ \theta \mathbf{v}(w,\cdot)$.  For $\countof X < 4$,
      no row is dominated by an affine combination of other rows.
\end{enumerate} 

\end{enumerate}
\end{theorem*}

Although diversity axioms play an important technical role, they are not
obviously behavioral. Instead, diversity axioms impose restrictions on what is
beyond the learner's control and on what is central to inductive inference:
experience. We contend that $\past$ may not be so rich as to support
$\preceqb_{\mbbd}$ satisfying \fourdiv. That is to say, there may exist $Y
\subseteq X$ such that $\countof Y = 4$, and such that the data is
insufficiently rich to support all $4 ! = 24$ strict rankings.

A casual comparison of condition \ref{rows-main} and \ref{rows-gsii} confirms
that the present framework accommodates the less experienced or equivalently
those settings where the data is not rich.  By doing so, we have identified an
important role for the imagination and in particular, a prudent imagination.
Our results show how inexperienced learners that are prudent can survive the
initial phase before going on to become experienced learners in their own
right.

We have shown that, even in the absence of prudent learners, provided the
market structure allows agents to exploit arbitrage opportunities, market
pricing is prudent. By this we mean that it is as if a prudent market maker
were guiding the price formation process. This novel form of efficiency is
grounded in inductive inference. Moreover, it implies a modular nature to the
way information is built into market prices. It implies stability in the
value of information in past cases. 

%\paragraph{Comparing the complexity of \condtwodiv\ and
%\fourdiv,}\hskip-8pt in the presence of the other axioms, provides a measure of
%the value of experience 
%As in \cref{sec-fin-app}, take $X$ and $\mbbt$ to be of cardinality $m$ and $n$
%respectively. 
%
%As an estimate, we compare condition \ref{rows-main}
%with \ref{rows-gsii} of Gilboa and Schmeidler's theorem.  Verifying
%\ref{rows-gsii} involves checking $n$ affine dominance constraints: one for
%each case type.  This is well-known to be equivalent to the complexity of
%linear programming with real variables \textup{\citep{DR-Linear_programming}}.
%In the absence of knowledge regarding the sparsity of $\mathbf v$, the fastest
%algorithm for achieving this is of order $n^{3}$
%\textup{\citep{LS_Linear_programming}}.  Since this holds for every subset of
%four distinct eventualities, checking \ref{rows-gsii} is of order
%$\binom{m}{4}n^{3}$.  In contrast, verifying $\mathbf{v}(x,\cdot )\not \leq
%\mathbf{v}(y,\cdot)$ takes at most $n$ steps for each of the $\binom{m}{2}$
%subsets of $2$ distinct elements. Likewise, for every three distinct elements
%$x,y,z$ in $X$, checking for noncollinearity of two vectors takes at most $n$
%steps. Thus, a na\"{i}ve algorithm for checking \ref{rows-main} is of order
%$\binom{m}{3}n$. Even for $m = 4$ and $n = 4$, the difference is stark:
%$\binom{m}{3}n = 16$ versus $\binom{m}{4} n^3 = 64$. (The threshold $4$ is
%important as we have shown in \cref{prop-fourdiv-empty} below.)
% % \footnote{In fact one way to check if \ref{rows-main}
%    % holds is to proceed as follows. Assume we have an enumeration of $\mbbt
%    %$. Let $t$ be the first member of $\mbbt$ that satisfies
%    % $\mathbf{v}(x,t)-\mathbf{v}(y,t) \neq 0$ and record this value. (If no
%% such
%    % $t$ exists, then \ref{rows-main} fails to hold.) Next, for each
%    % $z\in X\bs\{x,y\}$, solve for $\lambda\in \R$ such that
%    % $\mathbf{v}(x,t) - \mathbf{v}(y,t) =
%    % \lambda(\mathbf{v}(x,t)-\mathbf{v}(z,t))$. Next, stop the algorithm at
%% the
%    % first $t'>t$ such that either
%    % $(\mathbf{v}(x,t)-\mathbf{v}(y,t))\cdot(\mathbf{v}(x,t')-\mathbf{v}(y,t'))<
%    % 0$ or
%    % $\mathbf{v}(x,t') - \mathbf{v}(z,t')\neq \lambda
%    % (\mathbf{v}(y,t')-\mathbf{v}(z,t'))$. If, the algorithm reaches step $m$
%    % without finding such a $t'$, then \ref{rows-main} holds.  Thus,
%% algorithms
%    % for checking \ref{rows-main} are of order
%    % $O\hskip-1pt\left((m-1)\binom{m}{2} n\right)$.}
%
%\paragraph{In experimental settings, a robust test of {stability}\hskip-7pt} is
%available precisely when \ref{T}--\ref{c2d} hold and \fourdiv\ fails. For, when
%$n<\infty$, the existence of a similarity representation satisfying
%\cref{wrap-main} is equivalent to \stability. The test is robust in that,
%generically, learners that fail to check for the arrival of new cases also
%fail to satisfy \cref{wrap-main}. This is because, for every $x,y,z \in X$, the
%row differences $v^\xy := - \mathbf{v}(x,\cdot) + \mathbf{v}(y,\cdot)$ satisfy
%the Jacobi identity $v^{\xz} = v^{\xy} + v^{\yz}$. These latter three vectors
%span a linear space of dimension at most two. This in turn implies that the
%hyperplanes $H^{\{x,y\}}, H^{\{y,z\}}$ and $H^{\{x,z\}}$, to which the row
%differences are normal, are congruent. Congruence implies the hyperplanes are
%not in general position.  In other words, there is a zero (conditional)
%probability of the learner striking lucky and appearing to be prudent when
%they are in fact are not. 
%
%\paragraph{Novelty, memory and transitivity.} An early test and taxonomy of
%intransitive behavior is due to
%\citet{Weinstein-Intransitivity,Weinstein-Transitivity}. Weinstein points out
%that intransivity can sometimes be rational in complex situations.
%Interestingly, he shows that young are people significantly less transitive in
%their choices.  He also points out that the law itself is designed to
%accommodate irresponsible under-age decisions. In the psychology literature,
%\citet{BR-Novelty_and_intransitivity} show that presenting novel objects is
%more likely to trigger intransitive choices. 
%
%More recently, \citet{Enkavi-Hippocampal_dependence} provide evidence that
%people with a specific form of memory impairment (lesions in the hippocampus of
%the brain) are significantly more likely to violate transitivity in pairwise
%choices of chocolate bars: even though they rank numbers
%transitively.\footnote{ The hippocampus is associated with learning and memory.
%\Citet{Hassabis-Hippocampal_dependence} and
%\citet{Schacter-Hippocampal_dependence} present evidence showing that the
%hippocampus plays an important role in imagining future experiences on the
%basis of past ones. \Citet{Enkavi-Hippocampal_dependence} go further by showing
%that it plays a role in the value-based decision making framework of
%\citet{Rangel-Value-based_neurobiology}.  }
%
%In line with the case-based approach, the experimental evidence of
%\citet{Enkavi-Hippocampal_dependence} suggests that agents are constructing
%their preferences on the basis of past experience. Moreover, it seems natural
%to interpret agents with hippocampal impairment as inexperienced learners.
%The fact that impaired agents then make intransitive decisions is in line with
%what our model predicts as they are, in effect, facing a novel situation and
%are required to construct their preferences on the fly. It appears that
%impaired agents are also failing to be prudent, though it is not obvious that
%choices among chocolates warrant the additional neural computation that
%accompanies \stability.
%
%
%Whilst this literature does not offer
%a direct test of the present model, it supports the case-based framework of
%constructing preference as well as our premise that violations of transitivity
%by situations that require imagination or equivalently situations where the
%memory is impaired.
%  % In particular, \citet{Enkavi-Hippocampal_dependence}
%% show that people with hippocampal lesions are more likely to make
%% intransitive
%% consumption choices: even though they transitively rank numbers. For
%% instance, faced with
%% pairwise choices among chocolate bars (to be consumed at a future point in
%% time), they are more likely to choose Snickers over Mars; Mars over Bounty;
%% but
%% Bounty over Snickers.
%
%%\begin{remark*}
%%  A closer look at the relationship between the proportion $\rho$ of
%%  hippocampal impairment and the percentage $\sigma$ of intransitive choices
%%  \citep[in figure 2 of][]{Enkavi-Hippocampal_dependence} suggests another
%%  interpretation.
%%  For $\rho$ above $\frac{1}{4}$, $\sigma$ is above $20\%:$ twice as high as
%%  it is for $\rho < \frac{1}{4}$. Memories and the rankings they generate are
%%  latent variables to the observable $\rho$ and this threshold is where
%%  \condtwodiv\ fails to hold and our model breaks down. The $16$ cases in the
%%  data are thus partitioned into three groups$:$ two that satisfy \threediv\
%%  ($\rho < 0.05$ and $\sigma< 5\%$)$;$ $12$ intermediate cases that satisfy
%%  \condtwodiv\ ($0.05 \leq \rho \leq 0.25$ and $5\%\leq \sigma \leq 10\%$)$;$
%%  and
%%  $2$ severe cases that fail to satisfy \condtwodiv\ ($0.25 < \rho$ and
%%  $10\% < \sigma$).
%%\end{remark*}
%
%
%\paragraph{The success or failure of startups. \hskip-7pt} Inexperience raises
%significant barriers to entry. Overcoming these barriers is either the result
%of making mistakes and learning by doing ``on the fly'' or the result of being
%prudent. Which form of induction bears out in practice will depend on many
%factors.  Prudent learners are more likely to survive the initial phase of
%inexperience before they acquire a rich database. It will be interesting to see
%if ChatGPT is prudent enough to survive long enough to challenge the vast
%experience of Google.
%
%%The following proposition confirms our thesis that experienced learners (\ie\
%%those that satisfy \fourdiv) have indeed encountered a high number of case
%%types. As the main theorem of \gsii\ shows, experienced learners have no need
%%for the additional structure of {generalization}s. Unless the prediction problem
%%changes (\eg\ new eventualities become relevant),  they have no need to engage
%%in second order induction. This saving in cognitive effort is the prize that
%%experience confers.
%%
%%\begin{theoremEnd}{proposition}[experience and case
%%  types]\label{prop-fourdiv-empty}
%%  If $\preceqb_{\mbbd}$ satisfies \ref{K}--\ref{A} and \fourdiv, then $n \geq
%%  \min \{4, m\}$, and, for every $Y \subseteq X$ of cardinality $m'$ and
%%  regular $Y$-{generalization} $\ext$, the number $n'$ of equivalence classes of
%%  $\sim^{\ext}$ satisfies $n' \geq \min \{4,m'\}$.
%%  \end{theoremEnd}
%%  \begin{proofEnd}%[Proof of \cref{prop-fourdiv-empty}]
%%    Via \cref{lem-axiomsQ}, \ref{K}--\ref{A} and \fourdiv\ hold for
%%    $\preceq_{\mbbd}$ if and only if \ref{KQ}--\ref{AQ} and \fourdiv\ hold for
%%    $\preceq_{\mbbj}$. Let $Y\subseteq X$ be of cardinality $m' = 1, 2, 3$ or
%%    $4$ and let $\ext$ be a regular $Y$-{generalization}. Via \cref{lem-insep}, there
%%    exists a pairwise representation $v^{\dd}$. For $m' = 1$, $n' = 1$ because
%%    $\ext_{J}$ is constant on $\mbbjp$.  For $m' = 2$, $n' \geq 2$, since via
%%    part \ref{D-insep} of \cref{lem-insep}, $G^{\xy}$ and $G^{\yx}$ are both
%%    nonempty.
%%
%%    By way of contradiction, first suppose $n' = 2$ and $m' \geq 3$. Via
%%    \cref{eg-zaslavski} of appendix \ref{sec-proof-mainQ},
%%    $\total(\ext) \leq 4$. In contrast, \fourdiv\ requires $\total(\ext) = 6$.
%%    The remaining case is where $n' = 3$ and $m' \geq 4$. If the rank
%%    $\mathbf r$ of $v^{\dd}$ satisfies $\mathbf r \geq 3$, then the kernel
%%    $A^{Y}$ of $v^{\dd}$ is zero-dimensional. Then $0$ is the unique element of
%%    $A^{Y}$. Thus, the positive kernel $A^{Y}_{\mdoubleplus}$ of $v^{\dd}$ is
%%    empty. Then Zaslavski's theorem implies that $\total(\ext)< 4!$, so that
%%    \fourdiv\ fails to hold.  If $\mathbf r \leq 2$, then an application of the
%%    rank version Zaslavski's theorem (in particular \cref{eq-zaslavski-4} with
%%    $\acute{\mathbf r } = \mathbf r = 2$) yields
%%  \begin{linenomath*}
%%    \begin{equation*} \lvert \mc G_{\mdoubleplus} \rvert \leq 1 - 6 + 15 - 20 + 15 + 6 + 1 =
%%      12. \end{equation*}
%%\end{linenomath*}
%%Thus, once again \fourdiv\ fails to hold. Thus $n' \geq \min \{4, m'\}$, as
%%required.  Finally, since $Y\subseteq X$, $m\geq m'$, and, via part
%%\ref{item-dimension} of \cref{def-{generalization}}, $n \geq n'$.
%%\end{proofEnd}
%%In contrast, \condtwodiv\ implies no restrictions on the cardinality of
%%$\mbbt$ beyond $n \geq 2$ and this is also a virtue of \twodiv.
%
%
%% Related to this discussion is Quine's observation on scientific practice:
%% \begin{quote}``The experimenter settles for criteria that he can confidently
%%   adjudicate on the spot: confidently, not infallibly. He looks carefully, and
%%   if he is subsequently in doubt he repeats the experiment if he can. The finality of an
%%   experiment is historical not logical.''
%% \end{quote}
%% To which which we add that, if neither the experiment cannot be repeated and the
%% experimenter lacks confidence, then he may still look at the implications of      Thus, the present framework provides a
%% way of checking that the existing specification of the entire model (including
%% the definition of cases) does not generate violatations of the basic axioms when
%% second-order information arrives. In the proof of \cref{thm-main}, imprudent
%% learners that satisfy \ref{T}--\ref{A} and \condtwodiv\ are represented the
%% similarity function $\sum_{c \,\in D} v^{\xy}(c)$ that is additive, but
%% inseparable in $x$ and $y$. Yet inseparability on $X$ and nonlinearity on
%% $\mbbd$ are not mutually exclusive.\footnote{With additional regularity
%%   conditions (see \citet{OCallaghan-Parametric_continuity}), such learners are
%%   also represented by a nonlinear function $u: X \times \mbbd \rightarrow \R$
%%   such that for every $D \in \mbbd$ and every $x,y \in X$, $x \preceq_{D} y$
%%   if, and only if, $u(x,D)\leq u(y,D)$.}% This
%% remains true when axioms \ref{C}, \ref{A} and \condtwodiv\ hold. In which case
%% $\preceq_{\mbbd}$ is also represented by a . It is only when \ref{T} fails that the above $u$ will fail to exist.
%
%
% % The fact that on the model offers a way of certifying that she is not By
% %  recast the problem a the forward-looking startup understands that the domain
% %  of the similarity function will change and hence that, when a novel case
% %  arrives, the function itself will change. Unless the startup is prudent, she
% %  may end up generating intransitive (or perhaps random) SERPs. Or, worse still,
% %  in order to avoid intransitivities, the startup may become dogmatic and rule
% %  out good rankings. Both of these situations are described by a similarity
% %  function $\sum_{c \,\in D} v^{\xy}(c)
% % $ that is additive, but inseparable in $x$ and $y$.
%  
%\paragraph{When is {stability} worth the trouble?} The simple answer to this
%question is: when revising a model ``on the fly'', once a novel case arrives,
%is costly. We argue that in the setting of financial markets where a lack of
%{stability} can lead to arbitrage opportunities, {stability} will emerge as optimal
%behaviour with more prudent agents out-surviving others. 
%
%%\begin{example}\label{eg-arbitrage}
%% 
%%  Consider a fair market maker of \emph{Zeros} (zero-coupon treasury
%%  bonds).\footnote{Similar to a fair insurer, the fair market maker sets the
%%  market spread to zero.}
%%  The compound-interest formula for the accumulation
%%  process of such a bond is
%%  \begin{linenomath*}
%%    \begin{equation*}a^{\xy}= \left(1+r^{\{x,y\}}\right)^{-x + y}, \end{equation*}
%%  \end{linenomath*}
%%  where $r^{\{x,y\}}$ is the implied yield on a forward contract that accrues
%%  interest between dates $x$ and $y$. If $x$ is later than $y$, then the
%%  contract is to sell, and the market maker pays this yield, so that
%%  $r^{\{y,x\}} = r^{\{x,y\}}$. 
%%  Let $X \subseteq \R_{\mplus}$ index a suitable
%%  sequence of trading dates with $0 \in X$ being the spot date.  It is well
%%  known that a Zero that pays out one dollar at time $x > 0$ is arbitrage-free
%%  if, and only if, the log-accumulation process satisfies
%%  \begin{linenomath*}
%%  \begin{equation}\label{eq-no-arbitrage}
%%    \text{for every $x,y,z \in X$,}\quad   \log a^{\xy} = \log a^{\xz} +  \log a^{\zy}
%%    \,.\footnote{To see this, suppose that, for some
%%      $x< y$, she sets $a^{\xy} < a^{\xz}a^{\zy}$.  Another trader would do well
%%      to sell the forward contract $\xy$, buy the spot contract $\xz$ and sell the
%%      spot contract $\zy$.  A risk-free arbitrage opportunity is also
%%      available if the reverse inequality holds.}
%%  \end{equation}
%%\end{linenomath*}
%%This no-arbitrage condition is a special case of the Jacobi
%%identity. We now explain how a market maker might infer the
%%accumulation process from past cases.
%%
%%  \end{example}
%
%
%\paragraph{Discussion of second-order induction.}\vskip-8pt
%%The market maker of
%%\cref{eg-zeros,sec-fin-app} engages in second-order induction when she acts
%%prudently.
%%She reflects on her model by checking that the basic axioms of \gsii\ will
%%continue to hold when a novel case arrives. By way of contrast, suppose the
%%bond price of the market maker is such that $\preceq_{\mbbd}$ is consistent
%%with the basic axioms, but not \stability. Then when a novel case arrives, she
%%may be exposed to arbitrage and need to respecify her entire model ``on the
%%fly''. Such a step corresponds to the intermittent respecification of her
%%similarity weighting function $\mathbf{v}(x,c)$, as
%%\citet[p.10324]{argenziano2019second} describe. 
%In \citet{argenziano2019second}, the ``leave-one-out'' technique of
%cross-validating the model by omitting a case of each type is intuitively and
%operationally close to our inclusion of the free case $\novel$. The difference
%is that by introducing a degree of freedom, our market maker can use her
%imagination to study novel {generalization}s and peer into the future through
%the lens of her current model. She can exploit the intervals of time inbetween
%the arrival of novel cases by continuously engaging in second-order induction.
%
%Through an example, we now show that the present framework provides the
%  flexibility to accommodate second-order induction without sacrificing the
%  computational or normative advantages that additive similarity functions
%  provide.
%
%    % Agents may be comfortable with familiar theories and sets of empirical
%    % observations, but less so when combining their present model with a new one.
%    % (By way of analogy, in probabilistic contexts, it is often the case that
%    % joint distributions are poorly understood.\footnote{See, for instance, the
%    %   motivation to \citet{Carroll-Multidimensional_screening} in a
%    %   multidimensional-screening setting.})  The present framework is therefore
%    % geared towards settings where theories are combined over timeand the dynamics of
%    % learning matter.
%
%    % We restaurant example of second-order induction in light of
%    % the present framework.
%  \begin{example*}[second-order induction, \gsii, p.12] \label{eg-john_mary}
%    Let $c$ denote a case where Mary chooses restaurant $x$ over restaurant
%    $y$. In the absence of any further information, it is tempting to assume
%    some similarity between John and Mary. The learner then finds it plausible
%    that John prefers $x$ to $y$ given $\{c\}$. A separate database $D$
%    contains no choices between $x$ and $y$. Thus, in the absence of further
%    information, $x$ and $y$ appear equally likely based on $D$. Additivity of
%    the similarity function (or \ref{C})) implies it is plausible that John
%    prefers $x$ to $y$ given $\{c\} \cup D$. The violation of \ref{C} arises
%    when a more careful examination of the contents of $D$ reveals many choices
%    between other pairs of restaurants where John and Mary consistently differ.
%\end{example*}
%Quine's notion of perceptual similarity \citep{Quine-Roots_of_reference} offers
%a check on the learner's inference about John's choice given $\{c\}$. John and
%Mary may just as well be two drivers passing through an intersection at
%different times. Although their situations are broadly speaking very similar,
%if one faces a red light and the other a green light, their responses will
%differ. In the restaurant setting, some pivotal information is omitted from
%$c$.  Observing that the evidence in $c$ in favour of $x$ over $y$ is somewhat
%weak, a prudent learner instead recasts $\{c\}$ as a database $C$ that combines
%past observations with copies of the pivotal novel case $\novel$.  With a more
%refined model, that explicitly allows for omitted variables, the learner can
%check to see if her model {generalizes} to higher dimensions without violating
%the basic axioms.
%
%The same argument applies to nonlinear machine learning models. We recall the
%``kernel trick'' that extends the linear framework to the nonlinear by
%increasing the dimension of the problem and casting a nonlinear model as a
%linear one in the higher-dimensional space. With a suitable change of
%variables, a three-dimensional space allows for binary classification of
%vectors according to whether they lie in the unit circle using a linear kernel.
%
%%\begin{remark}
%%
%%  Predictors that satisfy \ref{T} and \ref{K}, but not \ref{C} can, with some
%%  additional regularity conditions, still be represented by a nonlinear
%%  function $u: X \times \mbbd \rightarrow \R$ such that for every $D \in \mbbd$
%%  and every $x,y \in X$, $x \preceq_{D} y$ if, and only if, $u(x,D)\leq u(y,D)$
%%  \citep[see][]{OCallaghan-Parametric_continuity}.  Similarly, learners that
%%  satisfy \ref{T}--\ref{c2d} but not \stability\ can also be represented by such
%%  a function: because $\preceq_{\mbbd}$ is complete and transitive for each $D
%%  \in \mbbd$. The present framework allows us to disentangle the latter kind of
%%  learner from those who, for good reason, fail to satisfy the combination
%%  axiom. (See \gsii\ for examples of such reasons.)
%%
%%\end{remark}
%% \begin{remark}(To go somewhere else)\label{rem-why-pru}
%% The main contribution of this paper is to show that, in the case where the
%% learner engages in higher-order sampling and explores novel {generalization}s,
%% this
%% absence of rich data does not preclude a similarity representation of the
%% consistent form that \gsii~derive. This is feasible provided the learner is
%% prudent enough to check that the arrival of novel cases will not force her
%% into the dilemma of choosing between being dogmatic and violating
%% transitivity.
%% \end{remark}
%% \begin{remark}
%% The reason we define testworthiness in terms of pairwise extremal total
%% suborders is that the alternative is less amenable to interpretation as a
%% behavioral condition. This is because the maximal novel {generalization} is
%% dependent on the number of case types. Defining testworthiness in terms of
%% maximal {generalization}s would work well in non-human agent settings.  The concept
%% of the order reversal or inverse ordering is familiar and captures the idea
%% of
%% being far from an existing order. It captures the idea that being prudent
%% requires that one should explore beyond one's comfort zone, beyond what one
%% can identify with the data. We argue that it is procedurally similar to what
%% an inexperienced agent would do in practice.  Moreover, for every quadruple
%% of
%% eventualities, maximum number of orders one has to test is at most
%% $4!/2=12$. And, in fact, if one {generalization} passes the test (in the sense of
%% the
%% {stability} axiom), then, as we show, so do they all.
%
%% \end{remark}
%
%% When $\current$ is sufficiently rich, that, in addition to the basic axioms,
%% $\preceqb _ \mbbd$ satisfies \fourdiv, the learner need not engage in
%% higher-order sampling or testing of novel {generalization}s. Testing, that is, to
%% see
%% if the arrival of novel cases will force her to either be dogmatic (and
%% exclude
%% accurate plausibility rankings/predictions) or violate transitivity. We will
%% assume the following holds for $k = 4$.
%
%
%% This resembles a robust version of two-pass methods
%% in econometrics: robust because only ordinal (ranking) information from the
%% initial step is retained when the similarity matrix is derived. In contrast
%% with
%% the standard bootstrap, since rankings are ordinal, this holds for every
%% sample
%% size.
%% Resampling in the present setting is akin to the familiar bootstrap except
%% that,
%% because ordinal rankings are primitive the sample size is not limited to that
%% of
%% $D^{\star}$. 
%% \footnote{We thank Jurgen
%% Eichberger for his question on this matter at the RUD Conference in
%% Heidelberg
%% 2018.}
%% is stored and used in the construction of the
%% similarity representation.\footnote{The standard reference for two-pass
%% methods
%% in finance is \citet{FM-Two_pass}.}
%
%
%\paragraph{The veracity of false news.} How should a learner check whether her
%model consistently {generalizes} to higher dimensions (when novel cases
%arrive)?  Our model and proof suggest that the most useful rankings that she
%might wish to examine are those that are far from her own. This is because
%testworthy {generalization}s involve assigning to the novel case $\novel$ the
%inverse of some total ranking $\preceq_{D}$.  This offers a rationale for why
%information that differs from our own is intrinsically valuable. Testworthy
%{generalization}s play a vital role in taming the complexity of our proof. It
%is plausible that something similar may be at play when agents encounter
%radically different information from their own on social media: \emph{even if
%it is fake}. This supports the evidence that false news is significantly more
%veracious than real news online \citep{Vosoughi-Roy-Aral-Veracity}. The fact
%that real news is typically closer to what we have observed in the past means
%that it is of less value to learners that find it costly to imagine worlds that
%are far from their own. \Cref{cor-efficiency} provides additional support for
%this argument.
\printbibliography

\makeatletter
\def\@seccntformat#1{Appendix\,\csname the#1\endcsname.\quad}
\makeatother
\begin{appendices}
%proof of thm-main
\section{The proof of \cref{thm-main}}\label{sec-proof-main}
% Steps of the proof

% The proof proceeds as follows.  In the first step of the proof 

% Then, by the richness condition and
% the fact that $\sim ^ \star$ is an equivalence relation, there exists $% \textup {T} \in \mc T$ such that the set $\mbbc _ { / \sim ^ \star }$ of
% equivalence classes of $\sim ^ \star$ in $\mbbc$ satisfies $\mbbc _ { /
% \sim ^ \star } = \textup{T}$.  In the second, ***


 %step-integers
Similar to \gsii, we  translate the model into one where
databases are represented by vectors, the dimensions of which are
case types. To allow us to focus on aspects of the present model, 
proceed directly to rational vectors and present the axioms  and a corresponding
theorem (\cref{thm-mainQ}) which, as we confirm, holds if,
and only if,  \cref{thm-main} does. The proof of \cref{thm-mainQ} can be found
in \cref{sec-proof-mainQ}.


\paragraph{Case types as dimensions.} From our definition
  of case types in  \cref{sec-model}, $\mbbt = \mbbc_{/\sim^{\star}}$ and $\mbbt^\novel \defeq \mbbt\cup [ \novel ]$. Let $\mbbtpp$ be a free variable
  in $\{ \mbbt , \mbbtp \}$.  When no possible confusion should arise, we use
  $\novel$ as shorthand for $[ \novel ]$.  It is straightforward to show
  that the following construction would work if instead we were to work with
  any partition $\textup T$ of $\mbbc$ that is at least as fine as $\mbbt$.
  The present construction is the one with the lowest feasible number $\countof
  \mbbt$ of dimensions.

% Let $\mbbip\upiota( \mbbt )$  and $\mbbi  ' _{ \bullet }$  be the corresponding
%subsets of $\nnint ^ \types$.
%\Cref{obs-Regular} implies that the same is true of $\mbbd _{/\sim ^
%\supext }$ for every $\ext  \in \reg ( X , \preceqb _ \mbbd )$.
%rankings counting vectors

\paragraph{Translation to  counting vectors.}
Let $\nnint$ denote the set of nonnegative integers and $\posint$ those that
are (strictly) positive.  Let $\mbbi \subseteq \nnint^{\mbbt}$ denote the
set of counting vectors $L : \mbbt \rightarrow \nnint$ such that
$\lbc t : L ( t ) \neq 0 \rbc$ is finite and let $\mbbip$ denote the
corresponding subset of $\nnint^{\mbbtp}$. Then let
\begin{linenomath*}
\begin{equation*}
\mbbipp = \left\{
\begin{array}{ll}
 \mbbi & \text{if, and only  if, $\mbbtpp= \mbbt$, and}\\
\mbbip  &\text{otherwise.}
\end{array}\right.
\end{equation*}
\end{linenomath*}
Modulo notation, the following construction is identical to \gsii.  For every
$D\in \mbbd$, let $L_{D}: \mbbt \rightarrow \nnint$ denote the function
$t \mapsto L_{D}(t) = \countof (D \cap t )$.  For each $D \in \mbbd$, let
$\preceqb_{L_{D}} \defeq \preceqb_{D}$.  We need to establish that
$\preceqb _{ \mbbi } \defeq \langle \preceqb _{L } : L \in \mbbi \rangle$ is
well-defined. For every $L\in \mbbi$, the richness assumption (on $\mbbtp$)
guarantees the existence of $D\in \mbbd$ such that $L_{D}= L$. By definition,
$\sim^{\star}$ is such that, for every $C,D \in \mbbd$, $C\sim^{\star}D$ if, and
only if, $L_{C}= L_{D}$. Straightforward mathematical induction on the
cardinality of $C$ shows that $C\sim^{\star} D$ implies
$\preceqb_{C} = \preceqb_{D}$.  This construction of $\preceqb_{\mbbi}$
ensures that the same notion of equivalence that we introduced in \cref{obs-reg-eq}
also applies here. Thus, $\preceqb_{\mbbi} \equiv \preceqb _{\mbbd}$.

%rational vectors
\paragraph{Translation to rational vectors.}
Similarly, let $\nnrat$ denote the nonnegative rationals and $\posrat$ those
that are (strictly) positive.  Take $\mbbj \subseteq \nnrat^{\mbbt}$ to be the
set of rational vectors with $\lbc t \in \mbbt : J ( t ) \neq 0 \rbc$ finite
and take $\mbbjp$ to denote the corresponding subset of $\nnrat ^{ \mbbtp }$.
For each $J \in \mbbj$, by virtue of the fact that $\posint$ is well-ordered
and $J$ has finite support, there exists (unique) minimal $k_{J} \in \posint$
such that $L_{J} \defeq k_{J} J$ belongs to $\mbbi$.  Let
$\preceqb_{J}\defeq \preceqb_ { L_{J} }$. (This definition acquires meaning
below once we translate and apply the combination axiom.)  In this way,
$\preceqb _{ \mbbj } = \langle \preceqb_ J : J \in \mbbj \rangle$ is
well-defined, and we may introduce axioms for $\preceq_{\mbbj}$ directly: \ie\
without first introducing axioms for $\preceq_{\mbbi}$. We first demonstrate
that $\preceqb_{\mbbj}$ and $\preceqb_{\mbbd}$ are equivalent. First note that,
for every $I,J\in \mbbj$ such that $L_{I} = L_{J}$,
$\preceqb_{I}=\preceqb_{J}$. Then, let $L^{\prime} = L_{J}$ and take any $D$
such that $L_{D} = L^{\prime}$. Then $\preceqb_{J} = \preceqb_{D}$.  The reverse
embedding follows by virtue of the fact that $\mbbi\subset \mbbj$. Thus,
$\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$.


% for any
% nonempty $Y \subseteq X$ the set $\reg ( Y , \preceqb _{ \mbbj })$ of
% regular {generalization}s that are indexed by rational vectors is simply the
% restriction of $\preceqb _{ \mbbj }$ to $Y$.  % Similar to before, let
% $\zeta : \mbbjp \rightarrow \mbbip$ be the map
% $J \mapsto \kappa ( J ) \defeq k_{ J } J$ such that, for each $J$,
% $k _ J \in \posint$ is minimal.  Then, for each $J \in \mbbjp$, let
% $\ext _ J \defeq \ext _{\kappa ( J )}$, so that
% $\ext _{ \mbbjp } \defeq \langle \ext _{ J } : J \in \mbbjp \rangle$ is
% well-defined.


 % {generalization}s-counting-vectors-construction

\paragraph{Construction of {generalization}s of $\preceqb_{\mbbj}$.}
We follow common practice by letting $2 ^{ X }$
denote the collection of nonempty subsets $Y \subseteq X$.
For each $Y \in 2 ^{ X }$, we will denote the set of regular, novel and
testworthy $Y$-{generalization}s (of $\preceqb_{\mbbd}$ or $\preceqb_{\mbbj}$) by
$\reg ( Y , \cdot )$, $\nov ( Y , \cdot)$ and
$\test ( Y , \cdot)$ respectively. Recalling that every
$Y$-{generalization} is either regular or novel, let $\Ext (Y,\cdot)$
denote the set of all $Y$-{generalization}s. We now clarify
what it means to be a {generalization} of $\preceqb_{\mbbj}$.

For each $t \in \mbbtpp$, we take  $\delta_{t} : \mbbtpp \rightarrow \R$ to be
the function satisfying $\delta_{t} ( s ) = 1$ if $s = t$ and $\delta_{t} ( s ) = 0$
otherwise. (When $\mbbtpp$ is finite, these are simply the basis vectors for $\R^{\mbbtpp}$.)  When we wish to emphasise that the  vectors belong to in
$\R^{\mbbtp}$, then, for each $\mbbtp$, we will write $\delta_{t} ^{ \novel }$.
Let
\begin{linenomath*}
\begin{equation*}
 \mbbjpp  = \left \{
\begin{array}{ll}
\mbbj & \text{ if, and only if, $\mbbtpp = \mbbt$, and}    \\
 \mbbjp & \text{ otherwise.}
\end{array}\right.
\end{equation*}
\end{linenomath*}
For every $I \in \mbbj$ and $J \in \mbbjpp$, we write $I \equiv J$ whenever
$I = J$ or $J = I\times 0$. (In the latter case, $J(t)= I(t)$ for every
$t\in \mbbt$ and $J(\novel )= 0$.) This notion reflects the fact that, for the purposes
of the present model, such $I$ and $J$ are equivalent.
\begin{definition}\label{def-{generalization}Q}
 
$\extb =\langle \extb_{J}: J \in \mbbjpp \rangle$ is a
  \emph{{generalization}}, and in particular a $Y$-\emph{{generalization}}, of
  $\preceqb _{ \mbbj }$ if, and only if, for some nonempty
  $Y \subseteq X$ both the following hold
\begin{enumerate}

\item for every $J\in \mbbjpp$, $\extb_{J}\in \relations(Y)$,
  $\nextb_{J} \defeq \extb_{J} \cap \extb_{J} ^ { -1 }$ and
  $\sextb_{J} \defeq \extb_{J} \bs \extb_{J} ^ { -1 };$

\item for every $J \in \mbbj$ and $L \in \mbbjpp$ such that $J \equiv
  L$, $\extb_{L} = \preceqb_{J} \cap (Y ^ 2)$. \label{item-preservingQ}
\end{enumerate}
A {generalization} $\ext_{\mbbjpp}$ (of $\preceq_{\mbbj}$) is proper if $\mbbjpp=
\mbbjp$ and improper otherwise.  A proper {generalization} is either regular or
novel. $\ext$ is novel if, for every $s \in \mbbt$, there exists $I$ in $\mbbj$ such that, for $J = I \times 0$ (in $\mbbjp$), we have $\extb _ { J +
  \delta _{s} ^{ \novel } } \neq \extb _ { J + \delta _ { \novel} ^{ \novel }
}$\,.
 \end{definition} 

 For every regular $Y$-{generalization} $\ext$ of $\preceq_{\mbbd}$ such that $Y=X$,
 \cref{obs-reg-eq} implies $\extb\equiv \preceqb_{\mbbd}$. And, via
 $\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$ and transitivity of equivalence, we
 conclude that $\ext$ is equivalent to $\preceq_{\mbbj}$.  Two sets of
 {generalization}s are isomorphic if there exists a canonical isomorphism between
 equivalent {generalization}s. 
% \footnote{That is,
                                %for every $J \in \upiota ()$, $\ext _ J  = 
                                %\ext ' _ A$ for every  $A \in { \upeta '} ^
                                %{\, -1 } ( J )$.}


\begin{theoremEnd}[no link to proof]{lemma}[proof on \cpageref{proof-nov-iso}]
  \label{lem-nov-iso}

  For every $Y \in 2 ^ { X }$, $\reg(Y, \preceqb_{\mbbj})$ is isomorphic to
  $\reg(Y,\preceqb_{\mbbd})$ and
  $\nov ( Y , \preceqb_{\mbbj} )$ is isomorphic to $\nov ( Y , \preceqb _ \mbbd )$.

% For each $Y \subseteq X$, every member of $\nov ( Y , \preceqb_  \mbbi )$ is
%equivalent to some member of $\nov ( Y , \preceqb _ \mbbd )$ and vice
%versa.

\end{theoremEnd}
\begin{proofEnd}%[Proof of \cref{lem-nov-iso}]
  \label{proof-nov-iso}

  We show that there exists a canonical embedding (a structure preserving
  injection) of $\nov ( Y , \preceqb _ \mbbj )$ into
  $\nov ( Y , \preceqb _ \mbbd )$. The fact that this map is also
  surjective follows from the fact that $\nov ( Y , \preceqb _ \mbbd )$
  can be embedded in $\nov ( Y , \preceqb _ \mbbj )$ in precisely the same
  way. The proof that the two sets of regular {generalization}s are isomorphic follows
  via a similar argument plus the observation that every $Y$-{generalization} is either
  regular or novel.

  Take $\extb \in \nov ( Y , \preceqb _ \mbbj )$ and define
  $\hextb = \langle \hextb_{C} : C \in \mbbdp \rangle$ via the property: for
  each $C \in \mbbdp$, $\hextb_{C}\defeq \extb_{J}$ if, and only if,
  $L_{C}= L_{J}$, where, as before, $t \mapsto  L_{C}(t)$ counts the number of cases of
  type $t$ in $C$ and $L_{J} = \kappa_{J} J \in \mbbip$ for some minimal
  $\kappa_{J} \in \nnint$. Now, for any
  $\extb ' \neq \extb$ in $\nov ( Y , \preceqb _ \mbbj )$, there exists
  $J\in \mbbjp$ such that $\extb'_{J}\neq \extb_{J}$. If we define $\hext'$
  analogously, so that it is equivalent to $\ext'$, then
  $\hextb ' \neq \hextb$. As a consequence, the canonical mapping
  $\extb\mapsto \hextb$ is injective. If we can show that $\hext$ does
  in fact belong to $\nov ( Y , \preceqb _ \mbbd )$, then we have constructed
  the required embedding. The fact that $\hext$ satisfies
  \ref{item-binary-rel} and \ref{item-preserving} of \cref{def-{generalization}}
  follows immediately from \cref{def-{generalization}Q}. The proof that \cref{item-dimension}
  of \cref{def-{generalization}} holds is as follows. Take any $c , c ' \in \mbbcp$
  and $D \in \mbbdp$ such that $c \sim ^ \star c '$ and
  $c , c ' \notin D$. First, observe that
  $D \cup \{c\} \sim ^ \star D \cup \{c'\}$, and moreover, for some $t \in \mbbtp$
  we have $c , c ' \in t$. Then, for every $t\in \mbbtp$,
  $\lvert D \cup \{c\} \rvert = \lvert D \cup\{c'\} \rvert = L$ for some
  $L \in \mbbip\cap \mbbjp$. Thus
  $\hextb _ { D \cup \{c\} } = \hextb _ { D \cup \{c'\}}$, as required for $\hext$
  to be a {generalization} of $\preceqb _{ \mbbd }$. Finally, via \cref{def-{generalization}Q},
  the definition of a novel {generalization} ensures that the induced equivalence
  relation $\sim^{\extb}$ on $\mbbcp$ satisfies $c \not \sim^{\extb} \novel$ for
  every $c \in \mbbc$. Since $\sim ^{ \hextb}$ inherits this property,
  $\hext$ is novel.
\end{proofEnd}

% \footnote{ $\reg ( Y , \preceqb _ \mbbi )$ is the singleton set that
% consists of the restriction $\preceqb _ \mbbi \cap   Y ^ 2$ of $\preceqb _
% \mbbi$ to $Y$.}
 % axiomQ
\paragraph{Axioms and theorem.} 
We restate the axioms for $Y$-{generalization}s $\ext$ of $\preceqb  _ \mbbj$.
\begin{enumerate}[label=\textup{A\arabic*}$^\flat$]
 \setcounter{enumi}{-1}
%   \item\label{p3dQ}
% For every $Y \subseteq X $ such that $2 \leq \countof Y \leq 3$, and
% every regular $Y$-{generalization} $\ext$,   $\countof Y\leq \countof \total ( \ext )$.
  %   For every distinct  $x, y , z \in Y $, here exists $x_{0} \in X$ such that, for every distinct
  % $x, y \in X$, it is not the case that for every $J \in \mbbjpp$,
  % \begin{equation*} \text{$x_{0} \ext_{ J } y$ if, and only if, $x_{0} \ext_{ J } z$.}\end{equation*}
  
\item\label{TQ}

  For every $J \in \mbbjpp$, $\ext _ J$ is transitive on $Y$.

\item\label{KQ}

  For every $J \in \mbbjpp$, $\ext _ J$ complete on $Y$.

\item\label{CQ}

  For every $I , J \in \mbbjpp$, every $x , y \in Y$ and every
  $\lambda , \mu \in \posrat$, if $x \ext _ I y$  and
  $x \ext_{J} y$, then $x \ext_{ \lambda I + \mu J } y ;$ moreover, if $x
  \sext_{I} y$ and $x \ext_{J} y$, then $x \sext_{\lambda I + \mu J}$.
  % ; moreover, if either holds strictly then $x \mathrel{\sext} _ { \lambda I + \mu J } y$.

\item\label{AQ}

  For every $I , J \in \mbbjpp$ and every $x , y \in Y$ if
  $x \mathrel{\sext} _ J y$, then there exists $0 < \lambda < 1$ such that,
  for every $\mu \in \mbb Q \cap ( \lambda , 1 )$,
  $x \mathrel{\sext _{ ( 1 - \mu ) I + \mu J }} y$.



\end{enumerate}


For $k = 2, 3, 4$, $k$-diversity is defined for {generalization}s of $\preceq_{\mbbj}$
in exactly the same way. We continue to use the term $k$-diversity in this
setting.  The following are \condtwodiv\ and \parthreediv\ respectively.
\begin{enumerate}[label=\textup{A\arabic*}$^ \flat$,resume]
  % [label=\textup{C}\textit{2}\textup{D}$^{\natural}$]
\item\label{c2dQ} For every three distinct elements $x , y , z \in Y$, one of
  the two subsets $\{ J' : x \prec _{J ' } y \}$ and
  $\{ J' : y \prec_{J ' } x \}$ of $\mbbj$ contains both $I$ and $J$ such
  that $z \prec _{ I } x$ and $x \prec _{ J } z$. If $\countof Y = 2$,
  then \twodiv\ holds on $Y$.

\end{enumerate}
\begin{enumerate}[label=\textup{A\arabic*}$^{'\flat}$,resume]
  % [label=\textup{P}\textit{3}\textup{Div}$^{\flat}$]
  \setcounter{enumi}{3}
\item\label{p3dQ}
  For every $Y^{\prime} \subseteq Y$ with cardinality $n = 2$ or $3$, every
  $Y^{\prime}$-{generalization} $\ext$ of $\preceq _{ \mbbj }$ is such that $\countof
  \total ( \ext ) \geq n$.
  
  % For every $Y \subseteq X$ such that $\countof Y = 2, 3$,
  % the regular $Y$-{generalization} $\ext$ of $\preceq_{\mbbj}$ is such that
  % $\countof Y\leq \countof \total ( \ext )$.

\end{enumerate}

% def-testworthyZ and test()

A proper {generalization} $\ext$ of $\preceqb _{ \mbbj }$ is \emph{testworthy} if
it satisfies \ref{KQ}--\ref{AQ} and, for some $J\in \mbbj$ such that
$\ext_{J\times 0 }$ is total, $\extb_{\novel} = \extb_{J\times 0}^{-1}$. Thus,
for each $Y \in 2 ^ { X }$,
$\test ( Y , \preceqb _{ \mbbd } ) \simeq \test ( Y , \preceqb _{ \mbbj
})$. For any pair of {generalization}s $\ext$ and $\hext$, $\hext$ is a perturbation
of $\ext$ if $\extb_{\novel} = \hextb_{\novel}$. Moreover, $\hext$ is a
{{diverse}} perturbation if
$\countof \total (\hext) \leq \countof \total (\ext)$.
\begin{enumerate}[label=\textit{4}-\textup{P}$^{\flat}$]
\item\label{PQ} For every $Y \subseteq X$ of cardinality $3$ or $4$, every
  testworthy $Y$-{generalization} of $\preceqb _{ \mbbj }$ that is novel has a
  {{diverse}} perturbation that satisfies \ref{TQ}\ref{AQ}.
\end{enumerate}

 % lemma relating axiomsZ to axiomsQ

The following result corresponds to claim 2 of \gsii. Its proof is a consequence
of mathematical induction and the combination axiom.
\begin{lemma}\label{lem-coneQ}

  If $\ext _ \mbbjpp$ and $\hext _ \mbbipp$ are equivalent and the latter
satisfies \ref{C}, then for every $J \in \mbbjpp$ and every rational number
$q >0$, we have $\extb _{ q J }  =  \extb _{J}$.

\end{lemma}
The fact that $\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$ immediately implies
that $\preceqb_{\mbbj}$ satisfies \ref{TQ}, \ref{KQ} and \ref{c2dQ} if, and
only if, the corresponding axiom holds for $\preceq_{\mbbd}$. In general, we
have the following result, which then also yields the equivalence for the
{stability} axiom.

\begin{theoremEnd}[no link to proof]{lemma}[proof on \cpageref{proof-axiomsQ}]
  \label{lem-axiomsQ}

  For $\extb _{ \mbbjpp } \equiv \hextb _{ \dpp }$, $\ext _ { \mbbjpp}$ satisfies
  \ref{CQ}--\ref{AQ} if, and only if, $\hext_{\dpp}$ satisfies
  \ref{C}--\ref{A}.
\end{theoremEnd}

 %proof of the lemma relating Z to Q
\begin{proofEnd}%[Proof of \cref{lem-axiomsQ}]
  \label{proof-axiomsQ}

  Fix $\extb_{\mbbjpp} \equiv \hextb_{\dpp}$ and assume that $\hextb_{\dpp}$
  satisfies \ref{C}. We show that $\extb _ { \mbbjpp}$ satisfies \ref{CQ}.
  Fix $x , y \in Y$ and $J \in \mbbjpp$ such that $x \ext_{J} y$ and $x
  \ext_{J'} y$. Fix $\lambda , \mu \in \posrat$ and let $\kappa$ be the
  smallest positive integer such that both $L \defeq \kappa \lambda J$ and $L ' \defeq \kappa \mu J '$ belong to $\mbbipp$.  Then, by
  \cref{lem-coneQ}, we have both $x \ext _{ L } y$ and $x \ext _{ L ' } y$.
  Moreover, for $D, D'$ such that $L_{D}= L$ and $L_{D'}= L'$ , we have $x
  \hext _{ D } y$ and $x \hext _{ D ' } y$ and, by \ref{C}, $x \hext _{ D
  \cup D ' } y$. Finally, since $L_{D} + L_{D'} = \kappa (\lambda J + \mu J
  ')$, one further application of \cref{lem-coneQ} yields $x \sext _{ \lambda
  J + \mu J ' } y$, as required for \ref{CQ}.
  

  The proof that \ref{C} implies \ref{CQ} is \emph{mutatis mutandis} a
  special case of the above argument and ommitted.  We now assume $\hextb _{
    \dpp}$ satisfies \ref{C} and \ref{A} and prove that $\extb _{ \mbbjpp }$
  satisfies \ref{AQ}.  Fix $x , y \in X$ such that $x \sext _{ J } y$ for
  some $J \in \mbbj$ and take any $J ' \in \mbbjpp$.  Then, by the
  construction of $\extb _ \mbbjpp$, there exists $L, L ' \in \mbbi$ such
  that $j J = L$ and $j ' J ' = L '$ for some $j , j ' \in \posint$.  By
  \cref{lem-coneQ}, $\extb_{L} = \extb_{J}$ and $\extb _ { L '} = \extb _ {J
  '}$.  Moreover, by construction, for some $D$ and $D'$ such that $L_{D}=L$
  and $I_{D'}= L'$, $\hextb_{D} = \extb _ J$ and $\hextb _ { D'} = \extb _
  {J '}$.  We therefore conclude that $x \hsext _{ D} y$, so that \ref{A}
  implies the existence of $\kappa \in \posint$ and $\{D_{l}:
  D_{l}\sim^{\hextb} D\}_{1}^{\kappa}$ such that $x \hsext _ { D_{1}\cup\cdots
  \cup D_{\kappa} \cup D '} y$.  Then, by the construction of $\extb _{
    \mbbjpp }$, $x \sext _ { \kappa L_{D} + L_{D'}} y$.  Let $\nu \defeq
  \frac { 1 } { \kappa j + j '}$ and take $\lambda = \nu j '$, so that $0 <
  \lambda < 0$ and $1-\lambda = \nu \kappa j$. In fact, since $\lambda \in
  \mbb Q$, we have \begin{linenomath*} \begin{equation*} K \defeq (1-\lambda ) J +\lambda J '
  \in \mbbjpp .\end{equation*} \end{linenomath*}  Simplifying, we obtain $K = \nu ( \kappa
  L + L ')$.  Since $\nu \in \posrat$ and $\kappa L + L ' \in \mbbjpp$,
  \cref{lem-coneQ} implies $\extb _ { K } = \extb _ { \kappa L + L ' }$.  This
  allows us to conclude that $x \sext _ K y$.  Finally, take any $\mu \in
  \mbb Q \cap ( 0 ,\lambda )$.  From basic properties of the real numbers,
  there exists $\xi < 1$ such that $\mu = \xi\lambda$ and, moreover, $\xi
 $ is rational.  Next, note that the definition of $K$ implies $\xi ( K - J
  ) = \xi\lambda ( J ' - J )$.  Adding $J$ to each side of the latter and
  applying the definition of $\mu$ yields \begin{linenomath*} \begin{equation*}(1 - \xi ) J
  +  \xi K =  ( 1 - \mu ) J + \mu  J ' . \end{equation*} \end{linenomath*} Then, since $x
  \sext _ J y$ and $x \sext _ K y$, \ref{CQ} implies $x \sext _ { ( 1 - \mu
  ) J + \mu J '} y$, as required for \ref{AQ}.

Conversely, we now assume that $\ext _{ \mbbjpp }$ satisfies \ref{CQ} and
\ref{AQ} and prove that \ref{A} holds.  Take $D , D ' \in \mbbd$ such
that $x \hsext_{D} y$ and any other $D' \in \mbbd$. Let $L=L_{D}$ and
$L' = L_{D'}$.  Then, by construction, $x \sext_{L} y$ and, by \ref{AQ}, there
exists $\lambda \in \mbb Q \cap ( 0, 1)$ such that
$x \sext _ { (1 -\mu ) L +\mu L'}  y$.  Then, since $\mu$ is rational,
$\mu = \nicefrac { j } { k }$ for some $j , k \in \posint$.  Let
$q : = (1 - \mu ) /\mu = ( k - j )/ j$ and let $\kappa = j q$, so that
$\kappa = k - j$.  The fact that $0 < \mu < 1$ ensures that
$\kappa \in \posint$.  To complete the proof, we show that
$x \sext _ { \kappa L + L'} y$, for then the existence of
$D_{1}, \dots , D_{\kappa}$ such that
$x \hsext _ { D_{1}\cup \cdots \cup D_{\kappa} \cup D'}$ immediately follows.  Together
$x \sext _ { (1 -\mu ) L +\mu L' } y$ and \cref{lem-coneQ} imply
$x \sext _ { q L + L ' } y$.  Similarly, together $x \sext _{ L } y$ and
\cref{lem-coneQ} imply $x \sext _ { ( j - 1 )q L } y$.  Then, since
  $( j - 1 )q L + ( q L + L ' ) = j q L + L '$ and $\kappa = j q$, an
  application of \ref{CQ} yields $x \sext_{\kappa L + L '} y$, as required.
\end{proofEnd}
The matrix $\mathbf{v}: X \times \mbbt \rightarrow R$ is a \emph{representation of
  $\preceq _{ \mbbj }$} whenever it satisfies
\begin{linenomath*}
\begin{equation}\tag{$\flat$}\label{eq-rep-mainQ}
  \left\{
  \begin{array}{l}
    \text{for every $x , y \in X$ and every $J \in \mbbj$,}\\
    x \preceq_{J} y \quad \text{if, and only if,} \quad
\sum _ { \, t \,\in\, \mbbt} \mathbf{v} ( x
    , t ) J(t) \leq \sum _ {\, t \,\in\, \mbbt } \mathbf{v} ( y , t ) J(t)  .
  \end{array}\right.
\end{equation}
\end{linenomath*}
% By \cref{lem-as-fine-as,lem-well-defined}, we are able to modify the statement
% so that, as in the corresponding result of \gsii, the fact that $\mbbt$
% coincides with $\mbbc _ { / \sim ^ \star }$ is given.

%main theorem integers
%\setcounter{theorem}{2}
We observe that, via the definition of case types, there exists a representation
of $\preceq_{\mbbd}$ that respects case equivalence if, and only if there
exists a representation of $\preceq_{\mbbj}$. The above translation and results 
imply that \cref{thm-main} is equivalent to
\begin{theorem}\label{thm-mainQ}
  Let there be given $X$, $\mbbtp$, $\preceqb _ { \mbbj }$ and associated {generalization}s,
  as above. Then \ref{ax-mainQ} and \ref{wrap-mainQ} are equivalent.

\begin{enumerate}[label=\textup{(\ref{thm-mainQ}.\roman*)}]

\item\label{ax-mainQ}

 \ref{TQ}--\ref{c2dQ} and
 \ref{PQ} hold for   $\preceqb _ { \mbbj }$ on $X$.
 
    
\item\label{wrap-mainQ} There exists a matrix
  $\mathbf{v} : X \times \mbbt \rightarrow \R$ that satisfies both$\,:$
  \begin{enumerate}[label=\textup{(\ref{thm-mainQ}.\alph*)}]
  \item\label{rep-mainQ}
  $\mathbf{v}$ is a representation of $\preceq _ { \mbbj }\,;$ and

\item\label{rows-mainQ} no row of
  $\mathbf{v}$ is dominated by any other row, and, for every three distinct elements
  $x,y, z \in X$, $\mathbf{v}(x,\cdot)-\mathbf{v}(z,\cdot)
 $ and $\mathbf{v}(y,\cdot)-\mathbf{v}(z,\cdot)$ are noncollinear (\ie\ linearly independent).
\end{enumerate}
\end{enumerate}
Moreover, $\mathbf{v}$ is unique in the sense of \cref{thm-main} part II, with
\ref{wrap-mainQ} replacing \ref{wrap-main} and $\mbbt$ replacing $\mbbc$.
\end{theorem}


%proof of thm-mainQ
\section{The proof of \cref{thm-mainQ}}\label{sec-proof-mainQ} 
%The present
%proof follows a similar structure to that of \gsii. That is, we begin with the
%proof for the case of arbitrary (nonempty) $X$ and finite $\mbbt$. We then
%show that we can ``patch'' the proof together to account for the case where $0
%<  \countof X < \infty = \countof \mbbt$.

% We adopt the following notation.  For any $Y$-{generalization} $\ext$ and $x,y\in Y$,
% let $\Phi^{\xy}, \Gamma^{\xy} , \textup{I}^{\{x,y\}} \subset \mbbj$ denote the
% sets $\{J : x \ext_{J} y\}$, $\{J : x \sext_{J } y\}$ and
% $\{J : x \next_{J} y\}$ respectively.  For suitable lists $p$ of elements in
% $Y$, we extend this notion by writing
% $\Gamma^{p} \defeq \{ J : \sextb_{J }\equiv p \}$. For instance, the list
% $(x,y,z)$ yields the set
% $\Gamma^{(x,y,z)} = \Gamma^{(x,y)}\cap \Gamma^{(y,z)} \cap \Gamma^{(x,z)}$
% whereas the intransitive list $(x,y,z,x)$ yields
% $\Gamma^{(x,y,z,x)} = \Gamma^{(x,y,z)} \cap \Gamma^{(z,x)}$. (Due to the nature
% of the axioms, we will only be concerned with sets $Y$ of cardinality $2, 3$ or
% $4$.)  We define $\Phi^{p}$ similarly and let
% $\textup{I}^{Y}\defeq \mcap \{ \textup{I}^{\{x,y\}}: x ,y \in Y\}$.


% (The fact that $\next_{J}$
% is symmetric ensures that $\textup{I}^{\{y,x\}} = \textup{I}^{\{x,y\}}$.)
% % As a matter of clarity
% we often suppress set notation $\{\,,\}$ in superscripts and write
%$\textup{I}^{xy}$, but we maintain the list notation. %

% Since distinct {generalization}s $\gext_{\mbbjpp}$ are associated with distinct sets
% $\grave{\textup{I}}^{xy}$ and 
% $\grave{G}^{(x,y)}$ of
% $\mbbjpp$, we will distinguish them via consistent  use of accents (as in
% \cref{lem-insep}, below).
For any pair of vectors $\acute v, J:
  \mbbtpp \rightarrow \R$ such that $J \in \mbbjpp$, the linear operator
\begin{linenomath*}
  \begin{equation*}
   J \mapsto \langle \acute v, J \rangle \defeq \sum_{\{t : J(t) > 0\}} v(t)
    \cdot J(t).
  \end{equation*}
\end{linenomath*}
is well-defined and real-valued by virtue of the fact that $J$ has finite
  support.

In our proof, we build on \gsii\ to directly prove all results regardless of
the cardinality of $X$ and $\mbbt$. To facilitate this approach, we first 
introduce the notion of an essentialization.

Let $\R^{\oplus\mbbtpp}$ denote the vectors in $\R^{\mbbtpp}$ that have finite
support and observe that $\mbbjpp \subseteq \R^{\oplus\mbbtpp}$ is the dense
subset of rational vectors.  Given a vector $\acute{v}: \mbbtpp \rightarrow
\R$, we associate the following subsets of $\R^{\oplus\mbbtpp}$: $\acute H =
\{J : \langle \acute v , J \rangle = 0 \}$, $\acute G = \{J : \langle \acute v,
J \rangle > 0\}$, and $\acute F = \{J : \langle \acute v, J \rangle \geq 0\}$.
For any finite collection $\acute{\mc V} = \{\acute{v}_1, \dots, \acute{v}_n\}$
of such vectors, let $\acute{\mc H}$ denote the associated collection or
\emph{arrangement} of hyperplanes in $\R^{\oplus\mbbtpp}$. let $\acute{S} =
\spann \acute{\mc V}$ denote the $\acute{\mathbf{r}}$-dimensional linear span
of $\acute{\mc V}$. Then $\acute S$ is a well-defined inner-product space in
its own right and let $\langle \cdot , \cdot \rangle_{\acute{S}}: \acute{S}^2
\rightarrow \R$ denote the inner product. Let $\acute{p}: \R^{\oplus\mbbtpp}
\rightarrow \acute{S}$ denote the orthogonal projection. Then observe that, for
every $i = 1, \dots, n$, and $J \in \R^{\oplus\mbbtpp}$, $\langle \acute{v}_i,
J \rangle = \langle \acute{v}_i, \acute{p}(J)\rangle$.  Moreover, note that,
since $\acute{v}_i \in S$, $\langle \acute{v}_i, J \rangle = \langle
\acute{v}_i, \acute{p}(J) \rangle_{\acute{S}}$.  The \emph{essentialization}
${\mc H}_{\acute{S}}$ of the arrangement $\acute{\mc H}$ is the arrangement we
obtain by orthogonally projecting $\acute{\mc H}$ onto $\acute{S}$.  That is,
for every $H_{\acute{S}} \in {\mc H}_{\acute{S}}$, there exists $\acute{H} \in
\acute{\mc H}$ such that $\acute{H} = \acute{p}^{-1}(H_{\acute{S}})$.  In the
literature on arrangements of hyperplanes, it is common to work with the
essentialization of an arrangement by default.  We therefore identify
$\acute{\mc H}$ and $\mc{H}_{\acute{S}}$ and suppress reference to the latter
subscript whenever no possible confusion may arise. The main benefit of
essentializations is that they will allow us to work in finite dimensions
whenever we consider a finite subset $Y\subseteq X$: regardless of the
cardinality of $\mbbtpp$.

Our domain of interest is $\nnreal^{\oplus\mbbtpp}$ and not the whole of
$\R^{\oplus\mbbtpp}$. In order to be able to apply results from the literature
on hyperplane arrangements without adjusting for boundaries, we find it useful
to work within the relative interior $\acute{S}_{\mdoubleplus}$ of
$\acute{S}_{\mplus} = \acute{p}(\nnreal^{\oplus\mbbtpp})$.  Observe
$\acute{S}_{\mdoubleplus}$ is an open subset of the
$\acute{\mathbf{r}}$-dimensional linear space $\acute{S}$.  For $\acute{H} \in
\mc H$, take $\acute{H}_{\mdoubleplus} = \acute{H}\cap
\acute{S}_{\mdoubleplus}$ to be the (strictly) \emph{positive} null-space of
$\langle \acute v , \cdot \rangle_{\acute{S}}$ and, for $0 \not \leq \acute v
\not \leq 0$, $\acute G_{\mdoubleplus}$ and $\acute F_{\mdoubleplus}$ are,
respectively, the open and closed half-spaces of $\acute{S}_{\mdoubleplus}$
associated with $\acute v$.  (For such $\acute v$, we also refer to $\acute
H_{\mdoubleplus}$ as a hyperplane in $\acute{S}_{\mdoubleplus}$.) We refer to
the non-negative counterpart of these sets as
$\acute{H}_{\mplus},\acute{G}_{\mplus}$ and $\acute{F}_{\mplus}$.  For any
finite $Y\in 2^{X}$ consider the matrix $\acute v^{\dd} : Y^{2}\times
\mbbtpp\rightarrow \R$. For a given $x, y \in Y$ and row $\acute v^{\xy}:
\mbbtpp \rightarrow \R$ of $\acute{v}^{\dd}$, the associated sets are
$\acute H^{\{x,y\}}_{\mdoubleplus}$, $\acute G^{\xy}_{\mdoubleplus}$ and
$\acute F^{\xy}_{\mdoubleplus}$ respectively.
%It is natural to ask why we do not work with the latter sets throughout.  The
%answer is that $\posreal^{\mbbtpp}$ has the same topological structure as
%$\R^{\mbbtpp}$. By working with hyperplanes in $\posreal^{\mbbtpp}$, the key
%result, Zaslavski's theorem, from the literature on arrangements (of
%hyperplanes) applies without modification.
%We briefly
%introduce this literature

%Observe that for $\countof X=2$,
%\cref{lem-insep} constitutes a proof of \cref{thm-mainQ}. For we may take
%$\mathbf{v}: X \times \mbbt \rightarrow \R$ such that $\mathbf{v}(x,\cdot)=0$
%and set $\mathbf{v}(y,\cdot)=v^{(x,y)}$, so that $v^{(x,y)}=
%-\mathbf{v}(x,\cdot) + \mathbf{v}(y,\cdot)$.
\begin{step}[characterisation of \ref{KQ}\ref{AQ}, \twodiv\ and novel
  {generalization}s]\label{step-twodiv} The following proposition corresponds to
  lemma 1 of \gsii\ and gives meaning to the statement ``the arrangement
  generated by a {generalization}''. 
 
\begin{theoremEnd}{proposition}\label{prop-insep}
  For every $Y \subseteq X$, \ref{KQ}--\ref{AQ} and \twodiv\ hold for the
  $Y$-generalization $\aext$ if, and only if, there exists $\acute{v}^{\dd}:
  Y^2 \times \mbbt \rightarrow \R$ such that,
  \begin{enumerate}[label=\textup{(\roman*)}]
    \item\label{prop-insep-rep} for every $x, y \in Y$ and $J \in \mbbjpp$, $x
      \aext_J y$ if, and only if, $\langle \acute{v}^{\xy}, J \rangle > 0$$\,;$
      \textup{and}
    \item\label{prop-insep-2div} for every $x, y \in Y$, there exists $s, t \in
      \mbbt$ such that $\acute{v}^{\xy}(s) < 0 < \acute{v}^{\xy}(t)$.
  \end{enumerate}
  Moreover, $\acute{v}^{\xy}$ is unique upto multiplication by a positive
  scalar.
\end{theoremEnd}
  \begin{proofEnd}
  Let $\mc Y = \{Y_\alpha \subseteq Y : \countof Y_\alpha = 2\}$ be the
    collection of distinct (unordered) pairs in $Y$. For every $Y_\alpha \in
    \mc Y$, \ref{T} holds simply because $Y_\alpha$ is of cardinality two;
    moreover, on $Y_\alpha$, \fourdiv\ is equivalent to \twodiv. This allows us
    to apply theorem 2 of \gsii.\footnote{\citeauthor{gilboa2003inductive}
    explicitly prove their theorem 1 holds for the case of arbitrary $X$ and
    $\mbbt$. But although their theorem 2 is stated and proved in the
    first step (the case where $X, \mbbt < \infty$) of the proof of their
    theorem 1, steps 2 and 3 of that proof apply equally to their theorem 2.}
    (Part \ref{D-insep} of the present lemma follows from claim 5 of the proof
    of lemma 1 of \gsii.) Thus, on $Y_\alpha$, \ref{KQ}\ref{AQ} and \twodiv\
    hold if, and only if, there exists of a matrix $v^{\dd}_{\alpha}:
    Y_\alpha^2 \times \mbbtpp \rightarrow \R$ with rows satisfying condition
    \ref{prop-insep-rep} of the present lemma.  In particular, \gsii\ yields a
    matrix representation $v_\alpha : Y_\alpha \times \mbbtpp \rightarrow R$.
    For $x, y \in Y_\alpha$, take $v^{\xy}_{\alpha} = - v_{\alpha}(x, \cdot) +
    v_{\alpha}(y, \cdot)$.  For condition \ref{prop-insep-2div}, note that via
    theorem 2 of \gsii, the matrix $v_\alpha$ is 2-diversified if, and only if,
    \fourdiv\ holds for $\aext$ on $Y_\alpha$. Thus, for every $\lambda \in
    \R$, $v_\alpha (x) \not \leq \lambda v_\alpha(y)$ if, and only if, \twodiv.
    Take $\lambda = 1$ and observe that this implies $v^{\xy}_{\alpha}(s) < 0 <
    v^{\xy}_{\alpha}(t)$ for some $s, t \in \mbbtpp$.
 
  To extend this to arbitrary $Y \subseteq X$, note that, for some $A'$,
    $\bigcup\{\mc Y_\alpha : \alpha \in A' \} = Y$. For an arbitrary function
    $f$, let $\graph f$ denote the graph of that function. To obtain the
    desired matrix $\acute{v}^{\dd}: Y^2 \times \mbbt \rightarrow \R$, take
  \begin{equation}
    \graph \acute{v}^{\dd} = \bigcup \{\graph v^{\dd}_{\alpha} : \alpha \in
    A'\}.
  \end{equation}
% , via
% \ref{prop-insep-rep}, there exists $J \in \mbbjpp$ such that $\langle
% v^{\xy}_\alpha, J \rangle > 0$ if, and only if, there exists $J \in
% \mbbjpp$ such that $x \asext_{J} y$. Take any $J \in \mbbjpp$ such that
% $\langle v^{\xy}_\alpha, J \rangle > 0$. The fact that $J(t') \geq 0$ for
% every $t' \in \mbbtpp$ then implies that, for some $t \in \mbbtpp$,
% $v^{\xy}_\alpha(t) > 0$. Conversely, if, for some $t \in \mbbtpp$,
% $v^{\xy}_\alpha(t) > 0$, then $x \asext_{J} y$ holds with $J = \delta_t$.
  \end{proofEnd}
We refer to a matrix $\acute{v}^{\dd}$ that satisfies condition
\ref{prop-insep-2div} of \cref{prop-insep} as a \emph{$2$-diverse matrix}. We
refer to a matrix $\acute{v}^{\dd}$ that satisfies \cref{prop-insep} as a
$2$-diverse (pairwise) matrix representation of $\ext$.

   
% The first lemma in the proof of \cref{thm-mainQ} builds on lemma 1 of \gsii.
\begin{theoremEnd}{lemma}[two-diverse pairwise representation]\label{lem-insep}
  Let $\aext$ be a $Y$-{generalization} of $\preceq_{\mbbj}$.  $\aext$
  satisfies \ref{KQ}--\ref{AQ} and \twodiv\ holds on $Y$, if, and only if,
  there exists a matrix $\acute{v}^{(\cdot,\cdot)} : Y^{2} \times \mbbtpp
  \rightarrow \R$ such that, for every $x,y\in Y$, row $\acute{v}^{(x,y)}:
  \mbbtpp \rightarrow \R$ and its associated spaces satisfy
  \begin{enumerate}[label=\textup{(\roman*)}]
     \item \label{K-insep} $\acute H^{\{x,y\}}_{\mplus} \cap \mbb Q^{\mbbtpp} =
       \{J: x \anext_{J} y\}$ and $\acute G ^{\xy}_{\mplus} \cap \mbb
       Q^{\mbbtpp}= \{ J: x \asext_{J} y\}$,
     \item\label{D-insep} $\acute{G}^{\xy}_{\mdoubleplus}$ and
       $\acute{G}^{\yx}_{\mdoubleplus}$ are both nonempty if $x \neq y$ and
       both empty otherwise,
     \item\label{skew-insep} $\acute H^{\{y,x\}}_{\mdoubleplus} =\acute
       H^{\{x,y\}}_{\mdoubleplus}$ (and in particular $\acute{v} ^ \yx = -
       \acute{v} ^ \xy$),
     \item\label{unique-insep} $\acute H^{\{x,y\}}_{\mdoubleplus}$ is the
       unique hyperplane in $\posreal^{\mbbtpp}$ that separates $\{J : x
       \asextb_{J} y\}$ and $\{ J: y \asextb_{J} x\}$.
  \end{enumerate}

Moreover, $\aext$ is novel if, and only if, for every $t \neq \novel$,
  $\acute{v}^{\dd}(t) \neq \acute{v}^{\dd}(\novel)$.
\end{theoremEnd}
\begin{proofEnd}%[Proof of \cref{lem-insep}]
  
%  In addition to \ref{KQ}\ref{AQ}, the proof of lemma 1 of \gsii\ only appeals
%  to \twodiv. That lemma, like the present one, does not require \ref{TQ}, or
%  any diversity condition stronger than \twodiv\ since it is a result about
%  distinct pairs of elements $x$ and $y$ in isolation.
  

  In the remainder of the proof of the present lemma, we suppress reference to
  the acute accent.  It remains for us to prove the characterisation of novel
  {generalization}s.  Let $\ext$ be a novel $Y$-{generalization} with matrix
  representation $v^{\dd}$ satisfying parts \ref{K-insep}\ref{unique-insep} of
  the lemma.  Fix arbitrary $t \neq \novel$.  Then \cref{def-{generalization}Q}
  implies the existence of $J \in \mbbj$ and $L = J \times 0 \in \mbbjp$ such
  that $\extb _ { L + \delta_{t} } \neq \extb _ { L + \delta_{\novel} }$.
  \Wlog, consider the case where, for some $x , y \in Y$, it holds that both $y
  \ext _ { L + \delta _ t } x$ and $x \sext _ { L + \delta_ \novel } y $.
  Equivalently,
\begin{linenomath*}
  \begin{equation*} \langle v^{\xy}, L + \delta_ t \rangle \leq 0 <\langle
    v^{\xy}, L + \delta_
  \novel\rangle \end{equation*}
\end{linenomath*}
 which, via linearity of $\langle v^{\xy}, \cdot \rangle$, we may rearrange to
  obtain
 \begin{linenomath*}
  \begin{equation}\label{eq-nov}
    v^{\xy} ( t ) \leq -\langle v^{\xy}, L \rangle < v ^ \xy ( \novel ) .
\end{equation}
\end{linenomath*}
Thus, $v^{\xy} (t) \neq v^{\xy}(\novel)$, as required for the lemma.

For the converse argument, fix arbitrary $t \in \mbbt$. Then
$\acute v^{\dd} (t) \neq \acute v^{\dd} (\novel)$ implies the existence of
distinct $x,y \in Y$ such that
$\acute v^{\xy}(t) \neq \acute v^{\xy}(\novel)$. We show that there exists
$J \in \mbbj$ and $L= J\times 0 \in \mbbjp$ satisfying \cref{eq-nov}. For then,
by retracing (in reverse order) the arguments that lead to \cref{eq-nov}, we
arrive at the conclusion that
$\extb _ { L + \delta_{t} } \neq \extb _ { L + \delta_{\novel}}$, as required
for $\ext$ to be novel.

Take $\mu = v ^ \xy ( t )$ and $\xi = v ^ \xy ( \novel )$ and consider the
case where $\mu < \xi < 0$.  Since $\mu<0$, \twodiv\ implies that there exists
$s\in\mbbt$ such that $v^{\xy}(s)$ is positive.  Then, for some
$\lambda \in \posrat$, $- \lambda v ^ \xy ( s ) \in ( \mu , \xi )$.  Let
$L = \lambda \delta ^{ \novel } _ s$ and observe that
\begin{linenomath*}
  \begin{equation*} \mu < - \langle v^{\xy}, L \rangle < \xi ,\end{equation*}
\end{linenomath*}
  as required. \emph{Mutatis mutandis}, the case where both $\mu$ and $\xi$
  are positive is the same.  If $\mu \leq 0 \leq \xi$, then take $L = 0$, so
  that $\mu \neq \xi$ yields
  $\extb _ { L + \delta ^{ \novel } _ t } \neq \extb _ { L + \delta ^{ \novel }
    _ \novel }$.
\end{proofEnd}

% \begin{remark*}
%   Note that although \cref{lem-insep} takes the {generalization} as primitive, we will
%   also find it useful to reason in the opposite direction. For instance, let
%   $v^{\dd}: Y^{2} \times \mbbt \rightarrow \R$ be a $2$-diverse representation
%   of the improper $Y$-{generalization} $\ext$ and let
%   $\acute{v}^{\dd}: Y^{2} \times \mbbtp \rightarrow \R$ be any matrix that
%   agrees with $v^{\dd}$ on $Y^{2} \times \mbbt$. That is, any matrix $\acute{v}$
%   with columns $\acute{v}^{\dd}(t) = v^{\dd}(t)$ for every $t \in \mbbt$. Now
%   take $\aextb \in (Y\times Y)^{\mbbjp}$ such that for every $x,y\in Y$,
%   $x \aext_{J} y$ if, and only if, $\langle \acute{v}^{\xy}, J \rangle \geq
%   0$. Via \cref{lem-insep}, $\aext$ is a proper $Y$-{generalization} of
%   $\preceq_{\mbbj}$.
% \end{remark*}
\paragraph{An \emph{arrangement}\hskip-4pt} is a collection of hyperplanes in
$\posreal^{\mbbtpp}$ or $\R^{\mbbtpp}$. (A \emph{hyperplane in
  $\posreal^{\mbbtpp}$} is the positive kernel of some nonzero vector.) The
chief result, from the mathematics of arrangements, to which we extensively
appeal is Zaslavsky's theorem two form of which we state below.  For any given
{generalization} $\ext$, Zaslavsky's theorem allows us to use information about the
intersections of hyperplanes in the arrangement to identify
$\countof \total (\ext)$.  It does so by counting the collection
$\mc G_{\mdoubleplus}$ of open and connected subsets of
$\R^{\mbbtpp} \bs \bigcup \{ H_{\mdoubleplus} : H_{\mdoubleplus} \in \mc A \}$
are the \emph{chambers} or \emph{regions} of the arrangement.  In the present
setting, each chamber corresponds to a \emph{CAR ranking} of the elements of
$Y$: a complete, antisymmetric and reflexive (but possibly intransitive) ranking
$R$. Every CAR ranking can be succinctly represented as a CAR list. For
instance, take $Y = \{ x, y , z\}$ and $x \mathbin{R} y \mathbin{R} z$, then
the corresponding list
\begin{linenomath*}
\begin{equation*}
l = \left \{
\begin{array}{ll}
  (x,y,z)& \text{if $R$ is transitive}\\
  (x,y,z,x)& \text{if $R$ is intransitive.}\footnotemark
\end{array}\right.
% \footnotetext{Note that the intransitive list $(x,y,z,x)$ is indistinguishable
%   from $(y,z,x,y)$ and $(z,x,y,z)$, but it is the inverse of $(x,z,y,x)$.} 
\end{equation*}
\end{linenomath*}
  The notation {generalizes} without exception to sets of cardinality $4$. \Eg,
  $(x,y,z,x,w)$ represents the CAR ranking that is intransitive over
  $\{x,y,z\}$ and such that $w$ dominates every other member.

\paragraph{The intersection semilattice\hskip-4pt}
of any arrangement $\mc A$ is the partially ordered (\emph{by reverse
  inclusion}) set $\mc L$ of intersections of members of $\mc A$. The unique
minimal element is obtained by taking the intersection $A^{\emptyset}$ over the
empty subarrangement $\mc A^{\emptyset}$ of $\mc A$ to obtain the ambient space
itself. That is $A^{\emptyset}= \R^{\mbbtpp}$ or $\posreal^{\mbbtpp}$,
depending on whether we are considering the lattice $\mc L$ or the lattice
$\mc L_{\mdoubleplus}$ respectively. In \gsii, as a consequence of \fourdiv,
$\mc H_{\mdoubleplus}$ is always central. In our setting, it is only $\mc H$
that is guaranteed to be central. In general an arrangement is central, if, and
only if, its intersection semilattice has a unique maximal element
\citep[proposition 2.3]{stanley2007introduction}. Thus, if $\mc H_{\mdoubleplus}$
is \emph{centerless}, then $\mc L_{\mdoubleplus}$ is a meet semilattice with
multiple maxima: as in \cref{eg-zaslavski}.  Extending our notation: if
$Y = \{x,y,z,w\}$, then the unique intersection $A^{Y}$ is the (nonempty)
\emph{center} of $\mc A^{Y} = \mc H$. By $A^{\{x,y,z\}}$, we mean the
intersection over $\mc A^{\{x,y,z\}} \defeq \{H^{\{i,j\}}: \text{$i\neq j$ in
 $\{x,y,z\}$}\}$. Finally, by $A^{\{x,y\}\{z,w\}}$, we mean the intersection
over $\mc A^{\{x,y\}\{z,w\}} \defeq \{H^{\{x,y\}},H^{\{z,w\}}\}$.

\paragraph{Zaslavski's theorem} provides two distinct methods for counting the
number of regions in an arrangement. The first states that
\emph{$\countof \mc G$ is equal to the sum of the absolute values of the
  M\"{o}bius function $\bmu: \mc L \rightarrow \mbb Z$} which is defined
recursively via
\begin{linenomath*}
  \begin{equation}\label{eq-mobius}
\bmu(A) = \left \{
\begin{array}{ll}
  1  & \text{if $A = A^{\emptyset}$}\\
  -\sum\{\bmu(B):  A \subsetneq B\} & \text{otherwise.}\footnotemark
\end{array}\right.
\end{equation}
\end{linenomath*}
% For $Y\subset X$, a given subarrangement $\mc A^{Y} = \{H^{\{i,j\}}:
% \text{$i \neq j$in$Y$}\}$, we abbreviate and write
% $\tilde\bmu(Y)\defeq \bmu( A^{Y})$. Fot the intersection $A^{\{x,y\}\{z,w\}}$
% of the subarrangement $\mc A^{\{x,y\} \{z,w\}} = \{H^{\{x,y\}}, H^{\{z,w\}}\}$,
% we will write $\tilde\bmu (\{x,y\}\{z,w\}) \defeq \bmu (A^{\{x,y\}\{z,w\}})$.
The above definition of Zaslavski's theorem is explicitly provided by
\citet{sagan1999why}. Specialised to the present setting, the more common
\citep[see ][]{orlik1992arrangements,dimca2017hyperplane,stanley2007introduction} ``rank''
version of Zaslavski's theorem is
\begin{linenomath*}
  \begin{equation*}\countof \mc G  =\sum_{\substack{\mc A \subseteq \mc H\\
        \mc A \textup{\ central}}} (-1)^{\lvert \mc A \rvert - \rank(\mc A)},\end{equation*}
  \end{linenomath*}
  where \emph{central} means that $\bigcap \{H: H \in \mc A\}$ is nonempty, and
  $\rank (\mc A)$ is the dimension of the space spanned by the normals to the
  hyperplanes in $\mc A$.\footnote{Equivalently, $\rank(\mc A)$ is the
    dimension of the orthogonal complement in $\R^{\mbbtpp}$ (or
    $\R^{\mbbtpp}_{\mdoubleplus}$) of the intersection over $\mc A$. Since the
    intersection over the empty arrangement $\mc A^{\emptyset}$ is the ambient
    space $\R^{\mbbtpp}$ (or $\posreal^{\mbbtpp}$), the only vector in
    $\R^{\mbbtpp}$ that is orthogonal to the ambient space is $0$,
    $\rank (\mc A ^{\emptyset}) = 0$.}

    % \begin{comment
%}   For any finite $Y\in 2^{X}$ and $Y$-{generalization}
% $\ext$ with $2$-diverse representation $v^{\dd}$, consider the (finite)
% arrangement
% $\mc H_{\mdoubleplus} \defeq \{ H_{\mdoubleplus}^{\{x,y\}} : x \neq y\} \subset \posreal^{\mbbtpp}$ of
% hyperplanes generated by $\{v^{\xy}: x\neq y\}$. We will sometimes work with the
% \emph{essentialization} $\ess(\mc H_{\mdoubleplus})$ of $\mc H_{\mdoubleplus}$ which has the nice
% property that it is irreducible (in terms of dimension) whilst preserving all
% the relevant structure of the arrangement. The usual essentialization is the
% hyperplane arrangement that arises via the orthogonal projection
% $\pi : \R^{\mbbtpp} \rightarrow V$, where $V$ is the row space of
% $v^{\dd}$. ($V$ is the $\spann v^{\dd}$ of rows in $v^{\dd}$.) Since our
% hyperplanes are positive kernels in $\posreal^{\mbbtpp}$, we modify this notion
% to obtain a map $\pi_{\mdoubleplus}: \posreal^{\mbbtpp} \rightarrow V_{\mdoubleplus}$, where
% $V_{\mdoubleplus} = V\cap \posreal^{\mbbtpp}$. \twodiv\ then ensures that, for every
% $x \neq y$ in $Y$, $H_{\mdoubleplus}^{\{x,y\}}$ and
% $H_{\mdoubleplus}^{\{x,y\}} \defeq \pi_{\mdoubleplus}(H_{\mdoubleplus}^{\{x,y\}})$ are uniquely defined
% hyperplanes in $\posreal^{\mbbtpp}$ and $V_{\mdoubleplus}$ respectively. We will typically,
% suppress reference to $V$ and take $H_{\mdoubleplus}^{\xy}$ to denote a hyperplane in
% $V_{\mdoubleplus}$. This entails no loss of generality in so far as the structure of the
% arrangement is concerned.
% % \footnote{Consider the simplest possible arrangement consisting of
%   % just one hyperplane in $\R^{n}$. In this case, the essentialization is a
%   % central arrangement consisting of a single point at the origin and the ambient
%   % space is some one-dimensional subspace of $\R^{n}$.}
% \end{comment}

\begin{example}[a comparison of $\mc L$ and $\mc L_{\mdoubleplus}$]\label{eg-zaslavski}
  Let $X = \{ x , y , z \}$, $\mbbt = \{s,t\}$, and
  $u ^{ (x,y) } = 1 \times - 1$ and $u ^{(y,z) } = 2 \times - 1$ denote
  vectors in $\R^{\mbbt}$. We now appeal to the Jacobi identity and take
  $u ^{(x,z) } = u ^{ (x,y) } + u ^{ (y,z) } = 3 \times - 2$ and extend to the
  remaining pairs in $X^{2}$ using \cref{lem-insep}.  Since these vectors are
  pairwise noncollinear and $\countof \mbbt = 2$, the associated arrangement
  $\mc H_{\mdoubleplus} = \left\{ H_{\mdoubleplus}^{\{x,y\}},
    H_{\mdoubleplus}^{\{y,z\}}, H_{\mdoubleplus}^{\{x,z\}}\right\}$ consists of
  three pairwise disjoint lines that partition $\posreal^{\mbbt}$ and
  $\mc G_{\mdoubleplus}$ has cardinality $4$.  We now confirm this using
  Zaslavski's theorem.
  \begin{figure}
    \begin{center}
  \begin{tikzpicture}
    \node (max) at (0,1.5) {\color{lightgray}$\{x,y,z\}$};
    \node (d) at (-2,0.25) {$\{x,y\}$};
    \node (e) at (0,0.25) {$\{y,z\}$};
    \node (f) at (2,0.25) {$\{x,z\}$};
    \node (min) at (0,-1) {$\emptyset$};
    \draw (min) -- (d);
    \draw (min) -- (e);
    \draw (min) -- (f);
    \draw[color = lightgray] (max)  -- (d);
    \draw[color = lightgray] (max)  -- (e);
    \draw[color = lightgray] (max)  -- (f);    
    %\draw[preaction={draw=white, -,line width=6pt}] (a) -- (e) -- (c);
\end{tikzpicture}
\caption{\label{fig-hasse-Y3-r2} The intersection semilattice
  $\mc L_{\mdoubleplus} = \mc L \bs A^{\{x,y,z\}}$.}
\end{center}
\end{figure}

In the present setting $A^{\emptyset}_{\mdoubleplus} = \posreal^{\mbbt}$ and,
via \cref{eq-mobius}, $\bmu(A^{\emptyset}_{\mdoubleplus}) = 1$. Then, since
$\posreal^{\mbbt}$ is the unique element in $\mc L_{\mdoubleplus}$ that
(strictly) contains each hyperplane in $\mc H_{\mdoubleplus}$, \cref{eq-mobius}
yields $\bmu(A) = -\bmu(A^{\emptyset}_{\mdoubleplus})$ for each
$A \in \mc H_{\mdoubleplus}$.  Now since the hyperplanes in
$\mc H_{\mdoubleplus}$ are disjoint, there are no further elements in
$\mc L_{\mdoubleplus}$ and we observe that
  \begin{linenomath*}
    \begin{equation*}\lvert \mc G_{\mdoubleplus} \rvert= \sum_{A \in \mc L_{\mdoubleplus}} \lvert
      \bmu(A) \rvert = 4.\end{equation*}
  \end{linenomath*}

  In contrast, although the structure of $\mc L$ is otherwise isomorphic to
  $\mc L_{\mdoubleplus}$, since $\{0\} \subset \R^{\mbbt}$ is a subset of every
  hyperplane in $\mc H$, $\{0\}$ is the center $A^{\{x,y,z\}}$ of $\mc H$ and
  the maximal element of $\mc L$. Via \cref{eq-mobius} and the calculations of
  the previous paragraph, we obtain
  $\bmu(A^{\{x,y,z\}}) = - \left(\bmu(A^{\emptyset}) -3 \bmu(A^{\emptyset})
  \right) = 2$.  Thus,
\begin{linenomath*}
  \begin{equation*}  \countof\mc G = \sum_{A \in \mc L} \lvert \bmu(A) \rvert = 6 = 3!.\end{equation*}
\end{linenomath*}
  % First note that
  % there are four sub-arrangements $\mc A \subseteq \mc H$ that are central: the
  % three singleton arrangements $\mc A^{\{x,y\}}, \mc A^{\{y,z\}}$ and
  % $\mc A^{\{x,z\}}$ that each consist of a single hyperplane with the same
  % index; and the empty arrangement (which contains no hyperplanes). Now
  % $\lvert \mbbt \rvert = 2$, and, for each singleton arrangement,
  % $\lvert \mc A^{\{i,j\}} \rvert - \rank ( \mc A^{\{i,j\}}) = 1 - 1$. Whereas,
  % for the empty arrangement
  % $\lvert \mc A^{\emptyset} \rvert - \rank (\mc A^{\emptyset}) = 0 - 0$. As a
  % consequence,
  % \begin{equation*}\lvert\mc G_{\mdoubleplus}\rvert = (-1)^{4} \left(3\cdot ( - 1 )^{1 - 1} + ( - 1 )^{0 -
  %     0}  \right) = 4 < 3! = 6.\end{equation*}
\end{example}
\begin{remark}[The relationship between $\mc L$ and $\mc L_{\mdoubleplus}$]\label{rem-lattice}
  Let $\aext$ be a $Y$-{generalization} with $2$-diverse representation
  $\acute{u}^{\dd}$. Since, for every distinct $x$ and $y$ in $Y$,
  $\acute{H}^{\{x,y\}}$ contains the origin, $\acute{\mc H}$ is centered. As we
  see in \cref{eg-zaslavski}, this is not the case for
  $\acute{\mc H}_{\mdoubleplus}$ where $\acute{\mc H}_{\mdoubleplus}$ is
  centerless and each of its members is maximal in
  $\acute{\mc L}_{\mdoubleplus}$.

  In \gsii, \fourdiv\ guarantees that, for every $Y\subseteq X$ of cardinality
  $2,3$ or $4$, the improper $Y$-{generalization} generates a centered arrangement in
  $\posreal^{\mbbt}$. The fact that $\posreal^{\mbbtpp}$ is open in
  $\R^{\mbbtpp}$ ensures that the dimension of any $L \in \acute{\mc L}$ is
  equal to its counterpart $L _{\mdoubleplus} \in \acute{\mc L}_{\mdoubleplus}$
  provided the latter exists.  Thus, $\acute{\mc L}_{\mdoubleplus}$ and $\acute{\mc L}$ are isomorphic if, and only if, $\acute{\mc H}_{\mdoubleplus}$
  is centered. For the same reason, $\acute{\mc G}_{\mdoubleplus}$ and
  $\acute{\mc G}$ are isomorphic if, and only if, $\acute{\mc
  H}_{\mdoubleplus}$ is centered.  \end{remark}
  % \begin{comment} \begin{remark}[Further notation on arrangements of
  % hyperplanes] To allow us to appeal to  the literature on arrangements of
  % hyperplanes, we recall some useful notation.  \begin{definition} Let $\mc A
  %$ be an arrangement (of hyperplanes) in a vector space $V$ and let $\mc
  % L = \mc L(\mc A)$ be the set of nonempty
  %   intersections of elements of $\mc A$.\footnote{The convention is to take
  %   $V = \bigcap \{ H_{\mdoubleplus} \in \mc A : H_{\mdoubleplus} \in
  %   \emptyset \}$, so that $V\in \Lambda$.}  Define a partial order on $\mc L
  %  $ by reverse inclusion: \begin{equation*}\text{$B \leq B'$ if, and only if, $%   B'\subseteq B$.}\end{equation*}
  % \end{definition} Note that, for every arrangement $\mc A$, $V$ is the
  % unique minimal element of $\mc L ( \mc A )$.  $\mc A$ is a central
  % arrangement, if, and only, if $\mc L$ has a unique maximal element.  We
  % partition the proof into two parts, depending on whether there exists a
  % novel $Y$-{generalization} $\hext$ of
  % $\preceq_{\mbbj}$ that generates a central arrangement and that also
  % satisfies the property \begin{equation}\label{eq-maximal-ext} \text{for
  % every $Y$-{generalization} $\ext$,\, $\countof \total ( \ext ) \leq \countof
  % \total (\hext )$.} \end{equation} The proof is somewhat more involved in
  % the case where $\preceq_{\mbbj }$
  % has no $Y$-{generalization} $\hext$ that generates a central arrangement $\mc
  % A$ in addition to property \eqref{eq-maximal-ext}.  In both cases, the key
  % is to construct testworthy {generalization}s such that every member of $\mc L$
  % intersects the interior of $\conv( \mbbjp)$. In the absence of \fourdiv,
  % regular {generalization}s may fail to satisfy this property.  When
  % this property holds, we are able to appeal to Zaslavski's theorem, and thus
  % count the open and connected sets of an arrangement.  \end{remark}
  % \end{comment}
  We now abstract a useful property from \cref{eg-zaslavski}.
  \begin{theoremEnd}{proposition}[polar opposite
    rankings]\label{prop-pairwise-extremal}

  If $\preceqb_{\mbbj}$ satisfies \ref{KQ}\ref{AQ} and \twodiv, then, for
    every $Y\subseteq X$ of cardinality $3$ or $4$, the improper $Y$-{generalization}
    $\extb$ is such that, for some $J,L \in\mbbj$, $\extb_{J} = \extb_{L}^{-1}
   $ belongs to $\total(\ext)$.

\end{theoremEnd}
\begin{proofEnd}%[Proof of \cref{prop-pairwise-extremal}]
  Fix $\countof Y= 3$ or $4$, via \cref{lem-insep}, let $v^{\dd}$ denote the
  $2$-diverse matrix representation of the improper $Y$-{generalization} $\ext$. Let
  $\mc H_{\mdoubleplus}$ denote the associated arrangement of hyperplanes.  For
  every distinct $x,y\in X$, \cref{lem-insep} implies that
  $H^{\{x,y\}}_{\mdoubleplus}$ intersects $\posreal^{\mbbt}$. Then, similar to
  \cref{eg-zaslavski}, the $1 \leq n \leq \binom{\countof Y}{2}$ distinct
  hyperplanes of $\mc H_{\mdoubleplus}$ cut $\posreal^{\mbbt}$ into at least
  $n+1$ regions.  At least one pair $G$ and $G^{*}$ in $\mc G_{\mdoubleplus}$
  are therefore separated by all $n$ distinct members of $\mc
  H_{\mdoubleplus}$. Take $J \in G$, so that, for every distinct $x,y \in Y$,
  $\langle u^{\xy}, J \rangle \neq 0$.  Thus $\ext_{J}$ is antisymmetric,
  complete and, via \ref{TQ}, total. Next, take $L \in G^{*}$, so that since $J$
  and $L$ are separated by every hyperplane in $\mc H_{\mdoubleplus}$,
  $\extb_{J}= \extb_{L}^{-1}$.
\end{proofEnd}
% We now show that \twodiv\ is too weak for the purposes of \cref{thm-mainQ}.
\begin{example}[insufficiency of \twodiv]\label{eg-lexicographic}
  Let $X = [0,1]^{2}$ and let $\leq^{\textup{lex}}$ denote the lexicographic
  ordering on $X$.  Let $\mbbt = \{s,t\}$, and, for each $J \in \mbbj$, let
\begin{linenomath*}
\begin{equation*}
\preceqb_{J} = \left\{ 
  \begin{array}{ll}
    X^{2} & \text{if  $J(s) = J( t )\,;$}\\
    \leq^{\textup{lex}} & \text{if  $J(s) < J( t )\,;$}\\
    (\leq^{\textup{lex}})^{-1} & \text{otherwise.}
    \end{array}\right.
\end{equation*}
\end{linenomath*}
Recall that if $\preceqb_{J} = X^{2}$, then $\preceqb_{J}$ is symmetric and
hence equal to $\simeq_{J}$.  Thus, for every distinct $x,y\in X$,
$H^{\{x,y\}}_{\mdoubleplus}= \{J \in \R^{\mbbt}_{\mdoubleplus} : J(s) =
J(t)\}$. Via \cref{lem-insep}, $\preceqb_{\mbbj}$ has a two-diverse matrix
representation $v^{\dd}$. But via \cref{lem-c2dQ}, below, \condtwodiv\ fails to
hold. The fact that $\preceqb_{\mbbj}$ fails to satisfy part \ref{rep-mainQ} of
\cref{thm-mainQ} follows from the fact that $\preceqb_{J}$ is lexicographic for
every $J$ outside $H$.
\end{example}
\end{step}
\begin{step}[for $\mbbt<\infty$, characterisations of \ref{c2dQ}]\label{step-condtwodiv}
  A matrix $v^{\dd}$ that satisfies the conditions of the next lemma is a
  \emph{conditionally-$2$-diverse (pairwise) representation}.

  \begin{theoremEnd}{lemma}[conditionally-$2$-diverse representation]\label{lem-c2dQ}
   Let $\ext$ be a $Y$-{generalization} of $\preceq_{\mbbj}$ with $2$-diverse matrix
    representation $v^{\dd}$.  Then \ref{c2dQ} holds on $Y$ if, and only if, for
    every three distinct elements $x,y,z\in Y$, $v^{\xz}$ and $v^{\yz}$ are
    noncollinear.
  \end{theoremEnd}
  \begin{proofEnd}%[Proof of \cref{lem-c2dQ}]
\label{proof-lem-c2dQ}
    Let \ref{c2dQ} hold on $Y$. Since $v^{\dd}$ is a $2$-diverse pairwise
    representation, $v^{\xz}, v^{\yz}\not \leq 0$.  By \ref{c2dQ}, one of
    $G_{\mplus}^{\xz}$ and $G_{\mplus}^{\zx}$ contains both $J,L$ such that
\begin{linenomath*}
  \begin{equation*}\langle v^{(y,z)} ,  L \rangle <0< \langle v^{(y,z)} , J\rangle.\end{equation*}
\end{linenomath*}
\Wlog, suppose $J, L$ belongs to $G_{\mplus}^{\xz}$. Then
$\langle v^{\xz},\cdot \rangle$ is positive on $\{L,J\}$, so that, for every
$\lambda \in \R$ $v^{\xz} \neq \lambda v^{\yz}$, as required.

  
    Conversely, let $x,y,z \in Y$ be such that $v^{\xz}$ and $v^{\yz}$ are
    noncollinear.  Then $H_{\mdoubleplus}^{\{x,z\}}\neq H_{\mdoubleplus}^{\{y,z\}}$, and
    there exists $L \in H_{\mdoubleplus}^{\{x,z\}}\bs H_{\mdoubleplus}^{\{y,z\}}$.
    % such that $L$ has finite support.
    % \footnote{For the case where $\countof \mbbt = \infty$, the
    % fact that some
    % $L\in H_{\mdoubleplus}^{\{x,z\}}\bs H_{\mdoubleplus}^{\{y,z\}}$ has finite
    % support follows directly
    % from \gsii. For if $H_{\mdoubleplus}^{\{x,z\}}$ and
    % $H_{\mdoubleplus}^{\{y,z\}}$ differ, but only on vectors of
    % infinite support, then the uniqueness part of lemma 1 of \gsii\ fails to
    % extend to infinite $\mbbt$, in contradiction of step 2 of the proof of the
    % main theorem of \gsii.}\end{comment}
    \Wlog, therefore, suppose
    $L \in H^{\{x,z\}}_{\mdoubleplus} \cap G^{\yz}_{\mdoubleplus}$.  Since
    $v^{\dd}$ is $2$-diverse, there exists $s,t\in \mbbt$ such that
    $v^{\xz}(s)<0<v^{\xz}(t)$. Noting that $L \in \posreal^{\mbbtpp}$, so that
    $v^{\xz} \neq \delta_{s},\delta_{t}$, let $\psi_{s}$ and $\psi_{t}$ be the
    convex paths from $L$ to $\delta_{s}$ and $\delta_{t}$ respectively. For
    sufficiently small  $\lambda > 0$,
    $\langle v^{\yz},\psi_{s'} (\lambda) \rangle$ remains positive for
    $s'= s, t$ and, moreover, since $L \in H^{\{x,z\}}_{\mdoubleplus}$,
\begin{linenomath*}
    \begin{equation*}\langle v^{\xz}, \psi_{s}(\lambda)\rangle<0< \langle v^{\xz},
      \psi_{t}(\lambda) \rangle. \end{equation*}
\end{linenomath*}
Finally, since $L$ has finite support, a finite sequence of perturbations of the
elements of $\psi_{s}(\lambda)$ and $\psi_{t}(\lambda)$ yields (rational-valued)
members of $\mbbjpp$ with the same properties, as required for \ref{c2dQ}.
\end{proofEnd}
The following is a translation of \cref{obs-c2d}.
%*********************
\begin{theoremEnd}{proposition}[on \ref{c2dQ} and \ref{p3dQ}]\label{prop-c2dQ}
  For $\preceqb_{\mbbj}$ satisfying \ref{TQ}--\ref{AQ}, \condtwodiv\ and
  \parthreediv\ are equivalent.
\end{theoremEnd}
\begin{proofEnd}%[Proof of \cref{prop-c2dQ}]
  \label{proof-prop-c2dQ}
  When $X=2$, \ref{c2dQ} and \ref{p3dQ} are identical to \twodiv.  Let
  $Y = \{ x, y , z \} \subseteq X$ and let $\ext$ denote the improper
  $Y$-{generalization} of $\preceq_{\mbbj}$. We begin by assuming \ref{c2dQ} and
  showing that $\countof Y + 1 = 4 \leq \countof \total ( \ext )$. Via
  \cref{lem-c2dQ}, there are three distinct hyperplanes
  $H_{\mdoubleplus}^{\{x,y\}}, H_{\mdoubleplus}^{\{y,z\}}$ and
  $H_{\mdoubleplus}^{\{x,z\}}$ in the associated arrangement
  $\mc H_{\mdoubleplus}$. Then, as in \cref{eg-zaslavski},
  $\emptyset = \posreal^{\mbbt}$ is the unique element of $\mc L_{\mdoubleplus}$ that
  lies below each member of $\mc H_{\mdoubleplus}$. Thus, via \cref{eq-mobius},
  $\bmu(A^{\emptyset}) = 1$ and $\bmu(A) = - \bmu(A^{\emptyset})$ for all three hyperplanes
  $A \in \mc H_{\mdoubleplus}$. Thus, Zaslavski's theorem implies that
  $\countof \mc G_{\mdoubleplus}$ is bounded below by $4$.  Thus
  $\total(\ext) \geq 4$, and since, for every $Y$-{generalization} $\aext$,
  $\countof \total (\aext ) \geq \countof \total (\ext)$, \ref{p3dQ} holds.

  Conversely, suppose \ref{p3dQ} holds and, once again let $\ext$ denote the
  improper $Y$-{generalization} of $\preceq_{\mbbj}$, so that
  $\total(\ext) \geq \countof Y = 3$. Now \ref{p3dQ} implies \twodiv, so that,
  via \cref{lem-insep}, there exists a $2$-diverse matrix representation with
  associated arrangement $\mc H_{\mdoubleplus}$. It is not the case that
  $\lvert \mc H_{\mdoubleplus} \rvert = 1$, for this would imply that
  $\countof \total(\ext) = 2$. \Wlog, suppose
  $H_{\mdoubleplus}^{\{x,y\}} \neq H_{\mdoubleplus}^{\{y,z\}}$. Observe that
  \ref{TQ} then implies
  $H_{\mdoubleplus}^{\{x,z\}} \neq H_{\mdoubleplus}^{\{x,y\}}$ and
  $H_{\mdoubleplus}^{\{x,z\}} \neq H_{\mdoubleplus}^{\{y,z\}}$. This implies
  that $v^{\xy}, v^{\yz}$ and $v^{\xz}$ are pairwise noncollinear. Finally, an
  application of \cref{lem-c2dQ} then yields \ref{c2dQ}.
\end{proofEnd}
  %
  \end{step}

  \begin{step}[for $\mbbt<\infty$, a characterisation of \stability]\label{step-stability}
   The following Jacobi identity plays a central role in the proof of \gsii.
  % Then, via an example, we introduce
  % some key concepts from the literature on hyperplane arrangements.\footnote{A
  %   more detailed introduction can be found in the classic text
  %   \citet{orlik1992arrangements}, the recent \citet{dimca2017hyperplane} and the
  %   lecture notes \citet{stanley2007introduction}.}
\begin{definition*}%\label{def-jac-rep} %We will use the following notation.

  For $Y \in 2 ^ { X }$, the matrix
  $v^{ \dd } : Y^{2}\times \mbbtpp \rightarrow \R$ satisfies the \emph{Jacobi
    identity} whenever, for every $x , y , z \in Y$,
  $v ^{ \xz } = v ^{ \xy } + v ^{ \yz }$.
\end{definition*}
For any given $Y$-{generalization} $\ext$, the \emph{Jacobi identity holds for
  $\ext$} whenever it holds for some pairwise representation $v^{\dd}$ of
$\ext$. Moreover, in this case, $v^{\dd}$ is a \emph{Jacobi
  representation}. Finally, if the $Y$-{generalization} $\ext$ is improper and the
Jacobi identity holds for $\ext$, we simply say that the \emph{Jacobi identity
  holds on $Y$}.  Consider
\begin{k-jac*}

  For every $Y \subseteq X$ with $3 \leq \countof Y \leq k$, the Jacobi
  identity holds on $Y$.
   
\end{k-jac*}
We will work with \threejac\ and \fourjac\ in particular. The following lemma is
the special case of \cref{thm-foureq} of \cref{sec-proof-foureq} where $\mbbt$
is finite. When $\mbbt$ is finite, for every $Y$, the set of testworthy
$Y$-{generalization}s that are novel is nonempty. In this case, \stability\ implies
\ref{TQ}--\ref{AQ} via nonrevision of rankings (part \ref{item-preservingQ}  of
\cref{def-{generalization}Q}).
 \begin{lemma}\label{lem-foureq}
   For $\preceq_{\mbbj}$ satisfying \ref{c2dQ}, \stability\ holds if, and only if,
   \fourjac\ holds.
 \end{lemma}
% \begin{comment}
% The next example shows that, when the hyperplanes in the arrangement are
% nonunique, \fourjac\ is insufficient for \stability.
%  % counterexample-nonunique-available
% \begin{example}[Nonequivalence of \stability\ and \fourjac\ in
%   general]\label{eg-noneq-of-4pru}
%   Let $\countof \mbbt = 3$, $Y = \{ x , y , z, w \}$ and let $\u ^{ \dd }$ be
%   a Jacobi representation of the improper $Y$-{generalization} $\dext$ of
%   $\preceq_{\mbbj}$ such that $\u ^{ \xy } = 0 \times 1 \times 1$,
%   $\u ^{ \yz } = 1 \times - 2 \times - 4$ and $\u ^{ \zw } = 2 \u ^{ \xy }$.
%   By the Jacobi identity $\u ^{ \xy } + \u ^{ \yz } = \u ^{ \xz }$ we have
%   $\u ^{ \xz } = 1 \times - 1 \times - 3$.  Similarly,
%   $\u ^{ \xz } + \u ^{ \zw } = \u ^{ \xw }$ implies
%   $\u ^{ \xw } = 1 \times 1 \times -1$ and
%   $\u ^{ \yx } + \u ^{ \xw } = \u ^{ \yw }$ implies
%   $\u ^{ \yw } = 1 \times 0 \times - 2$. For confirmation of the Jacobi
%   identity, note that
%   $u^{\yz} + u^{\zw} = (1 + 0 ) \times (- 2 + 2) \times (-4 + 2) = u^{\yw}$.

%   Since $u^{\xz}, u^{\xw}, u^{\yz}$ and $u^{\yw}$ have both positive and
%   negative entries, the corresponding hyperplanes are uniquely defined. As a
%   consequence, every other Jacobi representation $u^{\dd}$ of $\dext$ satisfies
%   $u^{\dd} = \lambda u^{\dd}$ for some $\lambda > 0$. \Cref{lem-distributes}
%   implies that, for every proper $Y$-{generalization} $\ext$ of
%   $\preceq_{\mbbj}$ that has a Jacobi representation can be written in the form
%   $v ^{ \dd } = \u ^{ \dd } \times \eta ^{ \dd}$. Via Zaslavski's theorem (see
%   \cref{step-} of the proof of \cref{thm-foureq}), collinearity of
%   $\u ^{ \xy }$ and $\u ^{ \zw }$ implies that $\countof \total (\ext)<
%   24$.

%   The present counterexample works because, we can show that there exists a {generalization}
%   $\hext$ that is testworthy, novel and such that
%   $\countof \total (\hext) = 24$: so that \fourjac\ holds even though
%   \stability\ does not.  Take
%   $u^{\dd}$ to satisfy $u ^{ \xy } = 0 \times 2 \times 1$,
%   $u ^ {\yx } = - u ^ { \xy }$ and let the remaining rows of $u^{\dd}$ equal
%   their counterparts in $\u^{\dd}$.  Note that $u ^ {\xy}$ and $u ^{\zw}$ are
%   noncollinear.  In fact, since the matrix with rows $u ^{ \xy }, u ^{ \yz }$
%   and $u ^{ \xz }$ has determinant $- 1$, the rank of these vectors is
%   $3$. Since $\countof \mbbt = 3$, arguments in \cref{step-} of the proof of
%   \cref{thm-foureq} imply the existence of a testworthy $Y$-{generalization} $\hext$
%   that satisfies $\countof \total(\hext)= 24$, but not \ref{TQ}.
%   %with representation
%   % $\hat v^{\dd} = u^{\dd}\times \hat \eta ^{\dd}$ such that . The same is true of
%   % the matrix with rows $\u ^ { \xy }, \u ^ { \yw}$ and $\u ^{ \xw }$.  Then
%   % every $1$-dimensional {generalization} $\v ^ {\dd }$ inherits the property that the
%   % corresponding triples also have $3$-dimensional span (and, via the arguments
%   % in \cref{thm-foureq}, fail to satisfy the Jacobi identity).  We will choose
%   % $\v^{\dd}$ such that it generates an arrangement with centre in the relative
%   % interior of $\mbbj$, for then Zaslavski's theorem implies that the
%   % arrangement has $28$ chambers.  Two of these chambers are associated with the
%   % intransitive cycles on $\{ x , y , z \}$ and another two with intransitive
%   % cycles on $\{ x , y , w \}$.  The remaining $24$ chambers correspond to the
%   % $4!$ transitive cycles on $Y$.  In particular, we now construct a
%   % testworthy $Y$-{generalization} $\ext$ of $\preceq _{ \mbbj }$ such that
%   % $\countof \total ( \ext ) = 24$.
%   Indeed, for every $i , j \in Y$ and for
%   $J = 1 \times 1 \times 1 \in \mbbj$, let
%   $\check \eta ^{(i,j) } = - \langle u^{(i,j)}, J \rangle$; then for
%   $L = (J \times 0) + \delta _{ \novel } = 1 \times 1 \times 1 \times 1$ in
%   $\mbbjp$, $\check v^{\dd} = u^{\dd} \times \check \eta^{\dd}$ satisfies
%   $\langle \check v^{(i,j)}, L \rangle = 0$.  As such, $L$ is a center of the
%   arrangement $\mc H_{\mdoubleplus}$ that the rows of $\check v^{\dd}$ generate
%   in $\R^{\mbbtp}$.  Since $L \in \relint \mbbjp$, every chamber of
%   $\mc H_{\mdoubleplus}$ has nonempty open intersection with $\mbbj$.
%   Moreover, note that $\check \eta ^{\xy} = - 3$, $\check \eta ^{\xz} = 3$,
%   $\check\eta ^{ \xw } = - 1$, $\check \eta ^{\yz } = 5$,
%   $\check\eta ^{\yw } = 1$ and $\check\eta ^{ \zw } = - 4$.  Then the
%   associated $Y$-{generalization} $\hext$ is such that $\hext _{ \novel }$ is total
%   as these numbers imply
%   $y \hsext _{ \novel } w \hsext _{ \novel } x \hsext _{ \novel } z$.
%   Finally, it remains to be shown that $\hext$ is novel.  Let
%   $M = (J \times 0) + \delta_{1} = 2 \times 1 \times 1 \times 0$. Then
%   $\hextb_{M} \neq \hextb_{L}$ because $\langle \check v^{\xy}, M \rangle = 3$
%   and $\langle \check v^{\xy},L \rangle = 0$. For the remaining dimensions of
%   $\mbbj$ a similar argument can be made to show that $\hext$ is indeed novel.
%  \end{example}
% \end{comment}
 \end{step}
\setcounter{step}{3}

\begin{step}[for $\countof \mbbt < \infty$, the concluding arguments in the
  proof of
  \cref{thm-mainQ}]\label{step-conc-mainQ} In \cref{step-condtwodiv} we showed
  that \ref{KQ}\ref{c2dQ} hold if, and only if, $\preceq_{\mbbj}$ has a
  conditionally $2$-diverse pairwise representation.  In \cref{step-stability},
  we showed that, when $\preceq_{\mbbj}$ satisfies \ref{TQ}\ref{c2dQ}, a
  necessary and sufficient condition for \fourjac\ is \ref{PQ}.  In
  \cref{step-induction}, via (mathematical) induction, we showed that
  conditionally $2$-diverse Jacobi representations of $Y$-{generalization}s of
  $\preceq_{\mbbj}$ such that $\countof Y = 4$, can be patched together to
  obtain a conditionally $2$-diverse Jacobi representation of $\preceq_{\mbbj}$
  (on all of $X$, regardless of cardinality). The fact that \ref{TQ} is
  necessary for a Jacobi representation follows from \gsii.  As a consequence,
  \ref{TQ}\ref{c2dQ} and \ref{PQ} are necessary and sufficient for a
  conditionally $2$-diverse Jacobi representation of $\preceq_{\mbbj}$. The
  following argument then completes the proof of \cref{thm-mainQ}.

  Let $v^{\dd}$ be a (conditionally $2$-diverse) Jacobi representation of
  $\preceq_{\mbbj}$ and define $\mathbf{v}: X\times \mbbt \rightarrow \R$ as
  follows. Fix arbitrary $w\in X$, and let $\mathbf{v}(w,\cdot) = 0$. Then, for
  every $x\in X$, let $\mathbf{v}(x,\cdot) = v^{\wx}$. Recalling that $v^{\wx}=
  -v^{\xw}$, note that, since the rows of $v^{\dd}$ satisfy the Jacobi
  identity, for every $x,y\in X$, we have
\begin{linenomath*}
    \begin{equation*}v^{\xy}= v^{\xw}+v^{\wy} = -\mathbf{v}(x,\cdot) +
    \mathbf{v}(y,\cdot).\end{equation*}
\end{linenomath*}
To see that \ref{rep-mainQ} holds note that, for every $J\in \mbbj$, we have $x
  \preceq_{J} y$, if, and only if, $0 \leq \langle v^{\xy}, J \rangle$, if, and
  only if, $\langle v(x,\cdot), J \rangle \leq \langle \mathbf{v}(y,\cdot), J
  \rangle$.  For \ref{rows-mainQ}, note that, since $v^{\dd}$ is a
  conditionally $2$-diverse pairwise representation, for every $x,y\in X$,
\begin{linenomath*}
  \begin{equation*}0 \not \leq v^{\xy} = - \mathbf{v}(x,\cdot) +
  \mathbf{v}(y,\cdot).\end{equation*}
\end{linenomath*} 
Finally, for every $z\in X$, we have, for every $\lambda \in \R$, $v^{\zx} \neq
  \lambda v^{\zy}$. Equivalently,
\begin{linenomath*}
  \begin{equation*} v(x,\cdot) \neq (1-\lambda) \mathbf{v}(z,\cdot ) + \lambda
    \mathbf{v}(y,\cdot) . \end{equation*}
\end{linenomath*}
\Cref{thm-main} part II, on uniqueness, follows from \cref{lem-induction} and,
  without modification, part 3 of the proof of theorem 2 of \gsii\ (see page
  23).
\end{step}
 %for lemmata/steps in the proof of thm-foureq
\begin{step}[for arbitrary $X$ and $\mbbt < \infty$, the induction
  argument]\label{step-induction} The present step is closely related to lemma
  3 and claim 9 of \gsii. There the authors establish that, when \fourdiv\
  holds, $3$-Jac is a necessary and sufficient condition for the (global)
  Jacobi identity to hold on $X$.  \gsii\ relies on the fact that \fourdiv\
  implies linear independence of $\{v^{\xy},v^{\yz}, v^{\zw}\}$ for every four
  distinct elements $x,y,z,w \in X$. In the present setting, where \condtwodiv\
  only implies linear independence of pairs $\{v^{\xy},v^{\yz}\}$, the Jacobi
  identity requires \fourjac.
 % induction argument
\begin{theoremEnd}{lemma}[Jacobi representation]\label{lem-induction} Let
  $\preceq_{\mbbj}$ have a conditionally-$2$-diverse representation $u^{\dd}$.
  Then \fourjac\ holds if, and only if, $\preceq_{\mbbj}$ has a Jacobi
  representation $v^{\dd}$.  Moreover, for every Jacobi representation $\mathbf
  v ^{ \dd }$ of $\preceq_{\mbbj}$ there exists $\lambda > 0$ satisfying
  $\mathbf v ^{ (\cdot, \cdot) } = \lambda v ^{(\cdot, \cdot)}$.
\end{theoremEnd}
\begin{proofEnd}\label{proof-lem-induction}%[Proof of \cref{lem-induction}]
  Note that, when $1 \leq \lvert X \rvert \leq 2$, \fourjac\ holds vacuously
  and $\preceq_{\mbbj}$ has a Jacobi representation via \cref{lem-insep}.  For
  general $X$, the fact that \fourjac\ is necessary for $\preceq_{\mbbj}$ to
  have a Jacobi representation follows simply because if the Jacobi identity
  holds on $X$, then it holds on every $Y\subseteq X$. For the sufficiency of
  \fourjac, we proceed by induction.  As in lemma 3 and claim 9 of \gsii, we
  assume that $X$ is well-ordered.  

  % initial step (uniqueness)

  In the case that $\lvert X \rvert \leq 4$, we only need to show that
  $v ^{ \dd }$ is unique. \Wlog, we take the initial step in our induction
  argument to satisfy $\lvert X \rvert = 4$. 
  Let $\mathbf v ^{\dd }$ denote any other Jacobi representation of
  $\preceq_{\mbbj}$. By \cref{lem-insep}, for every distinct
  $x , y \in Y ^{ 2 }$, there exists $\lambda^{\{x,y\}}> 0$ such that
  $\mathbf v ^{ \xy } = \lambda ^{\{x,y\}} v ^{ \xy }$. We need to show that
  $\lambda ^{\{x,y\}} = \lambda$ for every distinct $x , y \in Y$.  Let
  $Y = \{ x , y , z , w \}$.  By \cref{lem-c2dQ}, the set
  $\{ v ^{ \xy } , v ^{ \xz } , v ^{ \xw } \}$ is pairwise noncollinear. Then,
  since the Jacobi identity holds for both $v ^{ \dd }$ and
  $\mathbf v ^{\dd }$, we derive the equation
\begin{linenomath*}
  \begin{equation}\label{eq-jac-unique}
    (1 - \lambda ^{\{x,y\} })v ^{ \xy } + (1 - \lambda ^{\{y,z\}})v ^{\yz } =
    (1 - \lambda ^{\{x,z\}}) v ^{\xz}
  \end{equation}
\end{linenomath*}
  Suppose that $1 - \lambda ^{\{y,z\} } = 0$. Then, either the other coefficients
  in \cref{eq-jac-unique} are both equal to zero (and our proof is complete), or
  we obtain a contradiction of \cref{lem-c2dQ}.  Thus, $1 - \lambda ^{\{y,z\} }$
  is nonzero and we may divide through by this term and solve for
  $v ^{ \yz }$. First note that, since $v ^{ \dd }$ is a Jacobi representation,
  $v ^{ \yx }+ v ^{ \xy } = v ^{ (y,y) } = 0$. Then, since  
  $v ^{ \yx} = - v ^{ \xy }$, 
\begin{linenomath*}
  \begin{equation*}
    v ^{ \yz } =
    \frac{ 1- \lambda ^{ x y }}{ 1 - \lambda ^{ y z }} v^{ \yx}
    +
    \frac{ 1 - \lambda ^{ x y }}{ 1 - \lambda ^{ y z }} v^{ \xz }.
  \end{equation*}
  \end{linenomath*}
  We then conclude that both of the coefficients in the latter equation are
  equal to one. (This follows from linear independence of $v ^{ \yx }$ and
  $v ^{ \xz }$ together with the Jacobi identity
  $v ^{ \yz } = v ^{ \yx} + v ^{ \xz }$.)  Thus,
  $\lambda ^{\{x,y\}} = \lambda ^{\{y,z\}} = \lambda ^{\{x,z\}}$, as
  required. Repeated application of the same argument to the remaining Jacobi
  identities yields the desired conclusion, $\mathbf v^{\dd}= \lambda v^{\dd}$.
  % \footnote{Note that, by \twodiv,
    % $v ^{ \yz } \neq 0$. Linear independence of $v ^{ \xy }$ and
    % $v ^{ \xz }$, together with the Jacobi identity on $\{ x , y , z \}$, then
    % implies implies that $v ^{ \yz }$ does not belong to the span of
    % $v ^{ \xy }$ or $v ^{ \xz }$.}

 % Jac4-inductive step

For the inductive step, take $Y$ to be an initial segment of $X$. By the
induction hypothesis, there exists a Jacobi representation
$\v ^{ \dd }: Y^{2} \times \mbbt \rightarrow \R$ of the improper
$Y$-{generalization} $\extb = \preceqb_{\mbbj} \cap Y^{2}$ that is suitably unique.
\begin{claim}\label{claim-induction-well-defined}
  For every $w \in X \bs Y$ and $W \defeq Y \cup \{w\}$, there
exists a Jacobi representation
$\hat{v}^{\dd}: W^{2} \times \mbbt \rightarrow \R$ of the improper
$W$-{generalization} $\hextb$.
\end{claim}
\begin{proof}[Proof of \cref{claim-induction-well-defined}]
  Via \cref{lem-c2dQ}, there exists a conditionally $2$-diverse pairwise
  representation $u^{\dd}$ of $\preceq_{\mbbj}$. Fix any four distinct elements
  $x, x ' , y , z$ in $Y$.  \Cref{lem-insep} implies the existence of
  $\phi, \phi' \in \posreal$ such that
  $\phi u^{\xz} = \v^{\xz}$ and $\phi' u^{\xpz} = \v^{\xpz}$.  Let
  $Z = \{ x, y , z , w\}$ and $Z ' = \{ x' , y , z , w\}$. Since \threejac\
  holds, there exist positive scalars
  $\alpha , \beta , \hat \beta, \gamma, \sigma$ and $\tau$ such that
\begin{linenomath*}
\begin{align}\label{eq-xy}
  \alpha u ^{ \xw } + \beta u ^{ \wy } &= \gamma u ^{ \xy },\\
   \label{eq-yz}
    \hat \beta u ^{ \yw } + \sigma u^{ \wz } &= \tau u ^{ \yz }, \text{  and}\\
   \label{eq-xz}
  \gamma u ^{ \xy } + \tau u ^{ \yz } &= \v ^{ \xz } .
\end{align}
\end{linenomath*}
\addtocounter{linenumber}{-1}
Moreover, \fourjac\ ensures that we may take
$\beta = \hat \beta$.  Since $u^{\dd}$ is conditionally $2$-diverse,
$\{ u ^{ \xy } , u ^{ \yz } \}$ is linearly independent, and the linear system
\cref{eq-xz} in the unknowns $\gamma$ and $\tau$ has a unique solution.
This, together with the induction hypothesis (which yields
$\v ^{ \xy } + \v ^{ \yz } = \v ^{ \xz }$) implies that
$\gamma u ^{ \xy } = \v ^{ \xy }$ and $\tau u ^{ \yz } = \v ^{ \yz }$.
Similarly, for $Z '$, \fourjac\ yields
$\alpha ' , \beta ' , \sigma ' , \gamma ' , \tau ' > 0$ such that
\begin{linenomath*}
 \begin{align}\label{eq-xpy}
   \alpha ' u ^{ \xpw } + \beta ' u ^{ \wy } &= \gamma ' u ^{ \xpy },\\
\label{eq-yz-xp}
   \beta '  u ^{ \yw } + \sigma '  u ^{ \wz }& = \tau '  u ^{ \yz }, \text{ and }\\
\label{eq-xpz-y}
    \gamma '  u ^{ \xpy } + \tau ' u ^{ \yz } &= \v ^{ \xpz } .
 \end{align}
\end{linenomath*}
\addtocounter{linenumber}{-1} As in the arguments involving $\gamma$ and
$\tau$, the induction hypothesis yields
$\gamma ' u ^{ \xpy } = \v ^{ \xpy }$ and $\tau ' u ^{ \yz } = \v ^{ \yz }$.
We conclude that $\tau = \tau'$.  Substituting for $\tau'$ in \cref{eq-yz-xp}
and appealing to linear independence of $\{u^{\yw}, u^{\wz}\}$ then yields the
desired equalities $\beta = \beta'$ and $\sigma = \sigma '$.
  
  As a consequence of the above argument, for every $y , z \in Y$, take
  $\hat{v}^{\yw}$ and $\hat{v}^{\wz}$ to be the unique vectors in $\R^{\mbbt}$
  that solve the equation $\hat{v} ^{ \yw } + \hat{v} ^{ \wz } = \v ^{ \yz }$.
  For every $y , z \in Y$, let $\hat{v}^{\yz} = \v^{\yz}$ and $\hat{v}^{(w,w)}
  = 0$.  Then the matrix $\hat{v}^{\dd}$ with row vectors $\left\{
    \hat{v}^{\xy}: x , y \in W \right\}$ is a Jacobi representation of $\hext$.
\end{proof}
Our proof of \cref{claim-induction-well-defined} shows that the
{generalization} to $W$ holds for any initial subsegment of $Y$ consisting of
four elements. Our proof thereby accounts for the case where $X$ is infinite
and $w$ is a limit ordinal.
\end{proofEnd}
\end{step}

\begin{step}[the case of arbitrary $X$ and $\mbbt$]\label{step-infinite}
  Note that the inner product we have been working with throughout is
  well-defined on the infinite-dimensional, linear subspace $\R^{\oplus
  \mbbtpp} \subset \R^{\mbbtpp}$ of vectors with finite support. Indeed, for
  every $v \in \R^{\mbbtpp}$ and $J \in \R^{\oplus\mbbtpp}_{\mplus}$, the inner
  product is a finite sum
\begin{linenomath*}
  \begin{equation*}\langle v, J\rangle =
  \sum_{\{t:J(t)>0\}}v(t)J(t).\end{equation*}
  \end{linenomath*}
  Next note that $\mbbjpp$ is just the set of rational-valued vectors in
  $\R^{\oplus \mbbtpp}$. As such, the notions of orthogonality and hyperplanes
  carry over to the present, infinite-dimensional, setting.  \Wlog, let $\aext$
  be an improper $Y$-{generalization} of $\preceq_{\mbbj}$ that satisfies
  \ref{KQ}-\ref{p3dQ}.  When $\countof \mbbt = \infty$, \cref{lem-insep}, does
  not hold (since the interior of $\R^{\oplus\mbbtpp}$ is empty). Nevertheless,
  the preceding results extend to the present setting via the notion of an
  \emph{essentialization}.  First note that, via theorem 2 of \gsii, there
  exists a representation $\acute{u}^{\dd}$ of $\aext$. The construction is the
  same as the one we present in proof of
  \cref{lem-insep}.
  For every $Y\subseteq X$ of
  cardinality $2,3$ or $4$, the representation $\acute{u}^{\dd}$ of $\aext$ has
  finite rank $\acute{\mathbf r}$. The \emph{essentialization} $\ess(\acute{\mc
  H})$ of the associated arrangement $\acute{\mc H}$ in $\R^{\oplus \mbbt}$ is
  the arrangement we obtain via the by orthogonally projecting $\acute{\mc H}$
  onto the ($\acute{\mathbf r}$-dimensional) span of $\acute{u}^{\dd}$. Let
  $\textup{p}: \R^{\oplus \mbbt}\rightarrow S = \spann\{\acute{u}^{\dd}\}$
  denote this projection.  Then, for every $x,y \in Y$, any $J\in \mbbjpp$, $J$
  is the sum of $\textup{p}(J)$ and a term that lies in the null space of
  $\langle \acute{u}^{\xy}, \cdot \rangle$. Thus, for every $J \in
  \R^{\oplus\mbbt}$, $\langle \acute{u}^{\xy}, J \rangle = \langle
  \acute{u}^{\xy}, \textup{p}(J) \rangle$. As such, all the structure of
  $\acute{\mc H}$ in $\R^{\oplus\mbbt}$ is preserved by $\ess(\acute{\mc H})$
  in the finite-dimensional subspace $S$.  On this space, define an inner
  product $\langle \cdot , \cdot \rangle_{S}$.

  For $n = \acute{\mathbf{r}}$, take $\mc T= \{T_{i}: i = 1, \dots , n\}$ to be
  an orthonormal basis for $S$. Then let $T: \R^{n} \rightarrow S$ to be the
  matrix with columns $\mc T$. Then, for every $J \in \R^{n}$, $T(J) = J_{1}
  T_{1} + \cdots + J_{n}T_{n}$, where $J_{i} \in \R$ for each $i$. Via
  orthonormality of $\mc T$,
%
%\begin{linenomath*}
%\begin{equation*}
%\langle T_{i} , T_{j} \rangle_{S} = \left\{ 
%\begin{array}{ll}
%1 & \text{if $i = j$, and}\\
%0 & \text{otherwise.}
%\end{array}
%\right.
%\end{equation*}
%\end{linenomath*} 
it then follows that, for every pair of vectors $v,J \in \R^{n}$, we have
\begin{linenomath*}
\begin{equation*} \langle T(v) , T(J) \rangle_{S} = \sum_{i,j = 1}^{n}
  v_{i}J_{j} \,\langle T_{i},
   T_{j} \rangle_{S} = \langle v, J \rangle_{\,\R^{n}}.\end{equation*}
\end{linenomath*}

Let $\mathbf{\acute{u}}^{\dd}$ denote the matrix with rows
$\left\{\mathbf{\acute{u}}^{\xy} \defeq (T^{-1}\ccirc P)
\left(\acute{u}^{\xy}\right): x, y \in Y\right\}$. Then let $\mathbf{\aext}=
\langle \mathbf{\aext}_{J}: J \in \mbb Q_{\mplus}^{n} \rangle$ denote the
Y-{generalization} generated by $\mathbf{\acute{u}}^{\dd}$ in $\mbb
Q_{\mplus}^{n}$, for every $x,y\in Y$ and every $J \in \mbb Q_{\mplus}^{n}$,
satisfy $x \mathbin{\mathbf{\aext}}_{J} y$ if, and only if, $\langle
\mathbf{\acute{u}}^{\xy},J \rangle_{\,\R^{n}} \geq 0$. Then, by construction,
$\langle \mathbf{\acute{u}}^{\xy},J \rangle_{\,\R^{n}} \geq 0$ if, and only if
$\langle \acute{u}^{\xy}, T(J) \rangle_{S} \geq 0$. Thus, $x
\mathbin{\mathbf{\aext}}_{J} y$ if, and only if, $x \aext_{T(J)} y$.

Next note that, with the exception of \cref{lem-induction} of
\cref{step-induction}, all results in the proof of \cref{thm-mainQ} involve
$Y$-{generalization}s $\ext$ of $\preceq_{\mbbj}$ for $\countof Y = 2,3$ or
$4$. Thus, via the above equivalence, all these results extend to arbitrary
$\mbbt$.  And, since \cref{step-induction} holds for arbitrary $\mbbt$
and $X$, the proof of \cref{thm-mainQ} is indeed complete. 
\end{step}


%proof-thm-foureq 
\section{Statement and proof of \cref{thm-foureq}}\label{sec-proof-foureq}
  % Our enumeration of the results reflects this.
  %outline of proof 
 \begin{theorem}\label{thm-foureq}
   For $\preceq_{\mbbj}$ satisfying \ref{TQ}--\ref{c2dQ}, \stability\ holds if,
   and only if, the Jacobi identity holds (for some pairwise representation of
   $\preceq_{\mbbj}$).
 \end{theorem}
 Via \cref{lem-induction}, it suffices to show that \stability\ is equivalent
 to \fourjac. Then by construction, \stability\ and \fourjac\ are conditions
 that apply independently to each subset $Y$ of $X$ that has cardinality $3$ or
 $4$.  Throughout the present section, $Y\subseteq X$ has cardinality $3$ or
 $4$ and $\ext$ denotes the improper $Y$-{generalization} of $\preceq_{\mbbj}$.
 This will allow us to apply the translation of \cref{step-infinite} and work
 in a finite dimensional space. But first we need to rule out the following
 case.  \paragraph{Suppose there is no testworthy $Y$-{generalization} that is
 novel.}\hskip-7pt In this case, \stability\ holds vacuously on $Y$. The
 following lemma then confirms that \fourdiv\ holds and theorem 2 of
 \gsii\ applies, so that \fourjac\ holds on $Y$.
% Since we need to study what happens when $\mbbc_{/\sim^{\star}}$ is infinite,
 % lem test empty implies diversity
% This lemma does not rely on finiteness of $\mbbt$.
%(in this section) we work with
% $\mbbc_{/\sim^{\star}}$ instead of $\mbbt = \mbbt_{Y}$. We also work with
 % $\mbb
% Q^{\oplus \,\mbbc_{/\sim^{\star}}}_{\mplus}$ instead of $\mbbj = \mbb
% Q^\mbbt_{\mplus}$.
\begin{lemma}\label{lem-test-empty-fourdiv}
  If every testworthy $Y$-{generalization} of $\preceq_{\mbbj}$ is regular,
  then $\lvert \mbbt \rvert = \infty$ and \fourdiv\ holds on $Y$.
\end{lemma}

\begin{proof}[Proof of \cref{lem-test-empty-fourdiv}] \label{proof-test-empty-fourdiv}

  Let $\lvert Y \rvert = 4$. (The proof for case where $\lvert Y \rvert = 3$
  is similar and omitted.)  Via \cref{prop-central-testworthy}, the set of
  testworthy $Y$-{generalization}s is nonempty. Let $\hext$ be a testworthy
  $Y$-{generalization}, so that there exists $J$ in $\mbbj$ such that
  $\hext_{J\times 0 }$ is total and equal to the inverse of
  $\hextb_{\novel}$. Let $P \defeq \hsextb_{\novel}$. Thus $P$ is the
  asymmetric part of a total ordering of $Y$.  Via \cref{lem-insep}, let
  $\hat{v}^{\dd}$ be the matrix representation of $\hext$ and let $\hat{v}^{P}$
  denote the restriction of $\hat{v}^{\dd}$ to
  $P \times \mbbtp$.
   % and
  % ${v}^{P^{-1}}(t) \defeq \langle {v}^{p}(t): p$ in $P^{-1}
  % \rangle$.
  % Since $I$ in $\mbbj$, linearity
  % of $\langle {v}^{p},\cdot \rangle$ implies that, for every
  % $p$ in $P$, there exists $s ^{ p }$ in $\mbbt$ such that
  % ${v} ^{ p } ( s ^{ p }) > 0$. Let $S ^{ P }$ denote the set
  % of such $s ^{ p }$.
  % We will show that $\ext$ satisfies \fourdiv. Let ${v}^{\dd}$ be the
  % matrix representation of $\ext$. Suppose that, for some
  % $M$ in $\R^{Y\times Y}$ with $M<0$, we have ${v}^{\dd}(t) \neq M$ for
  % every $t$ in $\mbbt$.
  \begin{claim}
    \label{claim-test-empty}
    For every vector
    $\eta^{P} = \langle \eta^{\xy} \in \R_{\mdoubleplus }: \xy \in P \rangle$,
    there exist $s, t \in \mbbt$ such that
    $\hat{v}^{P}(s)= \eta^{P}$ and $\hat{v}^{P}(t)= - \eta ^{P}$.
    % , for every $p \in P$, ${v}^{p}(t) = \eta^{p}$.
  \end{claim}
  
  \begin{proof}[Proof of \cref{claim-test-empty}]
    \label{proof-test-empty}

    By way of contradiction, suppose there exists
    $\acute{\eta}^{P} \in \R^{P}_{\mdoubleplus}$ such that, for every $s$ in
    $\mbbt$, $\hat{v}^{P}(s) \neq \acute{\eta}^{P}$. That is $\eta^{P}$ such
    that, for every $s$ in $\mbbt$, there exists $\xy$ in $P$ such that
    $\hat{v} ^{ \xy} (s) \neq \acute{\eta}^{\xy }$. This property will suffice
    for the existence of a testworthy $Y$-{generalization} $\aext$ that is novel.
    Define $\acute{v}^{\dd}: X^{2} \times \mbbtp \rightarrow \R$ as
    follows. For each $\xy$ in $P$, let
\begin{linenomath*}
  \begin{equation*}
  \acute{v} ^{ \xy } (s)\defeq
  \left\{
    \begin{array}{ll}
      \acute{\eta}^{\xy} & \text{if $s  = \novel$,}\\
     \hat {v}^{\xy}(s) & \text{otherwise}.
      \end{array}
    \right.
  \end{equation*}
\end{linenomath*}
  For every $\xy$ in $P^{-1}$, since $\yx$ in $P$, take
  $\acute{v}^{\xy} = - \acute{v} ^{ \yx }$. Finally, since $P$ is the
  asymmetric part of a total ordering, for every remaining $\xy$ in
  $Y ^{ 2 }$, $x = y$, so let $\acute{v} ^{ \xy } = 0$.  Observe that, by
  construction, for every $s$ in $\mbbt$,
  $\acute{v}^{\dd}(s) \neq \acute{v}^{\dd}(\novel)$. This allows us to appeal to
  \cref{lem-insep} and take $\aextb$ to be the novel {generalization} that
  $\acute{v}^{\dd}$ generates.
  % and the definition of $M$, In particular, note that the product of
  % $\acute{v} ^{ p } ( \no\acute{v}el )$ and $\acute{v} ^ p ( s ^ p )$ is
  % negative for every $s ^{ p }$ in $S ^{ P }$. This ensures that, for each
  % $t$ in $\mbbc_{/\sim^{\star}}$, we have a function
  %  \begin{equation*} \rho ( \cdot ) = \acute{v} ^{ p } ( \cdot ) \acute{v} ^{ p } ( s ^{ p
  %  })\end{equation*}
% on  $\{ t , \novel \}$ that is neither positive nor constant.
  Moreover, $\aext$ is also testworthy. For together $P = \hsextb_{\novel}$ and
  $\hsextb_{\novel} = \hsextb_{J\times 0 }^{-1}$ imply that
  $\aextb _{ \novel } = \aextb _{ J \times 0 } ^{ - 1 }$ since, for every
  $\xy$ in $P$, we have
\begin{linenomath*}
  \begin{equation*} \langle \acute{v} ^{ \xy } , J \times 0 \rangle = \langle \hat{v} ^{ \xy } , J
    \times 0 \rangle< 0 < \acute{\eta}^{\xy} = \acute{v} ^{ \xy } (\novel).\end{equation*}
\end{linenomath*}

  % For every $M>0$, note that $-M$ is negative, so that the claim holds for
  % $-M$. That is, for some $t$ in $\mbbc_{/\sim^{\star}}$ we have
  % ${v}^{P}(t)=-M$. Then $-{v}^{P}= M$.

This contradiction implies that, for every $\eta ^{ P } \in \posreal^{P}$,
there exists $s$ in $\mbbt$ such that $\hat{v}^{P}(s) = \eta^{P}$.
\emph{Mutatis mutandis}, a repetition of the preceding argument by contradiction
confirms that there exists $t$ in $\mbbt$ such that
$\hat{v}^{P}(t) = - \eta^{P}$.% \footnote{In summary,
%     as before, by setting $\acute{v}^{P} (\novel) = \eta ^{P}$ and, for every
%     $t$ in $\mbbc_{/\sim^{\star}}$, $\acute{v}^{P}(t) = v^{P}(t)$, we generate a novel {generalization}.
%     The associated {generalization} $\aext$ is testworthy since, for each
%     $\xy$ in $P$,
%     \begin{equation*}\langle  \acute{v}^{\xy} , J\times 0 \rangle = \langle  {v}^{\xy}, J
%       \times 0 \rangle < 0 < \eta^{\xy} = \acute{v}^{\xy}(\novel).\end{equation*}
% }   
  % $\R^{P^{-1}}$. Then, via the preceding argument, there exists
  % $s$ in $\mbbt$ such that ${v}^{P^{-1}}(s) = \eta^{P}$.  Now observe that
  % $(x,y)$ in $P^{-1}$ if, and only if, $(y,x)$ in $P$. \Cref{lem-insep}
  % then yields ${v}^{(x,y)}(s)= \eta^{(y,x)}>0$.  
  % We therefore conclude that, for every $\eta^{P} > 0$, there exist
  % $s, t$ in $\mbbt$ such that ${v}^{P}(s)= \eta^{P}$ and
\end{proof}
\Cref{claim-test-empty} implies that, when every testworthy $Y$-{generalization} is
regular, the cardinality of $\mbbt$ is equal to the cardinality of $\R^{P}$.
We now show that \fourdiv\ holds on $Y$. Let ${R}$ denote an arbitrary
total ordering of $Y$.  We show that, for some $K$ in $\mbbj$,
$\langle \hat{v} ^{ \xy }, K \rangle \geq 0$ if, and only if, $\xy$ in
${R}$. \Cref{claim-test-empty} ensures that we can choose
$s$ in $\mbbt$ such that, for some $0 < \epsilon <1$
\begin{linenomath*}
  \begin{equation*}
    \hat{v} ^{ \xy } ( s ) =
    \left \{
      \begin{array}{l l}
        1+ \epsilon & \text{ if $\xy$ in ${R} \cap P $},\\
        1-\epsilon & \text{if $\xy$ in ${R} ^{ - 1 } \cap P  $}.
      \end{array}
    \right.
  \end{equation*}
\end{linenomath*}
   % Note that, since ${R}$ is a total order and $P$ is the asymmetric part of
  % a
  % total order, for every $p$ that does not belong to
  % ${R} \cup{R} ^{ - 1 }) \cap P$, either $p$ in $P ^{ - 1 }$ or
  % $p = x \times x$.
  % By \cref{lem-insep}, for every $p$ in $P ^{ - 1}\cap{R} \cup{R}^{-1})$,
  % $v ^{ p } ( s ) = - v ^{ p ^{ - 1 } } ( s )$ (and, for every
  % $p$ such that $p = x \times x$, $v ^{ p } ( s ) = 0$).
  Via \cref{claim-test-empty}, take $t$ in $\mbbt$ such that, for every
  $\xy$ in $P$, $\hat{v}^{ \xy } ( t ) = -1$.  Let
  $K : = \delta _{ s } + \delta _{ t }$ in $\mbbjp$, so that
  $\langle \hat{v}^{\xy}, K \rangle = \hat{v}^{\xy}(s) + \hat{v}^{\xy}(t)$. By evaluating
  terms and observing that $\epsilon > 0$ we obtain
\begin{linenomath*}  
 \begin{equation*}
   \langle  \hat{v}^{\xy}, K \rangle =
   \left \{
      \begin{array}{l l}
        (1+ \epsilon) - 1 > 0  & \text{if $\xy$ in ${R} \cap P  $},\\
                 (1- \epsilon) - 1 < 0 & \text{if $\xy$ in ${R}^{-1}\cap P$}.
      \end{array}
    \right.
  \end{equation*}
\end{linenomath*}  
  Since $\xy$ in ${R}^{-1} \cap P^{-1}$ if, and only if,
  $\yx$ in ${R}\cap P$ (and, similarly,
  $\xy$ in ${R} \cap P^{-1}$ if, and only if $\yx$ in
  ${R}^{-1}\cap P$), we appeal to $\hat{v}^{\xy} = - \hat{v}^{\yx}$ and obtain
\begin{linenomath*}
 \begin{equation*}
   \langle  \hat{v}^{\xy}, K \rangle =
   \left \{
      \begin{array}{l l}
        -( 1 + \epsilon) + 1 < 0 & \text{if $\xy$ in ${R}^{-1} \cap P^{-1}$,}\\
        -(1- \epsilon) + 1 > 0  & \text{if $\xy$ in ${R} \cap P^{-1} $.}
      \end{array}
    \right.
  \end{equation*}
\end{linenomath*}
Since
$P$ is the asymmetric part of a total ordering we conclude that, for every
$x\neq y$, $\langle \hat{v}^{\xy}, K \rangle$ has the right sign. Finally, for $x = y$, $\langle \hat{v}^{\xy}, K \rangle =
0$.
\end{proof}

As a consequence of \cref{lem-test-empty-fourdiv}, for the remainder of the proof of
\cref{thm-foureq} we work under the assumption that the set of testworthy
$Y$-{generalization}s that are novel is nonempty. 

\paragraph{For the remainder of this section, redefine
  $\mbbt : = \mbbt_{Y}$,}\hskip-7pt where recall  $\mbbt_{Y}$ is defined in \cref{step-infinite}.  Thus, in the present section, $\mbbt$ is the finite
set that indexes the translation to $\R^{n}$ of \cref{step-infinite}. Then
$\mbbtp = \mbbt \cup[\novel]$ as before and indeed all other notation remains
the same.  Whereas $\mbbt = \mbbt_{Y}$ is now finite, the set
$\mbbc_{/\sim^{\ext}}$ of equivalence classes of $\sim^{\ext}$ in $\mbbc$ may
be infinite as we have seen in \cref{lem-test-empty-fourdiv}.

 Since \ref{KQ}--\ref{c2dQ} hold, the pairwise representation
 $u^{\dd}: Y^{2}\times \mbbt \rightarrow \R$ of $\ext$ is conditionally
 $2$-diverse.  \Cref{lem-c2dQ} implies that $u^{\dd}$ has row rank
 $\mathbf r \geq 2$.

%  For any given $Y$-{generalization} $\aext$, $\acute{\mathbf r}$ will
 % denote the row rank of a representation $\acute{u}^{\dd}$.
% the For the case
% $\lvert Y \rvert = 2$, there is nothing to prove: \ref{PQ} holds vacuously and
% \fourjac\ holds as a direct consequence of \cref{lem-insep}. (In particular,
% since $u^{\yx} = - u^{\xy}$, we have $u^{(x,x)} = u ^{\xy}+ u^{\yx}=0$.)
% Henceforth, take $Y$ to satisfy $3 \leq \lvert Y \rvert \leq 4$.

Recall the definition of central arrangements. We now show that the set of
testworthy {generalization}s with a central arrangement is always nonempty.%
(The
% trouble is that such arrangements may also be dogmatic.)
\begin{lemma}\label{prop-central-testworthy}
  For every $J \in \mbbj$ such that $\ext_{J}$ is total, there exists a
  $Y$-{generalization} $\aext$ with $\aextb_{\novel } = \extb_{J } ^{-1}$.
  Moreover, its arrangement $\acute{\mc H}_{\mdoubleplus}$ is central$;$ its
  representation $\acute{u}^\dd$ has rank $\acute{\mathbf r}= \mathbf r \,;$
  and \fourjac\ holds for $\aext$ if, and only if, it holds for $\ext$.
\end{lemma}
\begin{proof} Let $J \in \mbbj$ such that $\ext_{J}$ is total. If, for some
  $t \in \mbbt$, $J(t) = 0$, then, via $\binom{\countof Y}{2}$ applications of
  \ref{AQ} (or the continuity properties of the inner product), there exists
  there $L \in \posreal^{\mbbt}$ such that $\extb_{L} = \extb_{J}$. Thus,
  \withoutlog, take $J(t)>0$ for every $t\in \mbbt$. For some rational
  $0<\acute{\iota}<1$, let $\acute{J} = (1-\acute{\iota})J \times
  \acute{\iota}$, so that $\acute{J}$ lies on the (relative) interior of
  $\mbbjp$.  Now, for every $x,y$ in $Y$, let
  \begin{linenomath*}
  \begin{equation*}\acute{\eta}^{\xy} := -\frac{1-
    \acute{\iota}}{\acute{\iota}}\langle u^{\xy}, J \rangle,
  \end{equation*}
  \end{linenomath*}
  and let $\acute{u}^{\xy}\defeq u^{\xy}\times \acute \eta^{\xy}$, so that
  $\langle \acute{u}^{\xy}, \acute{J} \rangle = 0$. Let $\aext$ be the
  associated $Y$-{generalization}, so that by construction $\aextb_{\novel} =
  \aextb_{J}^{-1}$.  Since $\acute{J} \in \acute{H}^{\{x,y\}}$ for every
  distinct $x,y\in Y$, $\acute{\mc H}$ is central. By construction, note that
  $\mathbf r \leq \acute{\mathbf r} \leq \mathbf r + 1$. Moreover, recalling
  the rank--nullity theorem, we have $\acute{\mathbf r} = \countof \mbbtp -
  \textup {dim}\left(\textup{ker}\left(\acute{u}^{\dd}\right)\right)$. Observe
  that $\acute{J}$ belongs to $\textup{ker}\left(\acute{u}^{\dd}\right) \bs
  \textup{ker}\left(u^{\dd}\right)\times 0$.  Thus,
  $\textup{ker}\left(\acute{u}^{\dd}\right)$ is of dimension one more than
  $\textup{ker}\left(u^{\dd}\right)\subset \R^{\mbbt}$. Then since $\countof
  \mbbtp = \mbbt +1$, the rank--nullity theorem yields $\acute{\mathbf r} =
  \mathbf r$.  Finally, via linearity of the inner product, the Jacobi
  equations \eqref{eq-xy}--\eqref{eq-xz} hold for $u^{\dd}$ if, and only if,
  they hold for $\acute{\eta}^{\dd}$ and hence $\acute{u}^{\dd}$.
\end{proof}

\paragraph{The case where $Y$ has cardinality $3$,\hskip-10pt} follows from the
next two lemmas.
  \begin{lemma}\label{lem-Y3-r2} 
    If $Y = \{x,y,z\}$ and $\mathbf r = 2$, then \fourjac\ and \stability\ hold
    on $Y$.
  \end{lemma}
  \begin{proof}[Proof of \cref{lem-Y3-r2}]
    Fix arbitrary $J \in \mbbj$ such that $\ext_{J}$ is total.  We first apply
    Zaslavski's theorem to prove that the $Y$-{generalization} $\aext$ of
    \cref{prop-central-testworthy} (which is testworthy relative to $J$)
    satisfies $\lvert \acute{\mc G}_{\mdoubleplus}\rvert = 6$.  Via
    \cref{lem-c2dQ}, $\lvert\acute{\mc H}_{\mdoubleplus}\rvert= 3$. And since
    every subarrangement of $\acute{\mc H}_{\mdoubleplus}$ is central, for
    every $k = 0, 1, 2, 3$ there are $\binom{ 3 }{k }$ ways to choose $\lvert
    \mc A \rvert = k$ hyperplanes from $\acute{\mc H}_{\mdoubleplus}$. For $k <
    3$, the rank of every subarrangement is $k$. For $k = 3$, the rank of the
    arrangement is $\acute{\mathbf r}$. So that, via
    \cref{prop-central-testworthy}, since $\acute{\mathbf r} = \mathbf r = 2$, 
\begin{linenomath*}
        \begin{equation}\label{eq-zaslavski-3}
                \begin{aligned}
                  \lvert \acute{\mc G}_{\mdoubleplus}\rvert =& \binom{3}{3}
                  (-1)^{3 -\acute{\mathbf{r}}}+\binom{3}{2}(-1)^{2-2}
                  +\binom{3}{1}(-1)^{1-1} + \binom{3}{0}(-1)^{0-0} = 6. %\\
% =&(-1)^{1}+3(-1)^{0}+ 3(-1)^{0}+(-1)^{0} = 6.                        %
                \end{aligned}
        \end{equation}
\end{linenomath*}
For both \fourjac\ and \stability, we require that every member of $\acute{\mc
    G}_{\mdoubleplus}$ is associated with a total ordering. This ensures that
    \threediv\ holds for $\aext$, so that, via lemma 2 of \gsii, \threejac\
    holds for $\aext$ and \cref{prop-central-testworthy} yields \threejac\ for
    $\ext$. (When $\lvert Y \rvert = 3$, \threejac\ and \fourjac\ coincide.)
    Also, for \stability, if $\hext$ is any novel, testable {generalization}
    that satisfies $\hextb_{\novel}=\extb_{J}^{-1}$, then $\aext$ is a
    {{diverse}} perturbation of $\hext$ that satisfies \ref{TQ}-\ref{AQ}.
   
   % We now prove that \fourjac\ holds on $Y$. First recall that \fourjac\ is equivalent
   % to \threejac\ when $\lvert Y \rvert = 3$. Above, in the proof that \stability\
   % holds, we saw that $\lvert \acute{\mc G}_{\mdoubleplus}\rvert = 6$, 
It therefore remains for us to show that every member of
$\acute{\mc G}_{\mdoubleplus}$ is associated with a total ordering of $Y$.
Since $\acute{\mc H}_{\mdoubleplus}$ is central,
$\acute{\mc G}_{\mdoubleplus} \equiv \acute{\mc G}$, we
suppress reference to $\mdoubleplus$. Let $\{\acute{G}_{r}: r = 1, \dots 6\}$ be
an enumeration of $\acute{\mc G}$ such that $\acute{G}_{r}$ and
$\acute{G}_{r+1}$ (\textup{mod} 6) are separated by a single hyperplane in
$\acute{\mc H}$. Moreover, let the first four members of this enumeration
intersect $\mbbj \times \{0\}$. Then, since
$\extb = \langle \extb_{I}: I \in \mbbj \rangle$ satisfies \ref{TQ},
$\acute{G}_{1}, \dots, \acute{G}_{4}$ are associated with total orders. Take
$L \in \acute{G}_{5} \cap \mbbjp$ and consider the affine path
$\lambda \mapsto \phi(\lambda)= (1-\lambda)\acute{L} + \lambda \acute{J}$, where
$\acute{J}$ is defined in \cref{prop-central-testworthy}. For some rational
$\lambda^{*}$ sufficiently close to, but greater than, $1$,
$L^{*} = \phi(\lambda^{*})$ belongs to $\acute{G}_{*} \cap \mbbjp$ for some
$\acute{G}_{*} \in \acute{\mc G}$. Since $\phi(1) = \acute{J}$ belongs to the
center of $\mc H$, $L$ and $L^{*}$ are separated by all three members of
$\mc H$, so that $\aextb_{L^{*}} = \aextb_{L}^{-1}$. Thus, $\aextb_{L}$ is
transitive if, and only if, $\aextb_{L^{*}}$ is. Since $\acute{G}_{5}$ is
separated from $\acute{G}_{*}$ by $3$ hyperplanes,
$\acute{G}_{*} = \acute{G}_{2}$. Thus $\aextb_{L^{*}}$ is transitive.
\emph{Mutatis mutandis}, the same argument shows that $\acute{G}_{6}$ is also
associated with a total ordering of $Y$.
  %  An alternative proof that \fourjac\ holds on $Y$ is as follows. Since
  %  $\mathbf r =2$, there exist unique $\alpha , \beta$ in $\R \bs \{ 0\}$
  %  such that $\alpha u^{(x,y)}+ \beta u^{(y,z)}= u^{(x,z)}$. Let $\Gamma$ be a
  %  standard (linear) rotation matrix that, for some $L^{xz}$ in
  %  $H^{\{x,z\}}_{\mdoubleplus}$, satisfies $u^{\xz} = \Gamma(L^{xz})$. Then,
  %  since $u^{\xz} = \alpha u^{(x,y)}+ \beta u^{(y,z)}$, linearity of $\Gamma$
  %  implies that
  %  $\Gamma (L^{xz}) = \alpha \Gamma (L^{xy}) + \beta \Gamma(L^{yz})$ for some
  %  $L^{xy}$ in $H^{\{x,y\}}$ and $L^{yz}$ in $H^{\{y,z\}}$.  We need to show
  %  that $L^{xy}$ in $H^{\{x,y\}}_{\mdoubleplus}$ and $L^{yz}$ in
  %  $H^{\{x,y\}}_{\mdoubleplus}$ and that both $\alpha$ and $\beta$ are
  %  positive.  Since \ref{c2dQ} holds, \cref{prop-c2dQ} implies
  %  $\lvert \mc G \rvert \geq 4$. \Wlog, both $G^{(x,y,z)}$ and $G^{(z,y,x)}$
  %  belong to $\mc G$, so that any maximal convex path from (a point in)
  %  $G^{(x,y,z)}$ to $G^{(z,y,x)}$ passes through all three hyperplanes. By
  %  \ref{TQ}, \withoutlog\ assume that $H^{\{x,y\}}_{\mdoubleplus}$ supports
  %  $G^{(x,y,z)}$. Then $G^{(y,x,z)}$ is adjacent to $G^{(x,y,z)}$ and, via
  %  \ref{TQ}, the other hyperplane that supports $G^{(y,x,z)}$ is
  %  $H^{\{x,z\}}_{\mdoubleplus}$. Thus $H^{\{x,z\}}_{\mdoubleplus}$ belongs to
  %  the open and positive cone
  %  \begin{equation*} \left\{ \lambda I + \mu J: \text{$I \in H^{\{x,y\}}_{\mdoubleplus}$,
  %      $J \in H^{\{y,z\}}_{\mdoubleplus}$ and $\lambda, \mu >0$} \right \} \end{equation*}
  %  generated
  %  by positive combinations of points in $H^{\{x,y\}}_{\mdoubleplus}$ and
  %  $H^{\{y,z\}}_{\mdoubleplus}$.
\end{proof}
  % \qedhere\end{step}\phantom{\qed}

\begin{lemma}\label{lem-Y3-r3}
  If $Y = \{x,y,z\}$ and  $\mathbf{r} = 3$, then neither \stability\ nor
  \fourjac\ hold.
\end{lemma}
\begin{proof}[Proof of \cref{lem-Y3-r3}]
  When $\mathbf r = 3$, $u^{\xy}, u^{\yz}$ and $u^{\xz}$ are linearly
  independent, so that \threejac\ fails to hold.

  We now confirm that \threepru\ also fails to hold.  Via
  \cref{prop-central-testworthy}, $\acute{\mathbf r} = \mathbf r$.  The only
  term in \cref{eq-zaslavski-3} that we adjust for the present lemma is
  $\mathbf{r}=3$. Thus,
\begin{linenomath*}
  \begin{align*}
    \lvert \acute{\mc G}\rvert
    = (-1)^{0}+3(-1)^{0}+ 3(-1)^{0}+(-1)^{0} = 8.
  \end{align*}
\end{linenomath*}
\addtocounter{linenumber}{-1}
  Then there are $3!=6$ members of $\total (\aext)$, and the two additional
  regions of $\acute {\mc G}$ are associated with intransitive CAR rankings. It
  remains for us to show that every $Y$-{generalization} $\hext$ with
  $\countof \total (\hext) = 6$ is of this form. % This follows from the other
  % version of Zaslavski's theorem in relation to which we present the relevant
  % Hasse diagram in \cref{fig-hasse-Y3-r3}.
  
  \begin{figure}
    \begin{center}
  \begin{tikzpicture}
    \node (max) at (0,3) {$\{x,y,z\}$}; \node (a) at (-3,1.75)
    {$\{x,y\},\{y,z\}$}; \node (b) at (0,1.75) {$\{x,y\},\{x,z\}$}; \node (c) at
    (3,1.75) {$\{y,z\},\{x,z\}$}; \node (d) at (-2,0.25) {$\{x,y\}$}; \node (e)
    at (0,0.25) {$\{y,z\}$}; \node (f) at (2,0.25) {$\{x,z\}$}; \node (min) at
    (0,-1) {$\emptyset$}; \draw (min) -- (d) -- (a) -- (max) -- (b) -- (f) (e)
    -- (min) -- (f) -- (c) -- (max) (d) -- (b); \draw[preaction={draw=white,
      -,line width=6pt}] (a) -- (e) -- (c);
\end{tikzpicture}
\caption{\label{fig-hasse-Y3-r3} The intersection lattice of a central
  arrangement for $\countof Y = 3$ and ${\mathbf r} =3$.}
\end{center}
\end{figure}

In the Hasse diagram of \cref{fig-hasse-Y3-r3}, an increase in level corresponds
to a decrease in dimension: since $\acute{A}^{\{x,y,z\}}$ is nonempty, it is of
dimension at least zero.  Since $\acute{J}$ belongs to the interior of
$\posreal^{\mbbtp}$ and $\acute{A}^{\{x,y\}\{y,z\}}$ is at least
one-dimensional, $\acute{A}^{\{x,y\} \{y,z\}}_{\mdoubleplus}$ is
one-dimensional.  Since
$\acute{A}^{\{x,y,z\}} \subset \acute{A}^{\{x,y\}\{y,z\}}$, the latter set is
nonempty whenever $\acute{\mc H}_{\mdoubleplus}$ is central. The same, of
course, applies to other members at the same level. Conversely, if
$\acute{A}^{\{x,y\}\{y,z\}}$ is empty, then so is $\acute{A}^{\{x,y,z\}}$. We
now use this to show that there is a unique form of $Y$-{generalization} $\hext$ such
that $\countof \hat{\mc G}_{\mdoubleplus} = 6$ and, moreover, that any such
$\hext$ fails to satisfy \ref{TQ}.

Recall from \cref{eg-zaslavski} that $\bmu(A^{\emptyset}) = 1$ and
$\bmu(A^{\{i,j\}}) = -\bmu(A^{\emptyset})$. Then, since
$\hat{A}^{\{x,y\}\{y,z\}} \subset \hat{A}^{\{x,y\}} , \hat{A}^{\{y,z\}}$, we
have $\bmu(A^{\{x,y\}\{y,z\}}) = - \bmu(A^{\emptyset})(1 - 2) = 1$. Finally, by
the same argument, since $\hat{A}^{\{x,y,z\}}$ belongs to every member of
$\hat{\mc L}$, we have
$\bmu(A^{\{x,y,z\}}) = - \bmu(A^{\emptyset})(1 - 3 + 3) = -1$.  Thus, when
$\mathbf{r} = 3$, if $\countof \hat{\mc G}_{\mdoubleplus} = 6$, then, in
addition to $\hat{A}^{\{x,y,z\}}_{\mdoubleplus}$, at most one of the
intersections $\hat{A}^{\{i,j\}\{k,l\}}_{\mdoubleplus}$ is empty. Then,
\withoutlog, suppose $\hat{A}^{\{x,y\}\{y,z\}}_{\mdoubleplus}$ is nonempty. We
will show that \ref{TQ} fails to hold. Take
$L \in \hat{A}^{\{x,y\}\{y,z\}}_{\mdoubleplus}$. If $L \in \mbbjp$, then since
$x \hnext_{L}y \hnext_{L} z$ and $\neg (x \hnext_{L} z)$, the proof is
complete. In any case, $L$ belongs to one $\hat{G}^{\xz}_{\mdoubleplus}$ or
$\hat{G}^{\zx}_{\mdoubleplus}$. \Wlog, suppose
$L \in \hat{G}^{\xz}_{\mdoubleplus}$. Then $\hat{H}^{\{x,y\}}$ and
$\hat{H}^{\{y,z\}}$ split $\hat{G}^{\xz}_{\mdoubleplus}$ into the four
(nonempty) open regions formed by expanding
\begin{linenomath*}
  \begin{equation*}\hat{G}^{\xz}_{\mdoubleplus}\bs \left(\hat{H}^{\{x,y\}} \cap
      \hat{H}^{\{y,z\}}\right) = \hat{G}^{\xz}_{\mdoubleplus}\cap
    \left(\hat{G}^{\xy}_{\mdoubleplus} \cup \hat{G} ^{\yx}_{\mdoubleplus}
    \right) \cap \left (\hat{G}^{\yz}_{\mdoubleplus} \cup
      \hat{G}^{\zy}_{\mdoubleplus} \right). \end{equation*}
\end{linenomath*}
Take any $K$ in the final member
$\hat{G}^{\xz}_{\mdoubleplus}\cap \hat{G}^{\yx}_{\mdoubleplus} \cap
\hat{G}^{\zy}_{\mdoubleplus}$ of this expansion. Then, since
$x \hsext_{K} z \hsext_{K} y \hsext_{K} x$, we observe that $\hext$ fails to
satisfy \ref{TQ}. We conclude that only dogmatic perturbations of $\aext$
satisfy \ref{TQ}.
  % In the present case, this
  % argument is especially simple, since we may rely upon the arguments of
  % \gsii. In particular, take $\hext$ maximal, so that it satisfies
  % \threediv. \emph{Per absurdum}, suppose $\hext$ also satisfies \ref{TQ}. In
  % this case, we have the conditions for theorem 2 of \gsii\ to hold for
  % $\hext$.\footnote{To be precise, for theorem 2 of \gsii\ to apply, we would
  %   need to consider the restriction $\hext'$ of $\hext$ to the set $\mbbip$ of
  %   counting counting vectors.}  Thus, there exists a Jacobi representation
  % $\v^{Y}$ of $\hext$. Since $\hext$ is a testworthy {generalization} of
  % $\preceq_{\mbbj}$, for some $u^{Y}\subset \R^{\mbbt}$ and
  % $\eta^{Y}\subset \R$, $\v^{Y}=\{u^{(i,j)}\times \eta^{(i,j)}: i<j\}$.  But, by
  % \cref{def-{generalization}Q}, $\dext$ and $\hext$ agree on $\mbbj\subset \R^{\mbbt}$, so
  % that we obtain the contradiction we sought: since $u^{Y}$ is a Jacobi
  % representation of $\dext$, the latter satisfies \threejac.
\end{proof}
\paragraph{The case where $Y$ has cardinality $4$.\hskip-4pt} Note that a
failure of \threejac\ on $Z \subset Y$ such that $\lvert Z \rvert = 3$ implies a
failure of \fourjac\ on $Y$. And since the arguments for the case where
$\lvert Y \rvert = 3$ account for the case where \threejac\ fails, we henceforth
assume that \threejac\ holds on $Y$. That is, our conditionally \emph{2}-diverse
representation $u^{\dd}$ will now satisfy equations \eqref{eq-xy}--\eqref{eq-xz}
with $\hat{\beta} = \beta$ if, and only if, \fourjac\ holds on $Y$.

First some some useful results that exploit \threejac.
\begin{proposition}\label{lem-2r3}
  If $Y = \{x, y, z, w\}$ and \threejac\ holds for $u^{\dd}$, then, for every
  $\acute{v}^{\dd} = u ^{\dd} \times \acute{\eta}^{\dd}$ with rank
  $\acute{\mathbf r}$, that satisfies \threejac,
  $2\leq \acute{\mathbf r}\leq 3$.
\end{proposition}
\begin{proof}[Proof of \cref{lem-2r3}]
  Via \cref{prop-c2dQ} and \cref{lem-Y3-r2}, $\mathbf r \geq 2$. Indeed the
  span of $\{ u^{\xw}, u^{\yw}\}$ is two. Let $S$ denote the span of
  $\{u^{\xw}, u^{\yw} ,u^{\zw}\}$.  Since $u^{\yw} = - u^{\wy}$ and $u ^{\dd}$
  satisfies \threejac, equations \eqref{eq-xy}--\eqref{eq-xz} hold for
  $u^{\dd}$.  (If \fourjac\ fails to hold, then $\beta \neq \hat \beta$, but the
  equations still hold.) Thus $u^{\xy}$, $u^{\yz}$ and $u^{\xz}$ all belong to
  $S$ and $\mathbf{r} \leq 3$. Now note that above argument does not depend on
  the cardinality of $\mbbt$, thus take $\acute{\eta}^{\dd}$ to
  satisfy \threejac: indeed with the same parameters that feature in
  equations \eqref{eq-xy}--\eqref{eq-xz} for $u^{\dd}$. The preceding
  argument then {generalizes} \emph{mutatis mutandis} to $\acute{v}^{\dd}$ and
  $2 \leq \acute{\mathbf r} \leq 3$.
\end{proof}

\begin{proposition}\label{lem-arrangement-cardinality-rank}
  If $X = \{ x, y , z , w\}$, then $4 \leq \lvert \mc H \rvert \leq 6$ and
  these bounds are tight. If, moreover, \threejac\ holds for $u^{\dd}$ and
  $\lvert \mc H \rvert < 6$, then $\mathbf{r} = 2$.
\end{proposition}
\begin{proof}[Proof of \cref{lem-arrangement-cardinality-rank}]
  % The isomorphic relationship between $\mc H$ and $\mc H_{\mdoubleplus}$ and fact that the
  % premises of present proposition only concerns the cardinality of these sets
  % ensure that we may simply work with $\mc H$.  
  The upper bound $\lvert \mc H \rvert \leq 6$ follows from the fact that there
  are $\binom{4}{2} = 6$ ways to choose distinct pairs of elements from the
  four-element set $X$.  Via \cref{lem-c2dQ} only the following equalities are
  feasible: $H^{\{x,y\}} = H^{\{z,w\}}$, $H^{\{x,z\}} = H^{\{y,w\}}$ and
  $H^{\{x,w\}} = H^{\{y,z\}}$.  Via \cref{prop-pairwise-extremal}, \withoutlog, take
  $G^{(x,y,z,w)}$ and $G^{(w,z,y,x)}$ to belong to $\mc G$. We consider a
  convex path $\phi$ from (some member of) $G^{(x,y,z,w)}$ to $G^{(w,z,y,x)}$
  such that $\countof \{G \in \mc G: (\image \phi) \cap G \neq \emptyset \}$ is
  maximal. Via \ref{TQ}, one of $H^{\{x,y\}} , H^{\{y,z\}}$ and $H^{\{z,w\}}$
  supports $G^{(x,y,z,w)}$.  If it is $H^{\{y,z\}}$, then so does $H^{\{x,w\}}$
  and, and since $x$ and $w$ lie at opposite ends of the ordering $(x,y,z,w)$,
  this would imply that $\lvert\mc H \rvert = 1$ in violation of
  \cref{lem-c2dQ}. Since we are seeking a greatest lower bound on
  $\lvert \mc H \rvert$, take $H_{1} = H^{\{x,y\}} = H^{\{z,w\}}$ to be the
  first hyperplane to intersect $\image \phi$ and the adjacent chamber is
  $G^{(y,x,w,z)}$.  Via \ref{TQ}, the second hyperplane to intersect
  $\image \phi$ is $H_{2} \defeq H^{\{x,w\}}$. If $H^{\{x,w\}} = H^{\{y,z\}}$
  then since $y$ and $z$ lie at opposite ends of $(y,x,w,z)$, this would once
  again lead to a violation of \cref{lem-c2dQ}. Then $G^{(y,w,x,z)}$ is next
  chamber to intersect $\image \phi$. Since we are seeking a greatest lower
  bound, via \ref{TQ}, take $H_{3} \defeq H^{\{y,w\}} = H^{\{x,z\}}$ to be the
  third hyperplane to intersect $\image \phi$. Then $G^{(w,y,z,x)}$ is the next
  chamber to intersect $\image \phi$. The final hyperplane to intersect
  $\image \phi$ is $H_{4} = H^{\{y,z\}}$ which brings us to the destination
  chamber $G^{(w,z,y,x)}$.  We conclude that at most two pairs of the six
  hyperplanes coincide, so that $\lvert \mc H \rvert \geq 4$.
  
  We now prove that \threejac\ and $H^{\{x,y\}} = H^{\{z,w\}}$ together imply
  $\mathbf r = 2$. Consider equations \eqref{eq-xy}--\eqref{eq-xz} (so
  \threejac\ holds, but \fourjac\ need not). Via \eqref{eq-xy},
  $S = \{ u^{\xw}, u^{\wy}, u^{\xy}\}$ is 2-dimensional. Since $u^{\xy}$ and
  $u^{\zw }$ are collinear, $u^{\zw}$ belongs to $S$. Finally, equations
  \eqref{eq-yz} and \eqref{eq-xz} yield $u^{yz} , u^{\xz} \in S$.
\end{proof}
\begin{lemma}\label{lem-r3-Y4}
  If $Y = \{x,y,z,w\}$, $\mathbf{r} = 3$ and \threejac\ holds on $Y$, then \stability\
  and \fourjac\ both hold on $Y$
\end{lemma}
\begin{proof}[Proof of \cref{lem-r3-Y4}]
  To see that \fourjac\ holds, appeal to the proof of lemma 3 of \gsii: if
  \threejac\ holds and \fourjac\ does not, then $\{u^{\xw}, u^{\yw}, u^{\zw}\}$
  is linearly dependent. This is in contradiction of $\mathbf{r} = 3$.
  
  We now verify that \stability\ also holds. We show that, for every $J\in
  \mbbj$ such that $\ext_{J}$ is total, the $Y$-{generalization} $\aext$ of
  \cref{prop-central-testworthy} is maximal in the sense that $ \total (\aext)
  = \lvert \acute{\mc G}_{\mdoubleplus} \rvert = 4!$. We then confirm that
  $\aext$ satisfies \ref{TQ}.  Then, if $\cext$ is any novel, testworthy
  {generalization} that satisfies $\cextb_{\novel }=\extb_{J }^{-1}$, then
  $\aext$ is a {{diverse}} perturbation of $\cext$ that satisfies
  \ref{TQ}-\ref{AQ}.

  Since $\mathbf{r}=3$, the contrapositive of
  \cref{lem-arrangement-cardinality-rank} implies that
  $\lvert\mc H_{\mdoubleplus}\rvert = 6$.  The proof that now follows is a
  simple {generalization} of \cref{lem-Y3-r2} to allow for the fact that
  $\lvert Y \rvert=4$.  Fix any $J \in \mbbj$ such that $\ext_{J}$ is total and
  \withoutlog\ take $J>0$.  Via \cref{prop-central-testworthy}, there exists a
  testworthy $Y$-{generalization} $\aext$ such that
  $\aextb_{\novel }= \extb_{J }^{-1}$ is total and
  $L = (1- \lambda)J \times \lambda$ belongs to both $\mbbjp$ and the center of
  $\acute{\mc H}_{\mdoubleplus}$ and $\acute{v}^{\dd}$ satisfies \fourjac. Via
  \cref{rem-lattice}, since $\acute{\mc H}$ is central,
  $\acute{\mc G}_{\mdoubleplus}$ and $\acute{\mc G}$ are isomorphic, so we work
  with $\acute{\mc G}$.  Since $\mathbf r = 3$,
  \cref{lem-arrangement-cardinality-rank} implies $\lvert \mc H \rvert = 6$, so
  that the same is true of $\lvert \acute{\mc H} \rvert$.  The rank of
  subarrangements with cardinality $4$ or more is $\acute{\mathbf r}$.  Let
  $\acute{\mathbf{\uptau}}$ denote the number of subarrangements $\mc A$ that
  have cardinality $3$ and rank $2$. Each of the other
  $\binom{6}{3}- \acute{\uptau}$ subarrangements with cardinality $3$ have rank
  $\acute{\mathbf r}$. All other subarrangements have rank equal to their
  cardinality.
\begin{linenomath*}
  \begin {equation}\label{eq-zaslavski-4}
    \begin{aligned}
      \lvert \acute{ \mc G} \rvert =&\binom{6}{6} (-1)^{6 -
        \acute{\mathbf{r}}}+\binom{6}{5}
      (-1)^{5 - \acute{\mathbf{r}}}+  \binom{6}{4}(-1)^{4-\acute{\mathbf{r}}}\\
      +&\binom{6}{3}(-1)^{3-\acute{\mathbf{r}}}-\,\,\,\,\acute{\mathbf{\uptau}}
      \,\,\,\,\,(-1)^{3-\acute{\mathbf{r}}}
      + \,\,\,\,\acute{\mathbf{\uptau}} \,\,\,\,\,(-1)^{3-2}\\
      +&\binom{6}{2}(-1)^{2-2}+\binom{6}{1}(-1)^{1-1} + \binom{6}{0}(-1)^{0-0}
    \end{aligned}
  \end{equation}
\end{linenomath*}  
Via \cref{prop-central-testworthy}, $\acute{\mathbf r} = \mathbf r$, so that
$\acute{\mathbf r} = 3$.  It remains for us to calculate the value of
$\acute{\uptau}$. Each of the $\binom{4}{3} = 4$ subsets of $Y$ that have
cardinality $3$ generates a subarrangement of cardinality $\binom{3}{2} =
3$. (For instance,
$\mc A^{\{x,y,z\}} =
\{\acute{H}^{\{x,y\}},\acute{H}^{\{y,z\}},\acute{H}^{\{x,z\}}\}$.)  For such
subarrangements, \fourjac\ implies a rank of $2$. Arguments from the final step
in the proof of \cref{lem-arrangement-cardinality-rank} confirm that every other
subarrangement with cardinality $3$ has rank $3$.  Substituting into
\cref{eq-zaslavski-4}, we obtain
\begin{linenomath*}
  \begin{equation*}
    \begin{aligned}
    \lvert  \acute{ \mc G} \rvert =
    % &\,\,\,1(-1)^{3}+6(-1)^{2}+15(-1)^{1}\\
    % +&20(-1)^{0}- 4(-1)^{0}+\,\,\,4(-1)^{1} \\
    % +& 15(-1)^{0}+6(-1)^{0}+\,\,\,1(-1)^{0} \\
    % =&
    -1+6-15+20-4-4+15+6+1=24 = 4!\hskip.5pt.
   \end{aligned}
 \end{equation*}
\end{linenomath*}
 The proof that every member of $\mc G$ is associated with a transitive ordering
 is as follows. Via \cref{prop-central-testworthy}, \fourjac\ holds for $\aext$
 if, and only if, it holds for $\ext$. We have seen that \fourjac\ holds. Then,
 via a straightforward application of the Jacobi identity and, for arbitary
 $K \in \mbbjp$, the definition of transitivity of $\aext_{K}$ yields
 \ref{TQ}. Thus, every member of $\acute{\mc G}$ is total.
  \end{proof}

  \paragraph{In the remaining case, where $Y = \{x,y,z,w\}$ and
    $\mathbf r = 2$,\hskip-8pt}
 the proof is complicated by the fact that maximally-diverse {generalization}s have centerless
 arrangements.
 We begin by choosing $\acute{\eta}^{\dd}$ so as to construct
 $\acute{u}^{\dd} = u^{\dd} \times \acute{\eta}^{\dd}$ with $\acute{\mathbf r} = 3$.

 Since $\mathbf r = 2$, it follows that $u^{\xz}, u^{\yz}$ and $u^{\wz}$ form a
 linearly dependent set. Thus, for some $\pi, \rho \in \R$,
\begin{linenomath*} 
  \begin{equation}\label{eq-iz}
   \pi u^{\xz} + \rho u^{\yz} = u^{\wz}.
 \end{equation}
\end{linenomath*}
Fix arbitrary $J \in \mbbj$ such that $\ext_{J}$ is total, and, as in
\cref{prop-central-testworthy}, \withoutlog, we take $J \in
G_{\mdoubleplus}^{(x,y,z,w)}$.  Then, for $\acute{\iota} = \half$ and
$\acute{J} = (1- \acute{\iota}) J \times \acute{\iota}$, let
\begin{linenomath*}
  \begin{equation}\label{eq-xyz}
   \acute{\eta}^{\ij} = - \tfrac{1-\acute{\iota}}{\acute{\iota}} \langle
    u^{\ij}, J \rangle = - \langle u^{\ij}, J \rangle, \quad \text{for every
    $i,j\in \{x,y,z\}$.}
 \end{equation}
\end{linenomath*}
%Since $G^{(x,y,z,w)}_{\mdoubleplus}$ is open and the inner product is
%continuous, there exists a compact neighbourhood $N_{J} \subset
%G^{(x,y,z,w)}_{\mdoubleplus}$ of $J$.
%such that, for every $L\in N_{J}$,
%\begin{linenomath*}
% \begin{equation*}
% \langle u^{\yz}, L\rangle < - \langle u^{\wz}, L \rangle \quad \textup{if,
% and only if,} \quad
% \langle u^{\yz},J \rangle <-\langle u^{\wz},J \rangle.
% \end{equation*}
%\end{linenomath*}
Let $f: G^{(x, y, z, w)}_{\mdoubleplus} \rightarrow \acute{G}^{(x, y, z,
w)}_{\mdoubleplus}$ be the mapping $L \rightarrow (1 - \acute{\lambda}) L
\times \acute{\lambda}$, where $\acute{\lambda}$ is the solution to
$\acute{\eta}^{\yz} = -\tfrac{1 - \acute{\lambda}}{\acute{\lambda}} \langle
u^{\yz}, L \rangle$.  In particular, substituting for $\acute{\eta}^{\yz}$
using \cref{eq-xyz}, we obtain $\frac{1- \acute{\lambda}}{\acute{\lambda}} =
\frac{\langle u^{\yz},J \rangle}{\langle u^{\yz}, L \rangle}$ and
$\acute{\lambda} = \frac{\langle u^{\yz}, L\rangle}{ \langle u^{\yz}, J \rangle
+ \langle u^{\yz}, L \rangle}$.  Since $J, L \in {G}^{\yz}$, all terms in the
expression for $\acute{\lambda}$ are positive, so that $0 < \acute{\lambda} <
1$ and, via convexity of $\acute{G}^{(x, y, z, w)}_{\mdoubleplus}$, $f$ is
well-defined. As the quotient of continuous functions of $L$ (with the
denominator $\langle u^{\yz}, J \rangle + \langle u^{\yz}, L \rangle$ 
bounded away from zero) $f$ is continuous.  Finally, $\lim_{L \rightarrow
J}f(L) = \acute{J}$.

Via \ref{p3dQ} and $J \in G^{(x, y, z, w)}_{\mdoubleplus}$, $\langle u^{\yz},
J\rangle \neq \langle u^{\zw}, J\rangle$ and both numbers are positive.  Via
$u^{\wz} = - u^{\zw}$, it follows that $\zeta = \frac{\langle u^{\wz},
J\rangle}{\langle u^{\yz}, J\rangle} < 0$ is the unique solution to 
\begin{linenomath*}
  \begin{equation*}
    \langle \zeta u^{\yz} - u^{\wz}, J \rangle = 0.
  \end{equation*}
\end{linenomath*}
Continuity of the map $L \mapsto \frac{\langle u^{\wz}, L\rangle}{\langle
u^{\yz}, L\rangle}$ on $G^{(x, y, z, w)}_{\mdoubleplus}$ and the fact that the
latter set is open suffices for the existence of a sequence $(L_n: n = 1, 2,
\dots)$, converging to $J$, such that, for every $n$, $L_n \mapsto \xi_{n} =
\frac{\langle u^{\wz}, L_n\rangle}{\langle u^{\yz}, L_n\rangle}$ satisfies
$\xi_n \neq \zeta$.

Next, take $(\epsilon_n : n = 1, 2, \dots)$ be the following non-zero
real-valued sequence of suitable small numbers that converge to zero:
\begin{linenomath*}
  \begin{equation} \label{eq-epsilon}
    \epsilon_n := \langle \xi_n u^{\yz} - u^{\wz}, J\rangle = \langle u^{\wz},
    \tfrac{1 - \acute{\lambda_n}}{\acute{\lambda_n}} L_n - J \rangle,
  \end{equation}
\end{linenomath*}
where $\frac{1 - \acute{\lambda_n}}{\acute{\lambda_n}} = \frac{\langle u^{\yz},
J\rangle}{\langle u^{\yz}, L_n\rangle}$ is defined as in the definition of $f$
above.
%(We arrive at \ref{eq-epsilon} via bilinearity of the inner product and
%simple substitution.)


%\footnote{Substituting for $\frac{1- \acute{\lambda}}{\acute{\lambda}}$ in
%the equation for $\epsilon$ in \eqref{eq-yzw} and rearranging yields a linear
%equation
%\begin{equation}\label{eq-k-linear}
% k \langle u^{\yz} , L \rangle =\langle u^{\wz} , L \rangle
% \end{equation}
% in $L$, where $k = \tfrac{\epsilon+\langle u^{\wz},J \rangle}{\langle
% u^{\yz},J\rangle}$. Note that, since $L \in G^{\zw}_{\mdoubleplus}\cap
% G^{\yz}_{\mdoubleplus}$, $\langle u^{\wz}, L \rangle <0$ and $\langle
% u^{\yz}, L \rangle > 0$; moreover, the same is true of $J$. Thus, $k<0$ or,
% equivalently, $\epsilon < -\langle u^{\wz},J \rangle$.  If $\langle u^{\yz},L
% \rangle < - \langle u^{\wz},L \rangle$, then, via \cref{eq-k-linear}, $k <
% -1$. Moreover, our choice of $N_{J}$ is such that $\frac{\langle u^{\wz},J
% \rangle}{\langle u^{\yz},J \rangle} < -1$. This ensures that, on $N_{J}$,
% $\epsilon$ lies in a neighbourhood of zero, as required.}
% %Thus, for any fixed $\epsilon \neq 0$, there is at least one
% % solution to \cref{eq-yzw}. For even in the case where $\mbbt = \{s, t\}$
% and
% % $L = (1- \lambda)\delta_{s}+ \lambda \delta_{t}$, we have two equations in
% % two
% % unknowns.


For every $(i, j) \in \{y,z,w\}^2 \bs \{(y, z), (z, y)\}$, let
$\acute{\eta}^{\ij} = - \frac{1 - \acute{\lambda}}{\acute{\lambda}} \langle
u^{\ij}, L \rangle$.  Now note that, for every $L \neq J$,
\begin{linenomath*}
  \begin{equation}\label{eq-3d-epsilon}
    \pi \acute{\eta}^{\xz} + \rho \acute{\eta}^{\yz} = \acute{\eta}^{\wz} +
    \epsilon \neq \acute{\eta}^{\wz}.
  \end{equation}
\end{linenomath*}
For $(i, j) \neq (x, w), (w, x)$, let $\acute{u}^{\ij}:= u^{\ij} \times
\acute{\eta}^{\ij}$.  Then, for every $\epsilon\neq 0$, 
\begin{linenomath*}
  \begin{equation}
    \{\acute{u}^{\ij}: \ij = \xz, \yz, \wz\}
  \end{equation}
\end{linenomath*}
  forms a linearly independent set.  To complete the definition of
  $\acute{u}^{\dd}$, we appeal to the fact that, via \threejac, $u^{\dd}$
  satisfies equations \eqref{eq-xy}--\eqref{eq-xz}. In particular, from these
  equations, take parameters $\alpha$, $\beta$, and $\gamma$ and let
  $\acute{\eta}^{\xw}$ be the (unique) solution to the Jacobi identity
\begin{linenomath*}
  \begin{equation}\label{eq-xw}
    \alpha \acute{\eta}^{\xw} = \gamma \acute{\eta}^{\xy} + \beta
    \acute{\eta}^{\yw} = -\langle \gamma u^{\xy} , J \rangle -
    \tfrac{1-\acute{\lambda}}{\acute{\lambda}}\langle\beta u^{\yw}, L \rangle.
  \end{equation}
\end{linenomath*}
For these parameter values, $\acute{u}^{\dd}$ also satisfies
\eqref{eq-xy}--\eqref{eq-xz} of the proof of \cref{lem-induction}. That is, for
$\{x,y,z\}$, via \cref{eq-xyz} and \eqref{eq-xz}, $\gamma
\acute{\eta}^{\xy}+\tau \acute{\eta}^{\yz} = \phi \acute{\eta}^{\xz}$, so that
$\acute{u}^{\dd}$ satisfies \eqref{eq-xz}. For $\{y,z,w\}$, Via \cref{eq-yzw}
and \eqref{eq-yz}, $\hat{\beta} \acute{\eta}^{\yw}+\sigma \acute{\eta}^{\wz} =
\tau \acute{\eta}^{\yz}$, so that $\acute{u}^{\dd}$ satisfies \eqref{eq-yz}.
For $\{x,y,w\}$, via \cref{eq-xw}, $\acute{u}^{\dd}$ satisfies \eqref{eq-xy}.
We now demonstrate that for the final triple $\{x,z,w\}$, the Jacobi identity
holds if $\hat{\beta} = \beta$, and
$\{\acute{u}^{\xw},\acute{u}^{\wz},\acute{u}^{\xz}\}$ has rank $3$ otherwise.

First extract the parameters from equations \eqref{eq-xy}--\eqref{eq-xz} to
obtain the matrix form
\begin{linenomath*}
  \begin{equation}\label{eq-matrix-6}
   \begin{blockarray}{ccccccc}
     \text{\footnotesize$\xw$} & \text{\footnotesize$\yw$} &
     \text{\footnotesize$\xy$} &\text{\footnotesize $\wz$}& \text{\footnotesize
     $\yz$} & \text{\footnotesize $\xz$}\\
     \begin{block}{[ccc|ccc]c}
       \alpha & -\beta & -\gamma & 0 & 0 & 0 &
       \text{\footnotesize\eqref{eq-xy}} \\
       0 & \hat{\beta} & 0 & \sigma& -\tau & 0 &
       \text{\footnotesize\eqref{eq-yz}}\\
       0 & 0 & \gamma & 0 & \tau & -\phi & \text{\footnotesize\eqref{eq-xz}}\\
     \end{block}
   \end{blockarray}
 \end{equation}
\end{linenomath*}
Since the triple $\{\acute{u}^{(i,z)}: i = x,y,w\}$ provides a basis for
$\spann(\acute{u}^{\dd})$, we will write all vectors in terms of this basis. To
this end, we derive the reduced row echelon form of \cref{eq-matrix-6}. In
particular, letting $r_{i}$ denote the rows of the matrix, we perform the
operation $r_{1} \mapsto r_{1} + \frac{\beta}{\hat{\beta}} r_{2} + r_{3}$ to
obtain
\begin{linenomath*}
  \begin{equation}\label{eq-matrix-6-echelon}
    \begin{blockarray}{ccccccc}
      \text{\footnotesize$\xw$} & \text{\footnotesize$\yw$} &
      \text{\footnotesize $\xy$} & \text{\footnotesize$\wz$} &
      \text{\footnotesize $\yz$} & \text{\footnotesize $\xz$}\\
      \begin{block}{[ccc|ccc]c}
        \alpha & 0 & 0 & \frac{\beta}{\hat{\beta}} \sigma&
        (1-\frac{\beta}{\hat{\beta}}) \tau & -\phi &
        \text{\footnotesize\eqref{eq-xy}} \\
        0 & \hat{\beta} & 0 & \sigma& -\tau & 0 &
        \text{\footnotesize\eqref{eq-yz}}\\
        0 & 0 & \gamma & 0 & \tau & -\phi & \text{\footnotesize\eqref{eq-xz}}\\
      \end{block}
    \end{blockarray}.
  \end{equation}
\end{linenomath*}
In \cref{eq-matrix-6-echelon}, the fact that $\hat{\beta}$ (instead of $\beta$)
that appears as a pivot in column $2$, is a consequence of the fact that, in
this derivation, we are choosing $\acute{v}^{\yw} = \hat{\beta}
\acute{u}^{\yw}$.
%\emph{Mutatis mutandis}, the conclusions we will draw are the
%same if instead we chose to define $\acute{v}^{\yw}$ using $\beta$.
The other (relevant) rows of $\acute{v}^{\dd}: Y^{2} \times \mbbtp \rightarrow
\R$ are $\acute{v}^{\xw} = \alpha \acute{u}^{\xw}$, $\acute{v}^{\xy} = \gamma
\acute{u}^{\xy}$, $\acute{v}^{\zw}= \sigma\acute{u}^{\zw}$, $\acute{v}^{\yz} =
\tau \acute{u}^{\yz}$ and
$\acute{v}^{\xz} = \phi \acute{u}^{\xz}$.  % From \cref{eq-matrix-6-echelon},
it
% is clear that the Jacobi identity
% $\acute{v}^{\xw} = \acute{v}^{\xz} + \acute{v}^{\zw}$ holds if, and only if,
% $\hat{\beta}= \beta$.  We now confirm that
% $\{\acute{v}^{\xw}, \acute{v}^{\xz}, \acute{v}^{\zw}\}$ has rank $2$ if, and
% only if, $\hat{\beta} = \beta$.
\begin{figure}[t]
    \begin{center}
      \begin{tikzpicture}
        
    \node[color=lightgray] (max) at (0,4) {\footnotesize$\{x,y,z,w\}$};
    
    \node (xyz) at (-6.55,2.75){\footnotesize$\{x,y,z\}$};

    \node (xyw) at (-4.66,2.75) {\footnotesize$\{x,y,w\}$};

    \node[color=lightgray] (xzlyw) at (-2.5,2.75) {\footnotesize$\{x,z\}\{y,w\}$};

    \node (xylzw) at (0,2.75) {\footnotesize$\{x,y\}\{z,w\}$};

    \node (xwlyz) at (2.5,2.75) {\footnotesize$\{x,w\}\{y,z\}$};

    \node (xzw) at (4.66,2.75) {\footnotesize$\{x,z,w\}$};

    \node (yzw) at (6.55,2.75) {\footnotesize$\{y,z,w\}$};
    
    \node (xy) at (-5,0.25) {\footnotesize$\{x,y\}$};

    \node (xz) at (-3,0.25) {\footnotesize$\{x,z\}$};

    \node (xw) at (3,0.25) {\footnotesize$\{x,w\}$};
    
    \node (yz) at (-1,0.25) {\footnotesize$\{y,z\}$};
   
    \node (yw) at (1,0.25) {\footnotesize$\{y,w\}$};

    \node (zw) at (5,0.25) {\footnotesize$\{z,w\}$};
    
    \node (min) at (0,-1) {\footnotesize$\emptyset$};

    \draw (min) -- (xy);
    \draw (min) -- (xz);
    \draw (min) -- (xw);
    \draw (min) -- (yz);
    \draw (min) -- (yw);
    \draw (min) -- (zw);

  \draw[color=lightgray] (max) -- (xyz);
    \draw[color=lightgray] (max) -- (xyw);
    \draw[color=lightgray] (max) -- (xzw);
    \draw[color=lightgray] (max) -- (yzw);
    \draw[color=lightgray] (max) -- (xylzw);
    \draw[color=lightgray] (max) -- (xzlyw);
    \draw[color=lightgray] (max) -- (xwlyz);

    \draw[color=blue](xyz)--(xy);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (xz);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (yz);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (zw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (yw);
    \draw[preaction={draw=white,-,line width=3pt},color=lightgray] (xzlyw) -- (yw);
    \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xwlyz) -- (yz);
    \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xwlyz) -- (xw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw) -- (xw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw) -- (yw);
    \draw[preaction={draw=white,-,line width=3pt},color=lightgray] (xzlyw) -- (xz);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyz) -- (yz);
    \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xylzw) -- (xy);
    \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xylzw) -- (zw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw)-- (xy);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (xw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (zw);
    \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyz) -- (xz);
\end{tikzpicture}
\caption{\label{fig-hasse-centerless-Y4} The intersection semilattices
  $\acute{\mc L}$ and
  $\acute{\mc L}_{\mdoubleplus} = \acute{\mc L} \bs \{A^{Y},
  \acute{A}^{\{x,z\}\{y,w\}}\}$ when $\countof \mbbt=2$, $\hat{\beta} = \beta$
  and $\epsilon$ is sufficiently small but distinct from zero.}
\end{center}
\end{figure}
The matrix of the equation that now follows, is invertible if, and only if,
$(1-\frac{\beta}{\hat{\beta}}) \neq 0$.
\begin{linenomath*}
  \begin{equation}\label{eq-matrix-3}
    \begin{blockarray}{[c]}
      \acute{v}^{\xw}\\
      \acute{v}^{\xz}\\
      \acute{v}^{\zw}
    \end{blockarray}
    =
    \begin{blockarray}{[ccc]}
      -\frac{\beta}{\hat{\beta}}  & -(1-\frac{\beta}{\hat{\beta}}) & 1 \\
      0 & 0 & 1\\
      -1 & 0 & 0\\
    \end{blockarray}
    \begin{blockarray}{[c]}
      \acute{v}^{\wz}\\
      \acute{v}^{\yz}\\
      \acute{v}^{\xz}
    \end{blockarray}
  \end{equation}
\end{linenomath*}
Thus, unless $\hat{\beta} = \beta$, we conclude that
$\{\acute{v}^{\xw}, \acute{v}^{\xz},\acute{v}^{\zw}\}$ has the same rank as
$\{\acute{v}^{\wz}, \acute{v}^{\yz}, \acute{v}^{\xz}\}$ which, by construction,
has rank $3$ for every choice of $\epsilon \neq 0$.

Since $\hat{\beta}= \beta$ if, and only if, \fourjac\ holds for $\ext$, we
conclude that \fourjac\ holds for $\ext$ if, and only if, it holds for $\aext$.
It remains for us to show that, for $\epsilon$ sufficiently small
$\acute{\mc G}$ is maximal. For if $\acute{\mc G}$ is maximal, then via
\cref{lem-Y3-r2} and \cref{lem-Y3-r3}, \ref{TQ} holds if, and only if,
$\rank\{\acute{v}^{\xw}, \acute{v}^{\xz},\acute{v}^{\zw}\} = 2$.

By Zaslavski's theorem, it suffices to show that every member of the
intersection lattice $\acute{\mc L}$, other than the center
$\acute{A}^{\{x,y,z,w\}}$, has nonempty intersection with
$\posreal^{\mbbtp}$. We now make explicit the dependence of the {generalization}
$\aext$ on our choice of $L \in N_{J}$, though we do so indirectly via
$\epsilon$.  Let
$d^{\epsilon} = \max \{\diff(\acute{J},\acute{A}): \acute{A} \in \acute{\mc
  L}^{\epsilon}\}$, where $\diff(\acute{J},\acute{A})$ is the minimum (Euclidean)
distance between $\acute{J}$ and the (closed) linear subspace $\acute{A}$ of
$\R^{\mbbtp}$. Note that for $\epsilon = 0$, we obtain a central arrangement of
the form of \cref{prop-central-testworthy} with $d^{\epsilon} = 0$. Moreover,
since the Euclidean metric is continuous in its arguments, and, for every
$\epsilon$, $\acute{\mc L}^{\epsilon}$ is finite, the map
$\epsilon \mapsto d^{\epsilon}$ is continuous and
$\lim_{\epsilon \rightarrow 0}d^{\epsilon} = 0$. Thus, for sufficiently small
$\epsilon \neq 0$, every $\acute{A} \in \acute{\mc L}^{\epsilon}$ intersects
$\posreal^{\mbbtp}$.

\begin{remark*}
  We note that the above arguments apply without modification to the case where
  $\countof \mc H = 4,5$. Consider, for example, the Hasse diagram of
  \cref{fig-hasse-centerless-Y4}. That case arises when $u^{\xz}$ and $u^{\yw}$
  are collinear, so that $A^{\{xz\}\{y,w\}}$ is a hyperplane of dimension
  $\countof \mbbt -1$. Assuming the same construction, with $\epsilon \neq 0$,
  so that $\acute{u}^{\dd}$ has rank $3$ and, via
  \cref{lem-arrangement-cardinality-rank}, $\acute{u}^{\xz}$ and
  $\acute{u}^{\yw}$ are linearly independent. Thus $\acute{A}^{\{xz\}\{y,w\}}$ is
  of dimension $\countof \mbbtp - 2 = \countof \mbbt - 1$. Thus,
  $\acute{A}^{\{x,z\}\{y,w\}} = A^{\{x,z\}\{y,w\}} \times \{0\}$ which belongs
  to the boundary of $\posreal^{\mbbtp}$. At $\epsilon = 0$,
  $\acute{A}^{\{x,z\}\{y,w\}}$ increases by one dimension and the upper two
  levels of the Hasse diagram collapse to equal $A^{\{x,y,z,w\}}$.
\end{remark*}
\newpage
\makeatletter
\def\@seccntformat#1{Online Appendix\,\csname the#1\endcsname.\quad}
\makeatother
\section{Proofs}
\printProofs
\end{appendices}








% \begin{appendices}
% \section{Proofs}\label{sec_Proofs}
% \subsection{Proof of \cref{thm-mainQ}}
% %\printproofs
% \end{appendices}
%\printbibliography


% \printproofs
\end{document}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End
