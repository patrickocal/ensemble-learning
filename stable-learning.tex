% Template for the submission to:
%   Econometrica   [ecta]
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% In this template, the places where you   %%
%% need to fill in your information are     %%
%% indicated by '???'.                      %%
%%                                          %%
%% Please do not use \input{...} to include %%
%% other tex files. Submit your LaTeX       %%
%% manuscript as one .tex document.         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%=============================================================================%
% use option [draft] for initial submission
%            [final] for the prepublication
\documentclass[ecta,nameyear,draft]{econsocart}
%
%-----------packages----------------------------------------------------------%
\usepackage{amsmath}%[fleqn]
\usepackage{mathabx} % for \simeq for instance
\usepackage{amsthm,amssymb}
\usepackage{mathtools}
\usepackage{bm}
%-----------------------------------------------------------------------------%
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{calc,shapes.geometric}
\usepackage{pgf}
\usepackage{pgfplots,pgfplotstable,subcaption}
\usepackage{lineno}
\pgfplotsset{compat=1.18}
\RequirePackage[colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue,pagebackref]{hyperref}
\usepackage[nodayofweek]{datetime} % to change the way dates and times are displayed
\usepackage{epigraph}%to write epigraphs
\usepackage{accents}
\usepackage{dsfont}
\usepackage{upgreek}
\usepackage[nameinlink]{cleveref}
\usepackage[shortlabels]{enumitem}
\usepackage{blkarray} % for block/bordered matrices
\usepackage[conf={one big link}]{proof-at-the-end}
\pgfkeys{/prAtEnd/global custom defaults/.style={
    proof at the end,
%    no link to proof
%one big link={The proof is on page~\pageref*{proof:prAtEnd\pratendcountercurrent}.}
}}
\usepackage{nicefrac}
%-----------local definitions-------------------------------------------------%
\startlocaldefs
%-----------begin math notation and shorthand---------------------------------%
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\newcommand{\reg}{\operatorname{reg}}
\newcommand{\nov}{\operatorname{nov}}
\newcommand{\test}{\operatorname{test}}
\newcommand{\proj}{\operatorname{Proj}}
\newcommand{\Ext}{\operatorname{ext}}
\newcommand{\countof}{\mathbin{\#}\hskip1pt}
\newcommand*{\medcap}{\mathbin{\scalebox{.85}{\ensuremath{\bigcap}}}}%
\newcommand*\mcap{\mathbin{\mathpalette\mcapinn\relax}}
\newcommand*\mcapinn[2]{\vcenter{\hbox{$\mathsurround=0pt
  \ifx\displaystyle#1\textstyle\else#1\fi\bigcap$}}}
\newcommand{\bs}{-}%\operatorname{\setminus}}
\newcommand{\relint}{\operatorname{ri}}
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\mbb}{\mathbb}
\newcommand{\mc}{\mathcal}
\newcommand{\ds}{\Delta}
\newcommand{\id}{\mathbf{id}}
\newcommand{\ccirc}{\kern0.5ex\vcenter{\hbox{$
\scriptstyle\circ$}}\kern0.5ex}
%\newcommand{\overbar}{\widebar}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}%\sffamily
\newcommand{\relations}{\operatorname{rel}}
\newcommand{\novel}{\mathfrak f}
\newcommand*\underdot[1]{%
  \underaccent{\dot#1}}
\newcommand{\overcirc}{\accentset{\ccirc}}
\newcommand*\undercirc[1]{%
  \underaccent{\ring#1}}
\newcommand{\smallgeq}{\mbox{\larger[-4]$\geq$}}
\newcommand{\smallleq}{\mbox{\larger[-4]$\leq$}}
\newcommand{\smallstrictlygr}{\mbox{\larger[-4]$>$}}
\newcommand{\smallstrictlyless}{\mbox{\larger[-4]$<$}}
\newcommand{\smallzero}{\mbox{\larger[-5]$0$}}
\newcommand{\smallasterix}{\mbox{\larger[-4]$*$}}
\newcommand{\smallplus}{\mbox{\larger[-4]$+$}}
\newcommand{\nneg}{\smallgeq}
\newcommand{\pos}{{\mpplus}}
\newcommand{\posint}{\mbb Z_{\mpplus}}
\newcommand{\nnint}{{\mbb Z}_{\mplus}}
\newcommand{\posrat}{\mbb Q_{\mpplus}}
\newcommand{\posreal}{\mbb R_{\mpplus}}
\newcommand{\negreal}{\mbb R_{-}}
\newcommand{\nnrat}{{\mbb Q}_{\mplus}}
\newcommand{\nnreal}{{\mbb R}_{\mplus}}
\newcommand\mpplus{\text{\srcsize$+\mkern-2mu+$}}
%\newcommand\mpplus{\text{\srcsize$> 0$}}
\newcommand\mplus{\text{\srcsize$+$}}
\newcommand{\spann}{\operatorname{span}}
\newcommand{\precsimb}{\mathbin{\preceq}}
\newcommand{\precb}{\mathbin{\prec}}
\newcommand{\approxb}{\mathbin{\simeq}}
\newcommand{\simb}{\mathbin{\sim}}
\newcommand{\preceqb}{\mathbin{\preceq}}
\newcommand{\simeqb}{\mathbin{\simeq}}
\newcommand{\half}{\frac{1}{2}}

\newcommand{\ext}{\mathrel{\mc R}}
\newcommand{\sext}{\mathrel{\mc P}}
\newcommand{\next}{\mathrel{\mc I}}
\newcommand{\supext}{{\ext}}

\newcommand{\extb}{\mathbin{\mc R}}
\newcommand{\sextb}{\mathbin{\mc P}}
\newcommand{\nextb}{\mathbin{\mc I}}

\newcommand{\dext}{\mathrel{\tilde{\mathrel{\mathcal R}}}}
\newcommand{\dsext}{\mathrel{\tilde{\mathrel{\mathcal P}}}}
\newcommand{\dnext}{\mathrel{\tilde{\mathrel{\mathcal I}}}}

\newcommand{\dextb}{\mathbin{\tilde{\mathbin{\mathcal R}}}}
\newcommand{\dsextb}{\mathbin{\tilde{\mathbin{\mathcal P}}}}
\newcommand{\dnextb}{\mathbin{\tilde{\mathbin{\mathcal I}}}}

\newcommand{\hext}{\mathrel{\hat{\mathrel{\mathcal R}}}}
\newcommand{\hsext}{\mathrel{\hat{\mathrel{\mathcal P}}}}
\newcommand{\hnext}{\mathrel{\hat{\mathrel{\mathcal I}}}}

\newcommand{\hextb}{\mathbin{\hat{\mathbin{\mathcal R}}}}
\newcommand{\hsextb}{\mathbin{\hat{\mathbin{\mathcal P}}}}
\newcommand{\hnextb}{\mathbin{\hat{\mathbin{\mathcal I}}}}

\newcommand{\aext}{\mathrel{\acute{\mathrel{\mathcal R}}}}
\newcommand{\asext}{\mathrel{\acute{\mathrel{\mathcal P}}}}
\newcommand{\anext}{\mathrel{\acute{\mathrel{\mathcal I}}}}

\newcommand{\aextb}{\mathbin{\acute{\mathbin{\mathcal R}}}}
\newcommand{\asextb}{\mathbin{\acute{\mathbin{\mathcal P}}}}
\newcommand{\anextb}{\mathbin{\acute{\mathbin{\mathcal I}}}}

\newcommand{\gext}{\mathrel{\grave{\mathrel{\mathcal R}}}}
\newcommand{\gsext}{\mathrel{\grave{\mathrel{\mathcal P}}}}
\newcommand{\gnext}{\mathrel{\grave{\mathrel{\mathcal I}}}}

\newcommand{\gextb}{\mathbin{\grave{\mathbin{\mathcal R}}}}
\newcommand{\gsextb}{\mathbin{\grave{\mathbin{\mathcal P}}}}
\newcommand{\gnextb}{\mathbin{\grave{\mathbin{\mathcal I}}}}

\newcommand{\cext}{\mathrel{\check{\mathrel{\mathcal R}}}}
\newcommand{\csext}{\mathrel{\check{\mathrel{\mathcal P}}}}
\newcommand{\cnext}{\mathrel{\check{\mathrel{\mathcal I}}}}

\newcommand{\cextb}{\mathbin{\check{\mathbin{\mathcal R}}}}
\newcommand{\csextb}{\mathbin{\check{\mathbin{\mathcal P}}}}
\newcommand{\cnextb}{\mathbin{\check{\mathbin{\mathcal I}}}}



%\newcommand{\tran}{\textup{total}}
\newcommand{\total}{\textup{total}}
%\newcommand{\intran}{\textup{intran}}
\newcommand{\preferences}{\preceqb}
%\newcommand{\trelations}{{\bm{\mc T}}}
\newcommand{\jac}{\textup{jac}}
%\newcommand{\rep}{\textup{rep}}

\newcommand{\unaware}{\mathbb U}
\newcommand{\ememories}{\mathbb E}
\newcommand{\icases}{\mathbb F}
\newcommand{\ccases}{\mathbb G}


\newcommand{\mbbd}{{\mathds D}}
\newcommand{\mbbdp}{{\mathds D^{\novel}}}
\newcommand{\dpp}{{\mathfrak D}}


\newcommand{\mbbc}{{\mathds C}}
\newcommand{\mbbcp}{{\mathds C^{\novel}}}
\newcommand{\cpp}{{\mathfrak C}}

\newcommand{\mbbt}{{\mathds {T}}}
\newcommand{\mbbtp}{{\mathds{T}^\novel}}
\newcommand{\mbbtpp}{{\mathfrak{T}}}


\newcommand{\mbbi}{{\mathds L}}
\newcommand{\mbbip}{{\mathds{L}^{\novel}}}
\newcommand{\mbbipp}{{\mathfrak L}}

\newcommand{\mbbj}{\mathds J}
\newcommand{\mbbjp}{{\mathds {J}^{\novel}}}
\newcommand{\mbbjpp}{\mathfrak{I}}

\newcommand{\past}{{D^\star}}

 \newcommand{\lbc}{\left\{}
\newcommand{\rbc}{\right\}}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}


\newcommand{\lbk}{\left(}
  \newcommand{\rbk}{\right)}

\newcommand{\ndash}{\textendash}


\newcommand{\indet}{\mathbb I}

\newcommand{\rel}{r}

\newcommand{\xp}{{x'}}
\newcommand{\xpp}{{x''}}
\newcommand{\xppp}{{x'''}}

\newcommand{\xxp}{{(0,1)}}
\newcommand{\xpx}{{(1,0)}}
\newcommand{\xxpp}{{(0,2)}}
\newcommand{\xpxpp}{{(1,2)}}
\newcommand{\xppxp}{{(2,1)}}

\newcommand{\yyp}{{(0,1)}}
\newcommand{\ypy}{{(1,0)}}
\newcommand{\yypp}{{(0,2)}}
\newcommand{\ypypp}{{(1,2)}}
\newcommand{\yppyp}{{(2,1)}}

\renewcommand{\ij}{{(i, j)}}
\newcommand{\ji}{{(j, i)}}


\newcommand{\xx}{(x,x)}
\newcommand{\xy}{{(x, y)}}
\newcommand{\yx}{{(y, x)}}

\newcommand{\yz}{{(y,z)}}
\newcommand{\zy}{{(z,y)}}

\newcommand{\xz}{{(x,z)}}
\newcommand{\zx}{{(z,x)}}

\newcommand{\xw}{{(x,w)}}
\newcommand{\wx}{{(w,x)}}

\newcommand{\yw}{{(y,w)}}
\newcommand{\wy}{{(w,y)}}

\newcommand{\zw}{(z,w)}
\newcommand{\wz}{(w,z)}

\newcommand{\xpw}{(x^{\prime},w)}
\newcommand{\wxp}{(x,x^{\prime})}

\newcommand{\xpy}{(x^{\prime},y)}
\newcommand{\yxp}{(y,x^{\prime})}

\newcommand{\xpz}{(x^{\prime},z)}
\newcommand{\zxp}{(z,x^{\prime})}

\newcommand{\xyz}{{(x,y,z)}}

\newcommand{\dd}{{(\cdot,\cdot)}}
\newcommand{\justdd}{\cdot\cdot}

\newcommand{\ab}{{(a,b)}}
\newcommand{\ba}{{(b,a)}}
\newcommand{\uh}{\hat {\mathbf{u}}}
\renewcommand{\u}{{\mathbf{u}}}
\newcommand{\up}{{\mathbf{u}'}}
\newcommand{\upp}{{\mathbf{u}''}}
\newcommand{\uppp}{{\mathbf{u}'''}}

\newcommand{\vh}{\hat {\mathbf{v}}}
\renewcommand{\v}{{\mathbf{v}}}
\newcommand{\vp}{{\mathbf{v}'}}
\newcommand{\vpp}{{\mathbf{v}''}}
\newcommand{\vppp}{{\mathbf{v}'''}}

\newcommand{\bmu}{\bm{\upmu}}
\newcommand{\eh}{\hat {\varepsilon}}
\newcommand{\ep}{{\epsilon'}}
\newcommand{\epp}{{\epsilon''}}
\newcommand{\eppp}{{\epsilon'''}}

\newcommand{\ih}{\hat {I}}
\newcommand{\ip}{{I '}}
\newcommand{\ipp}{{I ''}}
\newcommand{\ippp}{{I '''}}
\newcommand{\stability}{\ref{S}}
\newcommand{\Stability}{\textsc{{Stability}}}
\newcommand{\stabilityq}{\textsc{{stability}$^\flat$}}
\newcommand{\Stabilityq}{\textsc{{Stability}$^\flat$}}
\newcommand{\threepru}{\textsc{iii}-\textup{{stability}}}
\newcommand{\parthreediv}{\textsc{p3d}}
\newcommand{\Parthreediv}{\textsc{P3d}}
\newcommand{\condtwodiv}{\textsc{c2d}}
\newcommand{\Condtwodiv}{\textsc{C2d}}
\newcommand{\parthreedivq}{\textsc{p3d}$^\flat$}
\newcommand{\Parthreedivq}{\textsc{P3d}$^\flat$}
\newcommand{\condtwodivq}{\textsc{c2d}$^\flat$}
\newcommand{\Condtwodivq}{\textsc{C2d}$^\flat$}
\newcommand{\condthreediv}{\textup{conditional-\textsc{iii}-diversity}}
\newcommand{\Condthreediv}{\textup{Conditional-\textsc{iii}-diversity}}
\newcommand{\twodiv}{\textsc{ii}-\textup{diversity}}
\newcommand{\twodivhash}{\textsc{ii}-\textup{Div}$^{\hash}$}
\newcommand{\twodivstar}{\textsc{ii}-\textup{Div}$^{*}$}

\newcommand{\fourdiv}{\textsc{iv}-\textsc{diversity}}
\newcommand{\fourdivhash}{\textsc{iv}-\textup{Div}$^{\hash}$}
\newcommand{\fourdivstar}{\textsc{iv}-\textup{Div}$^{*}$}

\newcommand{\threediv}{\textsc{iii}-\textsc{diversity}}

\newcommand{\fourjac}{\textup{\textsc{iv}-Jac}}
\newcommand{\threejac}{\textup{\textsc{iii}-Jac}}
%%% Define a new math font
\DeclareSymbolFont{boldoperators}{OT1}{cmr}{bx}{n}
\SetSymbolFont{boldoperators}{bold}{OT1}{cmr}{bx}{n}

%-----------begin math operators----------------------------------------------%
\DeclareMathOperator{\rank}{\textup{rank}}
\DeclareMathOperator{\dis}{d}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\graph}{gr}
\DeclareMathOperator{\gr}{gr}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\range}{Ran}

\DeclareMathOperator{\bd}{Bd}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\interior}{Int}
\DeclareMathOperator{\extrema}{Extrema}
\DeclareMathOperator{\ess}{ess}
%\DeclareMathOperator{\id}{id}
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.55}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
%
\makeatletter
\providecommand*{\diff}%------the differential in integrals
{\@ifnextchar^{\DIfF}{\DIfF^{}}}
\def\DIfF^#1{%
\mathop{\mathrm{\mathstrut d}}%
\nolimits^{#1}\gobblespace}
\def\gobblespace{%
\futurelet\diffarg\opspace}
\def\opspace{%
\let\DiffSpace\!%
\ifx\diffarg(%
\let\DiffSpace\relax
\else
\ifx\diffarg[%
\let\DiffSpace\relax
\else
\ifx\diffarg\{%
\let\DiffSpace\relax
\fi\fi\fi\DiffSpace}
%--------------------------------------end math operators --------------------%
%--------------------------------------begin acronyms-------------------------%

\makeatletter
\newcommand\etc{etc\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\ie{i\@.e\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\eg{e\@.g\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\Eg{E\@.g\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\Wlog{W\@.l\@.o\@.g\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\withoutlog{w\@.l\@.o\@.g\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\suchthat{s\@.t\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\wrt{w\@.r\@.t\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\uhc{u\@.h\@.c\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\lhc{l\@.h\@.c\@ifnextchar.{}{.\@}}
\makeatother

\makeatletter
\newcommand\cf{c\@.f\@ifnextchar.{}{.\@}}
\makeatother


%----------------------------------------end acronyms-------------------------%
%-----------------------------start colors and sizes--------------------------%
\definecolor{patrickcolor1}{rgb}{1,.4,0}
\definecolor{patrickcolor2}{rgb}{1,0,1}
\definecolor{patrickcolor3}{rgb}{.4,.7,0}
\definecolor{patrickcolor4}{rgb}{0.5,1,0}

\makeatletter
\newcommand{\srcsize}{\@setfontsize{\srcsize}{3pt}{3pt}}
\makeatother
\makeatletter
\newcommand{\srcsizetwo}{\@setfontsize{\srcsizetwo}{2pt}{2pt}}
\makeatother
%----------------------------end colors and sizes-----------------------------%
\newcommand{\gsii}{$\textup{GS03}$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Axiom, Claim, Corollary, %%
%% Lemma, Theorem, Proposition, Hypothezis, %%
%% Fact                                     %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem*{axiom*}{Axiom}
%\newtheorem*{axioms}{Axioms}
\newtheorem*{lemma*}{Lemma}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem*{claim*}{Claim}
\newtheorem{claim}{Claim}[lemma]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem*{conjecture*}{Conjecture}
\newtheorem{conjecture}{Conjecture}%[section]
\newtheorem*{corollary*}{Corollary}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{observation*}{Observation}
\newtheorem{observation}{Observation}
\newtheorem*{assumption*}{Richness~Assumption}
\newtheorem{assumption}{Assumption}
\newtheorem*{stability*}{Stability axiom}
\newtheorem*{diversity*}{$k$-Diversity axiom}
\newtheorem*{axioms*}{The basic axioms}
\newtheorem*{condtwodiv*}{Conditional-ii-diversity axiom}
\newtheorem*{parthreediv*}{Partial-iii-diversity axiom}
\newtheorem*{condtwodivq*}{Conditional-ii-diversity$^\flat$ axiom}
\newtheorem*{parthreedivq*}{Partial-iii-diversity$^\flat$ axiom}
\newtheorem*{4-ind*}{\textsc{iv}-Independence}
\newtheorem*{k-jac*}{$k$-Jac}
%----------axioms environment-------------------------------------------------%
\newtheorem{taggedtheoremy}{}
\newenvironment{taggedblank}[1]
 {\renewcommand\thetaggedtheoremy{#1}\taggedtheoremy}
 {\endtaggedtheoremy}
%-----------------------------------------------------------------------------%
\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
  \hbox{}\nobreak\hfil(#1)%
  \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}
\newsavebox\mybox
%-----------axiom environment-------------------------------------------------%
\newtheorem{taggedtheoremz}{$k$-Diversity axiom}
\newenvironment{taggeddiv}[1]
 {\renewcommand\thetaggedtheoremz{#1}\taggedtheoremz}
 {\endtaggedtheoremz}
%-----------axiom environment-------------------------------------------------%
\newtheorem{taggedtheoremx}{Stability axiom ---}
\newenvironment{taggedstability}[1]
 {\renewcommand\thetaggedtheoremx{#1}\taggedtheoremx}
 {\endtaggedtheoremx}
%-----------axiom environment-------------------------------------------------%
\newtheorem{taggedtheoremw}{Conditional-2-diversity axiom ---}
\newenvironment{taggedc2d}[1]
 {\renewcommand\thetaggedtheoremw{#1}\taggedtheoremw}
 {\endtaggedtheoremw}
%-----------axiom environment-------------------------------------------------%
\newtheorem{taggedtheoremv}{Partial-3-diversity axiom ---}
\newenvironment{taggedp3d}[1]
 {\renewcommand\thetaggedtheoremv{#1}\taggedtheoremv}
 {\endtaggedtheoremv}
%-----------------------------------------------------------------------------%
%\newtheorem{taggedp3d}{Partial-3-diversity axiom}
%\makeatletter
%\newcommand{\settheoremtag}[1]{% \settheoremtag{<tag>}
%  \let\oldthetheorem\thetheorem% Store \thetheorem
%  \renewcommand{\thetheorem}{#1}% Redefine it to a fixed value
%  \g@addto@macro\endtheorem{% At \end{theorem}, ...
%    \addtocounter{theorem}{-1}% ...restore theorem counter value and...
%    \global\let\thetheorem\oldthetheorem}% ...restore \thetheorem
%  }
%\makeatother
%\newtheorem{theorem}{Theorem}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{???}[???]{???}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Definition, Example, Remark,         %%
%% Notation, Property                       %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{step}{Step}[section]
\newtheorem{definition}{Definition}%[section]
\newtheorem{example}{Example}%[section]
\newtheorem{remark}{Remark}%[section]
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{remark*}{Remark}
%\theoremstyle{remark}
%\newtheorem*{???}{???}
%\newtheorem{???}{???}[???]
%\newtheorem{remark}{Remark}[section]
%\newtheorem{???}[???]{???}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlocaldefs

%=============================================================================%
\begin{document}
%-----------------------------------------------------------------------------%
 
\begin{frontmatter}

%-----------------------------------------------------------------------------%
\title{Stable learning and no-arbitrage pricing\\ based on sentiments}
\runtitle{Stable learning}
%-----------------------------------------------------------------------------%

\begin{aug}
% use \particle for den|der|de|van|von (only lc!)
% [id=?,addressref=?,corref]{\fnms{}~\snm{}\ead[label=e?]{}\thanksref{}}
%
%% e-mail is mandatory for each author
%
%%% initials in fnms (if any) with spaces
%
\author[id=au1,addressref={add1}]
  {\fnms{Patrick}~\snm{O'Callaghan}\ead[label=e1]{p.ocallaghan@uq.edu.au}}
%-----------------------------------------------------------------------------%
% Addresses %
%-----------------------------------------------------------------------------%
\address[id=add1]{%
\orgdiv{Australian Institute for Business and Economics},
  \orgname{University of Queensland}}
\end{aug}

%% Put support info here.  Reminder: do not thank the handling coeditor anonymously or by name
\support{ I thank Itzhak Gilboa for engaging in helpful discussions and
  suggestions along the way; Khoa Hoang for helpful discussions and pointers to
  the data; and Flavio Menezes for helpful suggestions in the latter stages.
  This version has time stamp \currenttime~(AEST), \today. Other versions can
  be found at \url{https://arxiv.org/abs/1904.02934} and
\url{https://github.com/patrickocal/stable-learner}.  }
%
%-----------------------------------------------------------------------------%
\begin{abstract} \input{abstract/abstract-for-circ.tex} \end{abstract}
%-----------------------------------------------------------------------------%
%
%-----------------------------------------------------------------------------%
\begin{keyword} \kwd{case-based decision theory} \kwd{machine learning}
  \kwd{prediction} \kwd{stability} \kwd{kernel functions} \kwd{no-arbitrage}
\kwd{yield curves} \end{keyword}
%-----------------------------------------------------------------------------%
%
%-----------------------------------------------------------------------------%
\setlength{\epigraphwidth}{11.5cm} \epigraph{From the past, the present acts
prudently, lest it spoil future action.}{\emph{Titian:  Allegory of Prudence}}
%-----------------------------------------------------------------------------%
%
\end{frontmatter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main text entry area:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------------%
\section{Introduction} \label{sec-introduction}
%-----------------------------------------------------------------------------%
%\textsc{Prediction is based rich data or a fertile imagination}. 
\textsc{Memory and the imagination are complements in prediction}. Either the
learner (prediction model) has access to a sufficiently diverse set of past
cases, or it has the capacity to explore the world beyond the data when the
data is insufficient.
%The memory and the imagination go hand in hand: they complements in prediction.
To paraphrase \citep[pages 9 and 10]{hume1896treatise}: \emph{the memory is
  tied down, by original impressions, without any power of variation; in
contrast, the imagination is free to transpose and change its ideas.}
Well-established neuroscientific evidence points to an important role for the
imagination and the degrees of freedom or flexibility that it provides in
improving on the data when quality is scarce (\eg\ due to failures of memory)
\citep{bartlett1932remembering,suddendorf2007evolution,mullally2013memory}. In
machine learning, the practice of splitting the data into a training set and a
test set is standard \citep{hastie2009elements}. Cases in the training set
resemble memories and cases in the test set resemble instances of the
imagination, for they allow the learner to explore unseen and potentially novel
data.

The key to inductive inference and good predictions is generalization: the
ability to grow the model to previously unseen data and objects of interest.
The key to generalization is stability
\citep{bousquet2002stability,poggio2004general,mukherjee2006learning}. Loosely
speaking, a learner is stable if small changes in the data yield small changes
in predictions: continuity of the learning map from training sets to
weighting functions (kernels).  \cite{mccloskey1989catastrophic} provides
famous examples of unstable learning where training on new cases causes current
kernels to unravel in what is called catastrophic interference.

As in \citet[henceforth, \gsii]{gilboa2003inductive},
\citet{billot2005probabilities,gilboa2006empirical,argenziano2019second}, the
learner's kernel is a subjective assessment of how similar past cases are to
the current prediction problem.  We derive the learner's kernel on basis of
\emph{sentiments}: a mapping from samples of past cases to rankings of
eventualities (objects of interest).  To account for all kinds of
intelligence, we provide a framework that captures the behaviour of learners
with the ability to externally validate their model. The axioms we provide are
agnostic in relation to the method of external validation. The goal is instead
to allow an observer to determine whether or not the learner is able to
generalise to richer training sets of past cases in a consistent manner. Our
framework adopts a within-sample perspective \citep[page 7, experiment
1]{chervonenkis2015recollections} and considers the potential for generalizing
to richer samples of unseen cases.

Via three of the four basic axioms of \gsii\ (completeness, combination and
archimeanity), all learners (stable or not) are characterised by a kernel that,
\emph{for every pair of eventualities}, gives rise to a linear function on
samples of past cases. This restriction to linearity is not severe since one
can appeal to the ``kernel trick'' (of machine learning) and embed sentiments
and the kernel in a higher-dimensional vector space where the model is linear.
Indeed, the key feature of \gsii\ is the endogeneity of the dimension (number
of case types) of the model. The fourth basic axiom is of course transitivity.

Our derivation of the learner's kernel introduces two new axioms:
partial-3-diversity and stability. Partial-3-diversity requires that, for every
subset of $k \leq 3$ eventualities, at least $k$ distinct total orderings
feature in {sentiments}. This is a substantial weakening of the diversity axiom
(henceforth 4-diversity) axiom of \gsii: for every subset of $k \leq 4$
eventualities, at least $k!$ distinct total orderings feature in sentiments.
%\footnote{Here a \emph{kernel} is simply a real-valued weighting
%function of two variables.} 
Stablility encompasses the basic axioms and strengthens transitivity
by requiring that learners generalize to new data (novel case types) without:
(i) re-evaluating past rankings; (ii) generating intransitive rankings; or
(iii) suppressing novel (transitive) rankings. A stable learner's kernel is
literally stable whilst also allowing for meaningful, unbiased generalisations.

By strengthening the basic axioms, we allow for data that is less than
4-diverse. We justify our extension by testing partial-3-diversity and
4-diversity on daily US nominal yield curve data for the period 1961-2023. To
conduct the test, we count the number of days until we are certain a diversity
axiom holds: under the null hypothesis of linear a pricing kernel. We find
that, if one restricts the model to maturities of 1-8 years that trade since
1961, partial-3-diversity holds after the initial 1229 trading days (8\% of
days). For this restricted domain of maturities, 4-diversity holds after 20\%
of days. For the full set of 30 maturities that trade since 1985, the picture
is rather different. Partial-3-diversity holds after the initial 785 trading
days (8\% of days). For 3-diversity (\emph{a fortiori} 4-diversity) we still
cannot prove it holds after 9292 days (100\% of days). Moreover, the failure is
substantial: there is an insufficient variety of rankings  for over 20\% of
subsets of maturities with cardinality four.
 
We choose the market for zero-coupon bonds because of relatively mature and
stable nature. It is a standard setting for exploring arbitrage conditions
\citep{barillas2019speculation}. The relationship between stability and
arbitrage is fundamental to our model.
\footnote{The learner is stable if, and
  only if, the above pairwise kernel forms a groupoid over eventualities. In
  market settings, the groupoid property translates to (no-)arbitrage
conditions.}

%Whereas the connection between stability in the present context and in machine
%learning is obvious, the same cannot be said for diversity conditions.
%In machine learning, the working assumption is that there exists a probability 
%measure on the sample space and the training set is already sufficiently large
%to be representative of the population.
Diversity conditions measure the extent to which our current data is
representative of the potential sample space. It is natural to expect a
trade-off between diversity and the need for external validation in order to
guarantee stable learning and hence generalization to new training sets. When
4-diversity holds, generalization is guaranteed via the main theorem of \gsii:
the data is sufficiently rich.\footnote{The present framework makes no measure
  theoretic or topological assumptions. Akin to exchangeability, we require
  that the cases themselves carry the necessary information determine the
  kernel, not their order of arrival. Past cases can then be resampled to form
  new training sets and new rankings of eventualities. (See
\cref{sec-applications}.)} The threshold or emergent effects, where large
language models suddenly acquire abilities that smaller models do not, is
conceivably be due to 4-diversity conditions being satisfied, since this
requires less ``imagination''.

Stable learners stand in contrast to those that learn by doing. Children are
encouraged to learn by making (noncatastrophic) mistakes. They are free to
modify their beliefs and behaviour after the event: the intransitive rankings
that novel case types may reveal are of little consequence
\citep{bradbury1990effects}. (See
\citep{weinstein1968transitivity,bradbury1974transitivity} for more evidence of
intransitive behaviour in children.) But what of a market maker that exposes
their pricing kernel either directly or indirectly (by buying and selling
securities) on a financial market? We show that a market maker is unstable if,
and only if, the returns on investment that they offer provide a free lunch to
other agents today (\ie\ before any novel case types arrive and before they
have a chance to re-evaluate).

The equivalence between stability (when novel data arrives tomorrow) and
no-arbitrage (today) has a number of interesting implications. First, at the
individual level, unstable learners are unlikely to survive in markets. Second,
we can express the pricing kernel of a stable learner as an empirical
(geometric) mean. This means that, in stable markets (\ie\ those with only
stable learners), prices are efficient stores of information. A stable market
price formation process is modular in cases and separable in securities. This
implies that, if we are also given the order that past cases arrived, we can
identify the stable market pricing kernel (\ie\ the weighting function implied
by market prices). Third, suitably sophisticated learners may ``massage'' their
kernels to ensure they are arbitrage-free (and thus stable). This alternative
method of external validation is reminscent of De Finetti's Dutch books
argument for the formula of conditional probabilities: isn't this what bookies
do? Thus, in market settings, there is another form of external validation, one
that does not rely on sentiments, but direct manipulation of numbers. In some
market settings, we might expect such behaviour, in others, we might not.

The second implication speaks directly to the timeless topic of market
efficiency \citet{fama1970efficient,malkiel2003efficient}.  Stable pricing
kernels provide an explanation for why we might expect a passive (\ie\
buy-and-hold) strategy to perform at least as well as any other strategy.  (The
semi-martingale hypothesis of \citet{fama1970efficient}.) This explanation is
as follows. If the market pricing kernel is stable, then the success of an
active strategy depends solely on its ability to predict the impact of the next
case type.  Whilst this does simplify the task, it also means that the impact
of the next case is unlikely to be large. This is the nature of the empirical
mean: the most likely cases are those that have occurred most frequently in the
past. The impact of a new case is likely to be crowded out by the frequency of
those that it resembles.  On the flip side, unstable pricing kernels come with
arbitrage opportunities and learners modifying their kernels.  Moreover, by
virtue of the fact that unstable market pricing kernels cannot be expressed as
an empirical mean: price movements are likely to be larger; passive strategies
are less attractive; active strategies may indeed be less volatile.

The rest of the paper is organised as follows. In the following subsection, we
present some examples one of which we shall develop empirically later in the
paper. In \cref{sec-model} we present the model and key definitions. In
\cref{sec-main} we present the axioms and main representation theorem. In
\cref{sec-applications} we develop example \cref{eg-zeros} by deriving a
representation result for a bond-market setting. In \cref{sec-applications} we
also discuss the implications for market efficiency identification of pricing
kernels in more detail. In \cref{sec-discussion} we provide a more detailed
comparison of our main theorem with that of \gsii\ as well as matters such as
second-order induction \citep{argenziano2019second}.

\subsection{Examples}\label{sec-examples}
\begin{example}[Supervised learning]\label{eg-supervised} For suitable nonempty
  sets $X$ and $Y$, we are given a training set $D^{\star} \subset (X\times
  Y)^{n}$  consisting of $n$ cases: each drawn from some unknown probability
  distribution $\rho$ on $X\times Y$. On the basis of $D^{\star}$, the goal of
  supervised learning is to ``learn'' a function $f_{\rho}: X \rightarrow Y$
  such that, for any given $x$ in $X$, $f_{\rho}(x)$ provides an accurate
  prediction of the associated value $y$ in $Y$. For each $x$ in $X$,
  $f_{\rho}(x)$ is the expectation $\int y \diff \rho_{x}$ where $\rho_x$ is
  the conditional measure on $x \times Y$.
  %That is,
  %$f_{D^\star}(x_{\textup{new}})$ suitably close to $y_{\textup{new}}$.
  One interpretation of the present class of model is that, for the current
  value $x = x_{n+1}$, we provide axioms that simplify the estimation of
  $\rho_x$ based on sentiments generated by $D^{\star}$: see
  \cref{sec-applications} for a pseudo-algorithm. We establish
  minimal conditions under which it is possible to consistently generalize the
  model to the collection of training sets that include the new case $z_{n+1} =
  x_{n+1}\times y_{n+1}$.
  %When \fourdiv\ holds,
  %the main theorem of \gsii\ suffices for generalization. But, as we show in
  %\cref{sec-test}, the data may well not satisfy \fourdiv.
  %The central question turns on whether $z_{n+1}$ is a novel case type. If it
  %is, then it may reveal inconsistencies that 
  %%
  %In this paper, we derive a linear kernel $v_{x}: Y \times \mbbd
  %\rightarrow \R$ on the basis of a mapping $D \mapsto \preceqb_{D}$ on
  %$\mbbd$.
\end{example}

The following examples feature two kinds of learner: one who only looks
back at past cases (within-sample, Inny), and another who also considers the
potential impact of novel types of case (the prudent, Pru). Here, and in most
of sequel, we adopt the notation of \gsii\ and let $X$ denote the nonempty set
of \emph{eventualities} with elements $x, y, z, \dots$.

\begin{example}[A setting where \fourdiv\ holds]\label{eg-tali}
  A die with unknown number of sides is ``rolled'' over and
  over again. So far, it has produced only ones, twos and threes. By resampling 
  (with replacement), we obtain the empirical distribution conditional on
  potential samples of past cases. This conditional empirical distribution
  induces a single ranking of the set $X=\{1, 2, 3\}$ of eventualities
  (outcomes) for each sample. The resulting rankings map generates the
  arrangement of hyperplanes in \cref{fig-tali}.  To each region of this
  arrangement, {sentiments} assigns a distinct total ranking.  
  %At each sample $D$ of past cases, we obtain a
  %ranking of $X$ by comparing the empirical frequencies in $D$.
  In dice-like problems, the inherent symmetry between outcomes and past cases
  ensures all $3!=6$ total rankings of $X$ arise.
 
  Our learners, Inny and {Pru}, forecast the outcome of the next roll.  Both
  reveal plausibility rankings that agree with \cref{fig-tali}. By virtue of
  \fourdiv, their ranking maps generalise to higher dimensions (four case
  types) without any need for external validation. The data is sufficiently
  rich for making predictions about $X$.
  \begin{figure}
    \centering \input{./figures/fig-tali.tex} \caption{\label{fig-tali} Each
      rational point in this simplex corresponds to a sample of past case
      types: $t_1, t_2$ and $t_3$.  The total ranking $(1, 2, 3)$ arises at $D$
      (one is least likely and three is most likely). This reflects the
      relative frequencies of past cases in $D$.  The inverse ranking $(3, 2,
      1)$ arises at $D'$. Within each hyperplane $H^{\{x,y\}}$, faces $x$ and
      $y$ are equally plausible.  The three hyperplanes form a centered
    arrangement.}
  \end{figure}
\end{example}

\begin{example}[A non-market setting where \fourdiv\ fails to hold]
  \label{eg-rates} Each month, the federal reserve announces its target
  overnight rate.  Inny and {Pru} wish to forecast the next announcement. The
  consensus is that there will be a rate rise of ${0}, {25}$ or ${50}$ basis
  points.  Inny and {Pru} have observed the same past announcements.  Current
  circumstances are such that they both find that past cases generate just
  three distinct case types $t_1$, $t_2$ and $t_3$ where the indices capture
  the (increasing) degree to which the fed is ``behind the curve''.
  %At
  %(samples containing only cases of type) $t_3$, Inny and {Pru} agree with the
  %plausibility ranking $\left({0}, {25}, {50}\right)$, so that larger rate
  %rises are more likely.  At $t_1$, the opposite ranking holds: both find that
  %smaller rate rises are more likely. At $t_2$, they agree ranking is
  %$\left(\{{0}, {50}\}, {25}\right)$, so that a ${25}$ rise is most likely, and
  %the others are equally plausible. 
 
  \Cref{fig-rates} shows some subtle differences between their current (bold)
  rankings maps.  These subtle differences turn out to be substantial once we
  extend each learner's rankings map to a hypothetical fourth case type.  To
  test for stability, the observer assigns $t_4$ the novel ranking $\left({25},
  {0}, {50}\right)$ and then extends both learners' current rankings maps to
  samples that include cases of type $t_4$.  {Pru}'s extended rankings map can
  be chosen to form a congruent arrangement with six regions. In contrast,
  Inny's extended rankings map forms an incongruent arrangement with seven
  regions.  Samples in the seventh region give rise to the intransitive ranking
  $\left({25}, {0}, {50}, {25}\right)$.
  % \Cref{fig-rates} reveals intransitivities in Inny's extended rankings map,
  % but not {Pru}'s. {Pru} satisfies ``prudence'' whereas Inny does not.
  We may not be able to observe {Pru}'s imagination (cross-validation method),
  but since incongruence is a generic property for triples of hyperplanes, we
  can prove that he is almost certainly using one.
  % This is an example of a
  % non-market setting where \threediv\ (and hence \fourdiv) fails to hold for
  % the current rankings map.
  % Yet the two arrangements
  % differ substantially and in ways that are are only revealed once a fourth
  % type of case arrives.  By prudently going beyond memory to imagine novel
  % cases, we find that {Pru}'s current rankings map {generalizes} to a congruent
  % arrangement of hyperplanes in the higher dimensional simplex.
  \begin{figure}[ht] \centering
    \begin{subfigure}{.5\textwidth} \def \congruent{1} \centering
      \input{./figures/fig-rates.tex} \caption{\label{fig-rates-prati} {Pru}'s
      rankings map}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth} \def \congruent{0} \centering
      \input{./figures/fig-rates.tex} \caption{\label{fig-rates-sempi} Inny's
      rankings map}
    \end{subfigure}
    \caption{\label{fig-rates} In contrast with \cref{fig-tali}, the current
      (bold) rankings maps features four regions and the current hyperplane
    arrangement is uncentered. }
  \end{figure}
\end{example}

\begin{example}[a bond-market setting that we develop in \cref{sec-applications}]
  \label{eg-zeros} Inny and {Pru} are now fair market makers of \emph{Zeros}
  (zero-coupon treasury bonds).\footnote{Similar to a fair insurer, a fair
  market maker sets the market spread to zero.} The compound-interest formula
  for the accumulation process of such bonds is
  \begin{linenomath*}
    \begin{equation*}
      a^{\xy} = \left(1 + r^{\{x, y\}}\right)^{-x + y},
    \end{equation*}
  \end{linenomath*}
  where $r^{\{x,y\}}$ is the yield to maturity on a forward contract that
  accrues interest between dates $x$ and $y$.
 % If $x$ is later than $y$, then
 % the contract is to sell, and the market maker pays this yield, so that
 % $r^{\{y,x\}} = r^{\{x,y\}}$. 
  Let $X \subseteq \R_{\mplus}$ index a suitable
  sequence of trading dates with $0 \in X$ being the spot date.  The
  log-accumulation process has no arbitrages provided
  \begin{linenomath*}
    \begin{equation}\label{eq-no-arbitrage}
      \text{for every distinct $x, y, z \in X$,}\quad \log a^{\xy} = \log a^{\xz}
      + \log a^{\zy} \,.\footnote{If for some $x = 0 < y < z$, Inny sets
        $a^{\xy} < a^{\xz}\cdot a^{\zy}$, then  {Pru} would do well to sell the
        spot contract $\xy$, buy the spot contract $\xz$ and buy the forward
      $\zy$. The risk-free profit is $a^\xz \cdot a^\zy - a^\xy > 0$.}
    \end{equation}
  \end{linenomath*}

  In \cref{eq-no-arbitrage}, reference to the data that drives the
  market-makers' prices is suppressed. When $\log a^\xy$ depends on past cases,
  it forms a vector that is normal (\ie\ orthogonal) to a hyperplane such as
  those that separate regions in ranking maps such as \cref{fig-tali} or
  \cref{fig-rates}.  When the data is rich enough to generate the full
  diversity rankings (as in \cref{eg-tali}), both Inny and {Pru} set prices
  that are currently arbitrage-free.  Otherwise, generically, only {Pru}
  satisfies this property. Only {Pru} sets prices according to the empirical
  (geometric) mean
  \begin{linenomath*}
    \begin{equation} B(x, D) = \prod_{c\, \in
      D}\left( 1 + r^{\{0,x\}}_{c} \right)^{-{x}/{n}}
    \end{equation}
  \end{linenomath*}
  for every date $x$ and finite sample $D$ that we can generate by resampling
  past cases.
\end{example}

\begin{example}[A multi-sectoral investment setting]\label{eg-multi-sector}
  Suppose Inny and {Pru} are potential representative agents in a
  multi-sectoral macroeconomy \citep{long1983real,atalay2017how}.
  %(See
  %\citet{gilboa2006empirical} for a related axiomatic derivation for
  %representative agents.)
  $X$ is a variety of sectors with $x_0$ being the household (\ie\ consumption
  sector).  The following no-arbitrage equations may be derived from
  first-order conditions of the social planner's constrained optimisation
  problem provided we specify an explicit functional form for the instantaneous
  flow of utility. Here we take a nonparametric approach: we avoid the
  specification of a utility function and proceed from past cases to beliefs
  and decisions.

  For every pair of sectors $x$ and $y$ and nonempty set $D$ of past cases, let
  $S^{\xy}_{D}$ denote a suitably normalised quantity of sector $x$ inputs that
  the social planner invests in order to produce good $y$, conditional on $D$.
  For every sector $z$, the no-arbitrage equations are now
  \begin{equation}
    \log S^{\xy}_{D} = \log S^{\xz}_{D} + \log S^{\zy}_{D}\,.\footnote{We can
      confirm that, for rather general model specifications, these equations hold
    in our traditional simulations with explicit utility functions.}
  \end{equation}
  Such models may be used to model regional (subnational) economic transition.
  In such settings, the data is infamously poor: the modeller typically needs
  to regionalise national accounts. If 4-diversity fails on bond-market data,
  it will surely fail here too. 
  %planner, {Pru}, has a rankings map that satisfies the above no-arbitrage
  %conditions; only {Pru} sets the required rate of return on investment (with
  %the gross return $R(x_0, \cdot)$ to consumption normalised to one) according
  %to the canonical form of an empirical mean
  %\begin{linenomath*}
  %  \begin{equation}
  %    R(x, D) = \prod_{c \,\in D}\left(1 + r^{\{x_0,x\}}_{c}\right)^{1 / \lvert
  %    D \rvert}
  %  \end{equation}
  %\end{linenomath*} for every sector $x$ and finite sample $D$ that we can
  %generate by resampling (with replacement) from past cases; 
  For sectors $x$ and samples $D$, let $R(x, D)$ denote the required return on
  investment (with $R(x_0, \cdot)$ normalized to one). Only {Pru} satisfies
  the inter-sectoral (and inter-temporal) Euler equations
  \begin{equation}
    \log S^{\xy}_{D} = \log{R(y, D)} - \log {R(x, D)}.
  \end{equation}
\end{example}


%\begin{example}[Choosing factors]\label{eg-factors}
%  In \cref{eg-tali,eg-rates,eg-zeros,eg-multi-sector}, $X$ respectively
%  consists of a set of indices for faces of a tali, rate movements, maturity
%  dates and sectors. A different, causal interpretation is possible if we let
%  each $x \in X$ denote distinct combination of risk factors such as the market
%  factor, firm size and book-to-market equity \citep{fama2015five}. Thus each
%  $x$ forms a model with which to forecast stock or bond market returns
%  \citep{fama2015five,harvey2021lucky,gu2020empirical}.  Such an interpretation
%  brings our notation closer to the standard form that one finds in the
%  statistical learning literature
%  \citep{cucker2001mathematical,poggio2003mathematics,mukherjee2006learning}.
%
%
%  Let $(Y, \leq^*)$ denote a suitably ordered criterion space for evaluating
%  models.\footnote{We might induce $\leq^*$ via a norm on $Y$ so that $y \leq^*
%  y'$ if, and only if, for the true value $y^*$, $\lVert y - y^*\rVert \leq
%  \lVert y' - y^*\rVert$.}  The statement $x \preceq_D x'$ is interpreted as
%  ``At $D$, $x'$ is a more plausible model than $x$''. The learner subscribes
%  to this view if, and only if, the function $f_D: X \rightarrow Y$, $f_D(x')
%  \leq^* f_D(x)$.
%
%  Our goal is to derive a stable learning map $D \mapsto f_D$ on the basis of
%  {sentiments} $D \mapsto \preceqb_D$ where $D$ is any finite sample of past
%  cases.\footnote{The term learning map is introduced in
%  \citet{mukherjee2006learning}. There stability is defined in terms of continuity
%  of the learning map, something that we prove in \cref{prop-stability},
%  below.}
%\end{example}
%
%
%
%
%
%
%
%
%In market settings, agents with intransitive rankings do not survive the
% exploitation by others who take advantage of risk-free arbitrage
% opportunities.  But what about an agent that presents transitive rankings
% given any resampling of past cases? One might well expect that such an agent
% would be safe from exploitation.  It turns out that, when resampling of past
% cases fails to generate a sufficiently diverse collection of (transitive)
% rankings, purely empirical agents (who only consider past cases), will
% generically find themselves offering free lunches to others.  This is where
% an imagination should come in handy. 
%% But how is it to be used?  Moreover, how can we provide
%%a model with testable implications given that the imagination is unobservable?
%
% We aim to provide a testable, axiomatic foundation
% for such behavior. We demonstrate the model's efficacy through applications
% to no-arbitrage asset pricing.  The axiomatic framework for case-based
% prediction and inductive inference of \citet[henceforth
% \gsii]{gilboa2003inductive} is our starting point. 
% 
%%By compensating for inexperience or insufficiently rich data,
%%prudent use of the imagination to conjure up novel types of case can ensure an
%%agent's survival and collectively lead to market efficiency.
%%De Finetti's Dutch book arguments for the
%%formula of conditional probabilities provide a hint that this need not be
%%true.
%%
%%In this paper, we provide new, formal insights into the
%%this nexus between intransitive rankings, no-arbitrage asset pricing and the
%%use of the imagination to assist with prediction when resampling the data
%%fails
%%to generate a diversity of rankings. 
%% 
% 
%
%The original idea from which the present paper evolved is a criticism of
%\gsii.
%In practice, resampling past cases will probably fail to always generate a
%full
%\fourdiv\ of rankings. (This holds when every subset of four eventualities
%generates $4!=24$ distinct total rankings.) \fourdiv\ is likely to fail when
%the market maker is inexperienced or when the available data is insufficiently
%rich \emph{for the present prediction problem}.  (\gsii\ allow for the
%possibility that the same data may be rich for some prediction problems and
%yet
%poor in others by developing an endogenous notion case types.)
%
%We weaken \fourdiv\ to partial \threediv\ (every subset of three eventualities
%generates (at least) three distinct total rankings).  This is the weakest
%condition for which there exists a suitably unique functional representation
%of
%rankings conditional upon resamplings of past data of the same form as \gsii.
%But alone this weaker form of diversity will not do.  Market makers that
%satisfy the other (reasonable axioms) of \gsii\ will find themselves
%offering free-lunches to other market participants, unless they are prudent
%enough to use their imagination \emph{when past cases fall short of \fourdiv}.
%In such settings, prudent market makers peer into the future and rank
%eventualities conditional, not only upon past cases, but also upon imagined,
%novel types of cases.\footnote{Case types are the dimensions of the model just
%as future states are for Bayesian models.}
%%
%%The reason is
%%that they only consider past cases. Market makers that use their imagination
%%to
%%explore (unobservable) novel case types find themselves at an advantage.
%
%Market makers gather (traditionally in trading pits) and provide liquidity by
%filling buy and sell orders that brokers bring to market.  In over-the-counter
%bond markets, market makers buy and sell forwards across different horizons.
%Predictions are made on-the-fly and split-second decisions become contracts
%with implications for the months ahead.  A market maker with openly
%intransitive rankings over existing securities is unlikely to survive the
%inevitable exploitation by others.  But what about intransitivities that
%surface only once new information arrives? That is, what if intransitivities
%are uncovered by novel case types (in a higher dimensional generalization of
%the
%market maker's current model)?  Eradicating such intransitivities \emph{ex
%post} requires a re-evaluation or pricing of the informational content of past
%cases and their role in determining current yields.  A market pricing process
%of this form would be inefficient. 
%
%A key insight of the present paper is that potential intransitivities generate
%arbitrage opportunities even before the arrival of new information. This
%insight has a number of implications. First,
%this opens up a channel for market activity to weed out such
%intransitivities \emph{even in the absence of market makers that prudently use
%their imagination}. 
%Second, market makers such as Inny will not survive. Other agents will
%exploit their lack of imagination today.
%Third, a market maker who avoids using their imagination may
%appear prudent (in the sense of using their imagination) provided they are
%careful enough to set prices that are free of arbitrages.
%
%In our model, no-arbitrage pricing is equivalent to prudent pricing.  That is,
%equivalent to the existence of a suitably unique market pricing function
%(conditional on any resampling of past cases) that might belong to a market
%maker that is prudent.  That is to a market maker whose model {generalizes} to
%higher
%dimensions (novel case types) without generating intransivities or other
%violations of the basic axioms of \gsii.  This invisible hand/``Deus ex
%machina'' style result is similar in spirit, yet quite different from notions
%of market efficiency that are surveyed in
%\citet{fama1970efficient,malkiel2003efficient}. Those notions of
%market efficiency are expressed in terms of a ``fair game'' or expected
%return.
%Markets are efficient if the expected return of a portfolio is no higher than
%that which is possible with a buy-and-hold strategy. Or perhaps that markets
%are efficient whenever price increments follow a random walk.  These notions
%are somewhat more forward-looking than ours. Here, markets are efficient when
%the arrival of new information does not cause a re-evaluation of past cases.
%This is indeed efficiency in an operational sense because it ensures
%modularity
%of the market model: similarity weights that have been generated in the past
%remain unchanged when new information arrives.
%
%We would however expect to find differences in repeated experiments. Whether
%in
%the field or the lab, we would expect that the actual arrival of novel case
%types would have a different impact on Inny than on {Pru}.
%
%
%%
%%This demonstrates that shape associations and beliefs even in the absence of
%%market makers that are endowed with a rich imagination and that are prudent
%%enough to explore the impact of new information.  In other words, to borrow an
%%analogy with machine learning by eradicating current arbitrage opportunities,
%%market makers inadvertently make markets efficient. 
%
%% 
%%Intransitivities in preferences are known to open
%%the door to arbitrage opportunities.  Our main result is less obvious: in
%%information-poor situations, prudently avoiding future intransitivities can
%%ensure a no-arbitrage pricing strategy and hence survival \emph{today}. That
%%is, within the current prediction problem and before any novel case types
%%arrive.
%
%\emph{Robustness of these examples} On the one hand, there are far fewer
%maps of current rankings that are symmetric (in the sense that the
%intersection
%of hyperplanes occurs inside the simplex as in \cref{fig-tali}) than there are
%asymmetric (in the sense of \cref{fig-rates}). Simply consider the fact that
%the simplex of \cref{fig-tali} is of measure zero in the plane.
%% On the other,
%%once the prediction problem consists of four or more eventualities (members of
%%$X$), symmetric rankings maps may also expose arbitrage opportunities.
%
%\emph{A robust test of prudent behavior} In an experimental setting, if
%one finds that a subject sets prices that are arbitrage free, then the current
%rankings map generates congruent hyperplanes as is the case for {Pru} in
%\cref{fig-rates-prati}. Recall that three hyperplanes are said to be in
%general (\ie\ generic) position if they are incongruent. There is therefore a
%zero
%probability that a type-{Pru} subject is prudent by chance. 
%
%\emph{Machine learning application I} Consider a machine that is trained
%to set prices on the basis of past data. We would like to know whether the
%machine is capable of learning to avoid providing arbitrage opportunities
%without explicitly encoding the no-arbitrage condition in its objective. To do
%so it will have to be intelligent enough to ``imagine'' a future that is
%different to the present and perform well out-of-sample.
%
%\emph{Machine learning application II} Consider a machine that is trained
%to set prices on the basis of past data, only this time the no-arbitrage
%condition is explicitly encoded into its objective. Our results show that,
%regardless of what the future throws at the machine, the current rankings map
%will not need to be suddenly reconfigured to correct errors made today. This
%may substantially reduce the cost of checking generalizations of the current
%rankings map.
%
%
%%\begin{example}\label{eg-full-model}
%%
%% A canonical large-world setting is that of the global financial markets.  In
%% 2019, all models ommited details of COVID-19. Similarly, in 2007, a clear
%% description of the sub-prime mortgage crisis was beyond reach.  The central
%% role of simulation and bootstrap methods in empirical finance points to a
%% prevalence of inductive reasoning.\footnote{Consider
%% \citet{cowles1944stock}, \citet{white2000reality},
%% \citet{fama2015five} and \citet{harvey2021lucky}.} That is, to
%% forecasting on the basis of past cases as opposed to a full description of
%% future states. At the same time, market makers need to set prices that are
%% robust to changes that open the door to exploitation via arbitrage.
%%
%%\end{example} 
%%
%% This basic distinction between goes to the heart of the present paper. Our
%% notion of cases captures this distinction and
%% 
%%
%%Whilst imagining novel cases is easy to do in \cref{eg-tali}, in other settings
%%it will be hard.  Moreover, many ``real world'' settings feature much less
%%symmetry than the outcomes of a die (or even a tali). Asymmetry is where
%%intransitivities are more likely to arise.
%%
%%\pagenumbering{arabic} \setcounter{page}{2} Incomplete models are a key
%%motivation for recent axiomatic updates to the standard Bayesian framework such
%%as ``reverse Bayesianism'' of \citet{KV_Reverse_Bayes}. This and other work
%%(\citet{KV-Awareness_of_U}, \citet{HR-Knowledge_of_U} and
%%\citet{GKMQT_Robust_experiments}) on unawareness and robustness in the
%%state-space sense provide part of our inspiration for the present upgrade to
%%the axiomatic foundations of inductive inference in \citet[henceforth
%%\gsii]{gilboa2003inductive}. By taking rankings as primitive, we extend
%%\gsii\ to accommodate a forward-looking version of the second-order induction
%%of \citet{argenziano2019second}. 
%
%In the present framework, the basic building blocks of the model are
%observations or (synonymously) past cases. A given past case may be empirical
%or theoretical, and the learner's model is naturally bounded in size and
%scope by the learner's experience. Our contribution is to extend the
%framework of \gsii\ to model less experienced learners that are prudent. That
%is to agents that are able to proactively engage in second-order induction: by
%``pulling themselves up by the bootstraps'' and looking at how their model
%{generalizes} to novel cases. Thus allowing them to survive their initial
%phase of
%inexperience.
%
%In the remainder of this section, we informally introduce: the model of
%\cref{sec-model}; the axioms and matrix representation of
%\cref{sec-axioms-theorem}; and the applications to which we return in
%\cref{sec-discussion}. Proofs of main results appear in appendices
%\ref{sec-proof-main} to \ref{sec-proof-foureq}.
%
%
%
%The learner is endowed with a
%qualitative plausibility ranking of eventualities given her current database
%$D^{\star}$ of past cases. Moreover, the same is true for every finite
%resampling of $D^{\star}$. We identify conditions on the resulting family of
%(ordinal) rankings for the existence of a suitably unique
%real-valued matrix $\mathbf v$ on eventualities$\times$cases that
%\emph{represents} the
%information in these rankings. The form of this representation is linear on
%databases (\ie\ additive over cases) and separable on eventualities, so that
%for
%every database $D$, eventuality $y$ is
%more likely than $x$ if, and only if,
%\begin{linenomath*}
% \begin{equation}\label{eq-similarity}
% \sum_{c\,\in D} \mathbf v(x,c) \leq \sum_{c\,\in D} \mathbf v(y,c).
%\end{equation}
%\end{linenomath*}
%The similarity weight $\mathbf{v}(x,c)$ is the degree of support that case $c$
%lends to $x$.
%\begin{example}As a canonical example, consider predicting the slope
%coefficient
% $\beta_{i}$ from the regression of asset $i$'s returns on the market
% portfolio
% \citep[in the two-pass method of][]{FM-Two_pass}. Then values $\beta_{i}$ are
% eventualities and $D^{*}$ is the current sample of past returns. In this
% setting, $\mathbf v$ is an empirical log-likelihood function that we generate
% via a generalised notion of bootstrapping of cases in $D^{\star}$.
% % This empirical likelihood is robust to the car primitive rankings via an
% initial round of least-squares
% % maximisations, but we only retain qualitative ordinal information $\mathbf
% v$
% % is robust to the final is additive in the sample, maximum-likelihood
% % estimation conforms to maximising the the current class of prediction
% % rules. But the current . This can be seen by observing that the log Least
% % squares to generate a ranking given each resampling $D$; we retain only the
% % ordinal information objective function of a least squares minimisation
% % provides an ordinal ranking for every resampling of past observations. The
%\end{example}
%Key to the contribution of \gsii\ is an endogenous notion of case types: a
%partition of cases according to the marginal information they contribute to a
%given database. This marginal contribution is measured in terms of the impact
%on
%the rankings of eventualities.\footnote{This notion of case type is therefore
% close to Quine's ``perceptual similarity'' (see \cref{sec-discussion}).}
% Case
%types are analogous to states in the sense that they form the model's
%dimensions: the lower the dimension the less the experience.
%
%\begin{example}[Search Engine Results Page, SERP]\label{eg-search_engine}
% Advertising aside, when users conduct a web search, the search engine
% compiles
% a ranking $\preceqb_{D^{\star}}$ of (web)pages $x, y, z, \dots$ on the basis
% of its database $D^{\star}$ of past cases: searches of past users plus
% feedback from subsequent clicks.  Resampling yields other databases $D$ and
% other rankings.  At one extreme, past cases may be so similar to one another
% that the same plausibility ranking arises regardless of how the data is
% resampled.  At the other, past cases may be sufficiently rich that resampling
% yields every feasible plausibility ranking of eventualities.
%\end{example}
%
%A rich set of past cases is at the heart of the \fourdiv\ axiom of \gsii. This
%restricts the model to learners whose current data is sufficiently rich that
%a resampling exercise generates \emph{all $4!=24$ strict (\ie\ total) rankings
%of every subset of four eventualities}. In this paper, we accommodate less
%inexperienced learners by only replacing \fourdiv\ with \condtwodiv. Given
%the other basic axioms of \gsii, \condtwodiv\ turns out to be equivalent to
%requiring that, for every three distinct eventualities $x$, $y$ and $z$,
%resampling generates at least $3$ of the $3! = 6$ possible distinct strict
%rankings of this triple. \Condtwodiv\ is minimal in the sense that, in its
%absence, we lose both existence and uniqueness of the similarity
%representation
%(see \cref{eg-lexicographic}).  
%
%To compensate for a lack of experience, we introduce a subtle and more
%flexible
%notion of cases that allows us to capture the learner's potential awareness
%of her limited experience.  Formally, related notions in the literature go by
%the name of unforeseen consequences in \citet{GQ-Surprises} and shadow
%propositions in the setting dynamic awareness in \citet{HP-Dynamic_awareness}.
%Via a content-free case $\novel$, the learner can explore the impact on her
%model of the arrival of a novel case type. In effect, this involves
%meta-analysis of how her similarity function will evolve over time. Hence the
%reference to second-order induction.
%
%The prudent learner ensures her model is robust to the arrival of novel case
%types. She ensures that, when a novel case arrives, she can accept the ranking
%it generates without finding herself in the potentially costly position of
%generating intransitive rankings when she combines past and novel cases.
%
%\begin{example}[Second-order inductive inference]\label{eg-second-order}
% Consider a search-engine startup seeking to establish itself in the face of
% incumbents with the experience of Google. The start-up engages in second
% order induction when it is learning the similarity function itself. This may
% include learning the values $\mathbf{v}(x,c)$ of \cref{eq-similarity}, but it
% may also involve costly updates of the model structure ``on the fly'', \eg\
% redefining case types, rankings, \etc. The startup is prudent if, \emph{ex
% ante}, it structures its model to ensure that it is relatively costless to
% extend to novel case types$:$ once they arrive.
%\end{example}
%
%Prudence is only worthwhile when revisions of the learner's model `on the
%fly',
%once the novel case arrives, is costly.  % Yet, in many settings,
%% revising a model on the fly is indeed costly. And in some settings, cases are
%% by
%% definition novel.
%Consider ``zero-day attacks'' in the setting of cyber security.
%\Citet{Hota_et_al-Cyber_security} highlight the essence of time when a novel
%attack on a computer network arrives; and that such attacks are novel
%precisely
%because cyber-security experts have already built in solutions to known
%vulnerabilities. Also, tradeoffs between time, cost and learning are nowhere
%more important than in finance. For a bond-market setting, we are able to
%provide a formal equivalence between {stability} and arbitrage pricing in
%\cref{sec-discussion}.  In \cref{sec-discussion}, we also discuss: other
%applications; empirical evidence linking intransitivity, memories and novelty;
%and connections with the literature on second-order induction in more detail.
%
%
%
%% The exposition of the model appear in \cref{sec-model}, the axioms and main
%% theorem then follow in \cref{sec-axioms-theorem}. Proofs appear in
%% \cref{sec-proof-main} to \cref{sec-proof-foureq}.
%
%
%
%
%
%
%
%
% % \begin{example}\label{eg-Black_swan} Before the first European observation
% of
%% black swans in modern day Australia in 1697, the typical European zoologist's
%% dataset would justify the plausibility ranking ``On his expedition to New
%% Holland, it is more plausible that Willem de Vlamingh will observe white
%% swans than black ones.'' It is only with a sufficiently rich knowledge of
%% the migratory behavior of Swans, a rich dataset documenting their absence in
%% tropical regions, and a phylogenetic theory of Swan evolution that a
%% zoologist could convincingly propose the reverse plausibility ranking.  The
%% fact that de Vlamingh's observation had the impact on zoology that it did
%% reveals that few if any zoologists were capable of such a deep and convincing
%% hypothesis.  Nonetheless, a prudent zoologist might well anticipate that,
%% since expeditions typically yield new data, it would be wise to explore the
%% potential impact of a novel case on her forecast. For instance, is it feasible
%% that resampling some combination of past data with novel case will generate a
%% forecast with intransitive rankings?
%% \end{example}
%
%
%
%
%
%% %Yet, in the model of \gsii, this is the kind of ability the forecaster must
%% %have.
%
%% We shall model a novel case as a variable with unknown domain and unknown range.
%% This is because cases are defined independently of the rankings that they
%% determine.  An alternative approach would be to take the rankings, given each
%% database as primitive
%
%% Although the learner can only observe past cases, she might also observe that
%% resampling them does not yield a forecast with a diverse family of plausibility
%% rankings.  And, in turn, she may observe that, if the present prediction problem
%% turns out to be unlike past cases, her future forecast may well be more diverse
%% than her current one.  She may then ask whether she will need to revise her
%% current forecast in order to accommodate a forecast generated by richer
%% databases?  In the present paper, we show that this line of observation and
%% questioning naturally leads to the following conclusion: if the inexperienced
%% learner restricts her resampling procedure to past cases, then she may be
%% omitting important information that is concealed by her lack of experience.  At
%% least, that is, if she accepts the basic axioms for case-based decision theory
%% (and prediction in particular).
%
%% The basic axioms are as follows.  The first and most basic is that, conditional
%% upon any feasible database of past cases, the plausibility ranking is both
%% transitive and complete.  The second axiom, Combination, is the requirement that
%% whenever the plausibility rankings at two disjoint databases coincide, then the
%% combined database (formed by taking the union) generates the same plausibility
%% ranking.  When the number of eventualities is finite, the third axiom,
%% Archimedeanity, requires that for any pair of disjoint databases, the
%% information associated with the first is eventually swamped by the combining it
%% with sufficiently many copies of the second.
%
%% The formal question we shall ask is the following: for a learner with a
%% current forecast that satisfies the basic rationality axioms of \gsii, will
%% these axioms apply to any generalization of her current forecast that is formed by
%% considering databases involving resampled copies of a novel case?  If not, then
%% the omitted information that we refer to in the penultimate paragraph above may
%% bias her forecast in such a way that intransitive plausibility rankings arise in
%% every generalization of her current forecast that satisfies Completeness (at each
%% database), Combination and Archimedeanity.  This is essence of the Stability
%% axiom that we appeal to instead of the Diversity axiom of \gsii.
%
%
% % The consequence of this bias (\ie~a violation of the Stability axiom) is to
% % preclude the representation of the forecast by a real-valued suitably
% % additive weighting function of the form
%% \begin{equation*}\text{$v:$ eventualities $\times$ past cases $\rightarrow \R$.}\end{equation*}
%% That is a matrix $v$ such that for any pair of eventualities $x$ and $x'$ and
%% any database of resampled past cases $D$,
%% \text{$x\preceqb_D x'$\quad if, and only if,\quad $\sum_{c\,\in D} v(x,c)\leq \sum_{c\,\in D} v(x',c)$},
%% where $\preceqb_D$ is the plausibility ranking that is determined by
%% $D$. (For instance, in a typical empirical setting, $\preceqb_D$ might be the
%% ordinal content of the empirical likelihood conditional on $D$.)
%
%
%% When states, events and probabilities summarise the learner's view of the
%% world and its impact on her predictions, each state specifies a distinct
%% dimension of the model and the probability distribution assigns a weight to each
%% dimension.  A key feature of \gsii, where cases are primitive, is that the
%% dimensions are endogenous to the problem at hand.  The idea is that, for one
%% prediction problem, two cases may may be so similar as to considered as
%% equivalent to one another, or of the same \emph{case type}.  For another
%% prediction problem, the same two cases may no longer be equivalent, each giving
%% rise to different weights or indeed different plausibility rankings.  It is the
%% set of case types that determines the notion of dimension in the present model.
%
%% Our first conceptual generalization of \gsii~is to consider case types as one measure
%% of experience.  For instance, in \cref{eg-Search_engine}, the (extremely)
%% inexperienced engine has a forecast of dimension one, whereas the experienced
%% engine has a forecast that is of dimension at least $n!$, where $n$ is the
%% number of plausible webpages.  The number of case types is always at least as
%% large as the number of rankings of eventualities %The scenarios we describe in
%% \cref{eg-Search_engine} support this interpretation: the search engine with one
%% case type is, in effect, inexperienced and any resampling of past cases will not
%% change this fact.  %It is clear that, the richer the data the search engine has
%% access to, the more diverse its conditional predictions will be.  %Conditional,
%% that is, upon a given database that arises from resampling or subsampling.  Yet
%% one forecast is more diverse than another if it contains a greater number of
%% distinct plausibility rankings.  Thus, the diversity of a forecast provides
%% another measure the learner's experience which is distinct from, but closely
%% related to, case types.  %The essential distinction is that the diversity of a
%% forecast is a lower bound on the number of case types.
%
%% The main contribution of the present work is to allow for forecasts that are
%% nondiverse in the sense that they do not satisfy the diversity axiom of \gsii.
%% The latter axiom requires that, for every strict ordering of four eventualities,
%% there exists a database, conditional upon which, the learner's ranking over
%% all eventualities contains that ordering.  In \gsii, as well as in the closely
%% related models of \citet{GS_Act_similarity,GS_CBDT_Book}, the diversity axiom is
%% presented as a purely technical requirement for the desired representation.
%% When diversity fails to hold, the remaining axioms do not suffice for a matrix
%% representation that is separable across eventualities.  Our answer to this
%% fundamental difficulty (which also arises in other settings such as
%% \citet{Azrieli_Valence_dimension}), is to ask that the learner is forward
%% looking.  Our learner is forward looking if her behavior can be explained by
%% adding a ``novel case'' to the model and studying the ``potential generalizations''
%% of her forecast.
% %% We conclude the introduction with a brief discussion these concepts and
% the main
%% axiom to which they give rise.  In contrast with a standard case, which
%% determines a plausibility ranking, we define a \emph{novel case} to be one that
%% can be associated with any plausibility ranking.  %Indeed, it defines a free
%% variable on the set of feasible plausibility rankings.  Since a novel case is
%% intended to capture cases that lie beyond the experience of the learner, its
%% weight in a given a database should be indeterminate.  As such, the plausibility
%% ranking associated with any database that contains a copy of the novel case is
%% also indeterminate.
%
%% By assigning a plausibility ranking to this case and to its affiliated
%% databases, we obtain a \emph{(potential) generalization} of the (current) forecast.
%% It will suffice to study the generalizations that involve three or four
%% eventualities.  Two generalizations will be of the same type if they feature the same
%% degree of diversity or experience.  Our main axiom requires that every generalization
%% that satisfies all the necessary axioms of \gsii~(with the possible exception of
%% transitivity), has a modification that is of the same type and that also
%% satisfies transitivity.
%
%
%% The role of our main axiom, which we refer to as \emph{{stability}} is to exclude
%% the forecasts with generalizations that feature essential intransitivities. That is,
%% our learner fails to be prudent if she (behaves as if) she fails to foresee
%% that, should a certain case arise, the only way she can satisfy the \gsii~axioms
%% avoid specifying an intransitive plausibility ranking at some database is to
%% retrospectively change her current forecast or by being dogmatic and ruling out
%% certain plausibility rankings regardless of the data.  To borrow from our
%% epigraph, by being imprudent the learner may spoil her future action.
%
%% % The proof of our main theorem yields a technical condition that is equivalent
%% % to {stability} when there are at least two case types.
%% % This condition, that may be easier to verify in certain settings.
%
%
%% Through examples, we will highlight the ways in which {stability} is strictly
%% weaker than imposing the diversity axiom on the forecast.  We will also show
%% that {stability} is weaker than potential diversity (the forecast admits an
%% generalization that satisfies the diversity condition).
%
%% A casual comparison of our representation with that of \gsii~leads to the same
%% conclusion.
%%In essence, of the learners with an additive the only learners we
%%The representation of \gsii~characterises the experienced learner via a
%%matrix that is diversified.
%
%Hume establishes two basic principles. The first is that all \emph{ideas
%proceed mediately or immediately from their correspondent impressions}.
%Impressions are experiences such as sensations, passions and emotions. Ideas
%are faint images in thinking and reasoning. Before establishing his second
%principle he notes that there are two basic forms of idea, the first is a
%memory which ``retains to a considerable degree its vivacity and is between an
%impression and an idea''.  It is the faculty by which we repeat our
%impressions.  The second form of idea is the imagination where perception is
%faint and languid.
%\begin{quote} \dots yet the imagination is not restrain'd to the
% same order and form with the original impressions; while the memory is in a
% manner ty'd down in that respect, without any power of variation.
%\end{quote}
%The second principle concerns the imagination: \emph{of the liberty of the
%imagination to transpose and change its ideas.} In this way, both memories and
%our imagination form our experiences.  But how should we model this
%distinction? How should a learner use his limited constrained memories or the
%liberty of her imagination?
%%model
%\emph{The primitives of our model}%\label{sec-primitives}
% consist of the nonempty sets $X$ and $\current$.
%For the current prediction problem, we interpret
%members of $X$ as \emph{eventualities} and members of $\current$ as
%\emph{current} cases.  Our first substantive departure from \gsii\ is to allow
%for two forms of current case.  That is we take $\current$ to be the union
%$\past \cup \{\novel\}$ of \emph{past} cases $\past$ and a \emph{free} case
%$\novel$.\footnote{
%We assume $\novel$ to be the only free case $\current$, not
%because we think learners are constrained in this way, but because one degree
%of freedom is sufficient for our purposes.  One may view the matter from an
%evolutionary perspective: {Pru} represents a minimal deviation from Inny.}
%As
%per Hume's distinction (second paragraph of our introduction), past cases are
%``tied down, original impressions'' \ie\ constant (of arity zero); and
%$\novel$
%is ``unrestrained'', and, for now, a variable (of positive arity).

%-----------------------------------------------------------------------------%
\section{Model}\label{sec-model}
%-----------------------------------------------------------------------------%
\emph{The primitives} consist of the nonempty sets $X$ and
$\mbbcp$.  For the current prediction problem, we interpret members of $X$ as
\emph{eventualities} and members of $\mbbcp$ as \emph{current} cases. We depart
from \gsii\ by allowing for two forms of current case: $\mbbcp$ denotes the
union $\mbbc \cup [\novel]$ of a set $\mbbc$ of constant, \emph{past} cases and
a set $[\novel]$ of copies of the variable, \emph{free} case $\novel$.
\begin{remark*}
  %Recalling Hume's distinction (second paragraph of our introduction), past
  %cases are indeed ``tied down, original impressions''. Moreover, the
  %imagination is indeed free to ``transpose and change its ideas'' via copies
  %of $\novel$, all of which are free of content and, in this sense, identical.
  By way of analogy with computer memory, a natural implementation is as
  follows.  Take any case $c \in \mbbcp$ to consist of a pair $p \times m:$ a
  pointer $p$ that references a memory location and the memory content $m$. For
  $c\in \mbbc$, just as in the setting of \gsii, content of $m$ is meaningful. 
  For $c \in [\novel]$, although $m$ is empty (assigned a ``garbage'' value),
  the allocation is itself valuable. Pairs $c, d\in [\novel]$ are
  indistinguishable in terms of content and are in this sense copies of
  $\novel$. One is free to assign novel, imagined content to $\novel$.
  %and before
  %going on to explore the prediction problem with this content in mind.  As we
  %will show, an observer (or more sophisticated learner) who observes past
  %cases, but not the imagination, can nonetheless use $\novel$ to analyse the
  %learners' behaviour.
\end{remark*}
%Our choice of $\current$ as primitive ensures the learner's model is
%``pointed''. That is to say $\current$ is the current data. The learner is
%nonetheless endowed 
%Not all cases in $\current$ 
%By resampling (with replacement) cases in $\current$, we obtain a set
%$\mbbcp$. 
% \emph{Databases} %

With case \emph{resampling} from the literature on bootstrapping in mind, let
\begin{equation*}
  \mbbd\defeq \lbc D\subseteq \mbbc: \countof D < \infty \rbc
\end{equation*} denote the set of (finite) \emph{determinate or constant
databases} or memories and let $\mbbdp$ denote the set of all
finite subsets of $\mbbcp$. $\cpp$ will denote a member of $\{\mbbc, \mbbcp\}$.
Similarly, 
\begin{linenomath*} \begin{equation*} \text{$\dpp$} = \left\{\begin{array}{ll}
    \mbbd & \text{if, and only if, $\cpp = \mbbc$, and}\\ \mbbdp &
\text{otherwise.} \end{array}\right.  \end{equation*}
\end{linenomath*}

\emph{Sentiments.} We now take a first step towards formalising the ranking
maps of \cref{fig-tali,fig-rates}.  (The translation from databases to
rational vectors appears in \cref{sec-proof-main}.) For each $D$ in $\mbbd$, the
learner is endowed with a well-defined plausibility ranking $\preceqb_D$ in the
set $\relations(X)$ of binary relations on $X$.  Denote the symmetric and
asymmetric parts of $\preceqb_D$ by $\simeq _D$ and $\precb_D$ respectively. 
\emph{Sentiments} $D \mapsto \preceqb_{D}$ are thus a vector in
$\relations(X)^{\mbbd}$ of the form
\begin{linenomath*}
  \begin{equation*}\preceqb_{\mbbd} \defeq \langle\preceqb_{D} : D\in
  \mbbd\rangle\,.\end{equation*}
\end{linenomath*}
This subtle departure from the form $\{\preceqb_{D} : D \in \mbbd\}$ of \gsii\
is closer in structure to visual representations (\cref{fig-tali,fig-rates})
and the graph $\{D \times \preceqb_{D} : D \in \mbbd\}$.\footnote{I thank
Maxwell B.  Stinchcombe for bringing this point to my attention.}

%For any given $D$, the ranking $\preceq_D$ on $X$ is induced by the
%anticipated
%ranking of eventualities $Y$. That is, let $X = \times_i^m X_i$ and let $Y =
%\R^m$, then, for some function $f_D:X \rightarrow Y$, if $x\in X$ and $y =
%f_D(x)$ for some $D \in \mbbd$, then for $0 \leq k, l \leq m+1$, $x_k
%\preceq_D
%x_l $ if, and only if, it is more plausible that security $l$ will yield a
%higher return than security $k$ (\ie\ $y_k < y_l$).  Under this
%interpretation,
%$x_k$ is the data point generated through current research into security $k$.
%When the function $f: X \times \mbbd \rightarrow Y$ is unknown, the learner
%exploits information in the ranking map to estimate $f$ and in turn use the
%estimated function to generate a prediction of the vector $(y_0, \dots,
%y_{m+1})$. This
%information is revealed by varying the sample and generating new rankings of
%$X$ (and, implicitly, new rankings of the set $S$ of securities and $Y$).
%The {sentiments} are the mathematical object to which the axioms apply. It
%corresponds to what \citet{mukherjee2006learning} refer to as the learning
%algorithm which is a mapping $D \mapsto \mc H$. Where 

%For each $C$ in $\mbbdp \bs \mbbd$, the fact that for some $c \in [ \novel ]$,
%$c \in C$ means that $\preceqb_{C}$ is indeterminate, free variable in
%$\relations(X)$.
%\begin{remark}
%Although, in isolation each such $\preceqb_{C}$ is free, when the axioms we
%  introduce hold, the potential values of the variable $\preceqb _ \mbbdp
%  \defeq \langle \preceqb _ C : C \in \mbbdp \rangle$ are constrained by the
%  current values of the constant $\preceqb_\mbbd$.
%\end{remark}

% \begin{remark}

%   The fact that the empty database $\emptyset$ is absent from
% $\mbbd$ means that $\preceqb_\emptyset$ is undefined (just as
% $\preceqb_D$ is not defined when $D$ is infinite). We leave
% $\preceqb_\emptyset$ undefined since our axioms will accommodate the
% possibility that $x\precb_D y$ for every $D\in \mbbd$. Indeed the situation
% where our forecaster is without data and without a free  case would only seem to
% be justified when she is entirely unaware of the prediction problem. But in that
% case, we find it unreasonable to suppose that $\preceqb _ \emptyset$ is
% complete and transitive on $X$.
% \end{remark}

% At the same time, the learner that is aware of the prediction problem may
% find it useful to contemplate what she would do if she had
%Instead, we   accommodate this hypothetical scenario and simultaneously
%simplify our exposition by including $\emptyset$ in the set of generalizations of
%$\mbbd$, which we introduce shortly.



\emph{Case types.} As in \gsii, two past cases $c , d \in \mbbc$ are of the
same \emph{case type} if, and only if, the marginal information of
$c$ is everywhere equal to the marginal information of $d$. Formally, $c \sim
^{\star} d$ if, and only if, for every $D \in \mbbd$ such that $c , d \notin
D$, $\preceqb _ {D \cup \{c \}} = \preceqb _ {D \cup \{d \}}$.  By
observation 1 of \gsii, $\sim^{\star}$ is an equivalence relation on
$\mbbc$. The equivalence classes of $\sim^\star$ generate a partition $\mbbt$
of case types.  We extend $\sim^{\star}$ to $\mbbcp$ by taking $[ \novel ]$ to
be an equivalence class of its own, so that, for every $c \in \mbbc$, $c
\nsim^{ \star} \novel$.  We let $\mbbtp$ denote the corresponding partition of
$\mbbcp$.  Like \gsii, we also extend $\sim^{\star}$ to $\mbbdp$ by treating
databases that contain the same number of each case type as equivalent. That
is, $C \sim^{\star} D$ if, and only if, for every $t \in \mbbtp$, the numbers
$\countof (C \cap t)$ and $\countof (D \cap t)$ of that case type coincide. To
enable a surjection $D \mapsto \langle\countof (D \cap t): t \in \mbbtp
\rangle$ from databases to counting vectors, we impose
% The set $\mbbt$ of case types in $\mbbc$ consists of the
% partition generated by $\sim^{\star}$ on $\mbbc$.
% We then denote the extended set
% of case types by $\mbbtp$.
\begin{assumption*}
  For every  $t \in \mbbtp$, there are infinitely many cases in $t$.
  %There are infinitely many cases in $[ \novel ]$ and in each equivalence class
  %of $\sim^\star$ in $\mbbc$.
  %and  there are at least two case
  % types.
\end{assumption*}

% For each $D \in \mbbdp$, let
% $t \mapsto I _{D} ( t ) = \countof ( D \cap t )$ denote the vector that
% counts the number of cases each case type in $D$.  As in \gsii, take
% $C, D \in \mbbdp$ to be \emph{equivalent}, written $C\sim^\star D$,
% if, and only if, $I _{C} = I _{D}$.

\emph{Generalizations} of sentiments $\precsimb_\mbbd$ to $\mbbdp$ require a
suitable structure. The following definition provides that structure. It also
simplifies the exposition by accommodating generalizations that restrict
attention to subsets $Y$ of $X$.
\begin{definition}\label{def-generalization} $\extb \defeq \langle \extb_{D}:
  D\in \dpp \rangle$ is a \emph{generalization} (or $Y$-generalization) of
  $\preceq_{\mbbd}$ if, for some nonempty $Y \subseteq X$, the following all
  hold$:$
  \begin{enumerate}%[label=\textup{(\roman*)}]
    \item \label{item-preserving} for every $D \in \mbbd$ and every $x,y\in Y$,
      $x \ext_{D} y$ if, and only if, $x \preceq_{D} y$,
    \item\label{item-binary-rel} for every $D\in \dpp$, $\ext_{D}$ belongs to
      $\relations (Y)$,
    \item \label{item-dimension} for every $D\in \dpp$ and every $c,d \in
      \cpp\bs D$,  if $c \sim^\star d$ then $\extb _ {D \cup \{c\}} = \extb _
      {D \cup \{d\}}$.
  \end{enumerate}
  A generalization $\ext_{\dpp}$ is \emph{proper} if $\dpp = \mbbdp$ and
  improper otherwise.
  % , for some $Y$ and
  % every $D \in \mbbd$, $\extb_{D} = \preceqb_{D}\cap Y^{2}$.
  %Let $\ext^\star$ denote the unique (improper) generalization
\end{definition}
Let $\ext$ be a $Y$-generalization.  Part \ref{item-preserving} of
\cref{def-generalization} implies that, for every $D\in \mbbd$, $\ext$ is
simply the restriction $\preceqb_{D}\cap Y^{2}$ of $\preceq_{D}$ to $Y$. We
refer to generalizations that satisfy part \ref{item-preserving} as stable. We
also refer to this condition as the preservation or \emph{nonrevision}
condition. Part \ref{item-binary-rel} ensures that, for $\ext$ proper,
$\ext_{D}$ is a well-defined binary relation on $Y$ for every $D \in \mbbdp$.
We let $\next_{D}$ and $\sext_{D}$ respectively denote the symmetric and
asymmetric parts of $\ext_{D}$. Via part \ref{item-dimension}, the partition
$\mbbtp$ of case types generated by $\sim^{\star}$ is at least as fine the
partition generated by the equivalence relation generated by $\ext$. Two cases
$c,d \in \mbbcp$ are \emph{equivalent with respect to $\ext$}, written $c
\sim^{\extb}
d$, if, for every $D \in \mbbd$ such that $c,d \notin D$, $\extb_{D \cup \{c\}}
= \extb_{D \cup \{d\}}$.
%This notion allows us to partition the set of proper
%generalizations as follows.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%novel vs regular

\begin{definition*}\label{def-novel} A proper generalization $\ext$ is either
  \emph{regular} or \emph{novel}.  It is novel whenever $[\novel]$ is a
  distinct equivalence class of $\sim^{\supext}$, so that, for every $c \in
  \mbbc$, $c \nsim^\supext \novel$.
\end{definition*}
% In the present model, the only novel generalizations that we need will involve $Y
% \subseteq X$ of cardinality $3$ or $4$.  Let $\textup{Nov} ( Y , \preceqb
% _ \mbbd )$ denote the subset of novel $Y$-generalizations and let its
% complement $\textup{Reg} ( Y , \preceqb _ \mbbd )$ in $\Ext ( Y ,
% \preceqb _ \mbbd )$ denote the set of regular generalizations.
% Spanning generalizations are the richest (in terms of the rankings they feature)
%feasible generalizations of the learner's current forecast $\preceqb _ \mbbd
%$ and as such they are the natural candidates for testing for inconsistencies.
%In the terminology of \citet{HR-Knowledge_of_unawareness}, $\novel$ is
%aplaceholder for experience that the learner cannot currently describe.

For any given novel generalization $\ext$, $[\novel]$ is the unique novel
case type that $\sim^{\extb}$ generates.  We impose this restriction, not
because we think the imagination of learners is constrained in this way, or
because the model would not work that way, but rather because one degree of
freedom is sufficient for our purposes: {Pru} represents a minimal deviation
from Inny.

%For novel generalizations, $\novel$ mimmicks the potential arrival of new
%information. Yet novel generalizations need not feature qualitatively new rankings
%(\ie\ rankings that do not feature in $\preceq_{\mbbd}$). In \cref{lem-insep},
%we show that this is because a novel case type is characterised by the
%quantitative notion of a similarity weight.

For every regular generalization $\ext$, there exists $c \in \mbbc$ such that
$c \sim^{\extb} \novel$. Thus, there are $\countof{\mbbt}$ distinct regular
generalizations. Yet every regular $Y$-generalization $\ext$ is
\emph{equivalent} to the unique improper
$Y$-generalization $\aext = \langle \preceqb_{D} \cap Y^{2}: D \in
\mbbd\rangle$ in the sense that, for every $C \in \mbbdp$, there exists $D \in
\mbbd$ such that $C \sim^{\extb} D$ and $\extb_{C} =
\aextb_{D}$.\footnote{There is a canonical embedding of $\left\{C \times
  \extb_{C}: C \in \mbbdp\right\}$
  in $\{D \times \aextb_{D}: D \in \mbbd\}$. The converse embedding follows
  from the nonrevision condition of \cref{def-generalization}. (See
\cref{obs-reg-eq} of \cref{sec-online})}
% \Cref{obs-reg-eq} allows us to identify $\preceq_{\mbbd}$ with any regular
% $Y$-generalization $\ext$ such that $Y=X$. This motivates our treatment of
% $\preceq_{\mbbd}$ as an improper generalization of itself.
%axioms and main theorem
%-----------------------------------------------------------------------------%
\section{Axioms and main results}\label{sec-axioms-theorem}
%-----------------------------------------------------------------------------%
%-----------basic axioms
We begin by rewriting the basic axioms of \gsii\ in terms of generalizations.
We will only impose these basic axioms indirectly, via the stability axiom that
follows.

\begin{axioms*}
  Let $\ext$ be an arbitrary $Y$-generalization of $\preceq_{\mbbd}$.
  \begin{enumerate}[label=\textup{A\arabic*}] \setcounter{enumi}{-1}
    \item\label{T} \textsc{--- Transitivity axiom for $\ext$ ---} for every $D
      \in \dpp$, $\ext_{D}$ is transitive.
    \item\label{K} \textsc{--- Completeness axiom for $\ext$ ---} for every $D
      \in \dpp$, $\ext_{D}$ is complete.
    \item\label{C} \textsc{--- Combination axiom for $\ext$ ---} for every
      disjoint $C, D\in
      \dpp$ and every $x,y\in Y$, if $x \ext_{C} y$ and $x
      \ext_{D} y$, then $x \ext_{C \cup D} y ;$ and if $x \sext_{C} y$ and $x
      \ext_{D} y$, then $x\sext _{C \cup D} y$.
    \item\label{A} \textsc{--- Archimedean axiom for $\ext$ ---} for every
      disjoint $C,D \in \dpp$ and every $x,y\in Y$, if $x \sext_{D} y$, then
      there exists $k \in \posint$ such that, for every pairwise disjoint
      collection $\lb D_j : \text{$D_j \sim^\supext D$ and $C \cap D_j =
        \emptyset$}\rb_1^k$ in $\dpp$, $x \sext_{C \cup D_{1} \cup \cdots
        \cup D_{k}} y.$
    \end{enumerate}
  \end{axioms*}

%-----------------------------------------------------------------------------%
%\subsection{Regular generalizations and the diversity axioms}
  \emph{The diversity axioms} apply to regular generalizations.
  They require that $\mbbd$ is sufficiently rich to support $Y$-generalizations
  $\ext$ with a variety of \emph{total} orderings: \ie\ complete, transitive
  and antisymmetric
  ($x \ext_{D} y$ and $y \ext_{D} x$ implies $x = y$). %\label{sec-div-pru}
  Let $\total(\ext)$ denote the set $\lb R : \text{for some $C \in \dpp$, $R =
  \extb_{C}$ is total}\rb$ of of total orders that feature in $\ext$. For $k
  = 4$, the following axiom is a restatement of the diversity axiom of \gsii.
  \begin{diversity*} For every $Y \subseteq X$ of cardinality $n= 2, \dots, k$,
    every regular $Y$-generalization $\ext$ of $\preceqb _{\mbbd}$ is such that
    $\countof \total(\ext) = n!$\hskip.5pt.
  \end{diversity*}
  A given diversity axiom \emph{holds} (for $\preceqb_{\mbbd}$) \emph{on
  $Z\subseteq X$} if the axiom holds with $Z$ in the place of $X$.  We
  introduce the following axioms:
  %  \settheoremtag{par-3-div}
  \begin{taggedp3d}{\textsc{p3diversity}}\label{p3d} For every $Y\subseteq X$ with
    cardinality $n = 2$ or $3$, every regular $X$-generalization $\ext$ of
    $\preceq_{\mbbd}$ is such that $\countof{\total(\ext)} \geq n$.
  \end{taggedp3d}
  \begin{taggedc2d}{\textsc{c2diversity}}\label{c2d} For every three distinct
    elements $x , y$ and  $z$ in $X$, one of the sets $\{D' : x \prec _{D '} y \}$
    and $\{D' : y \prec_{D '} x \}$ contains  both $C$ and $D$ such that $z
    \prec _{C} x$ and $x \prec _{D} z$. If $\countof X = 2$, then \twodiv\
    holds on $X$.
  \end{taggedc2d}
  %For the present purposes, \ref{p3d}\ and \ref{c2d}\ are equivalent:

\begin{observation}\label{obs-c2d} Let $\preceq_{\mbbd}$ satisfy
  \ref{T}--\ref{A}. Then \ref{p3d}\  holds if, and only if, \ref{c2d}\
  does.
\end{observation}
\begin{proof}\label{proof-obs-c2d} By virtue of \cref{prop-c2dQ}
  and the translation of \cref{sec-proof-main}.
\end{proof}

%-----------------------------------------------------------------------------%
%\subsection{Novel generalizations and the stability axiom}
\emph{The {stability} axiom } is our second requirement. It is distinguished by
the structure it imposes on novel generalizations. In the online appendix, we
present an alternative derivation of the main theorem where stability is
restricted to novel generalizations. Here we streamline the exposition and
apply it to the testworthy class of proper generalizations:
%that assigns $\novel$ a ranking that is far from that of some database of past cases.
%First two definitions that simplify both the statement of the axiom and the
%proof of the main theorem.
%As we will see in the proof of the main theorem (see
%\cref{lem-insep}), novel generalizations are characterised by a cardinal notion. In
%contrast, the notions of testworthiness and perturbation that we now introduce
%are ordinal in nature.
% def-testworthy
\begin{definition*}\label{def-testworthy} A proper generalization $\ext$ of
  $\preceqb_{\mbbd}$ is \emph{testworthy} if it satisfies
  \textup{\ref{K}--\ref{A}} and, for some $D\in \mbbd$ such that $\ext_{D}$ is
  total, $\extb _{\novel}$ is the inverse of $\extb_{D}$.\footnote{Recall that
    the inverse $\ext _{D}^{- 1}$ of $\ext
    _{D}$ satisfies $x \ext _{D}^{- 1} y$ if, and only if, $y \ext _{D}
  x$.}
\end{definition*}

Testworthy generalizations are distinguished by the ranking at $\novel$ and the
fact that they need not satisfy \ref{T}.  Stability requires that any
testworthy generalization can be suitably perturbed to satisfy \ref{T}. By
suitable we mean
\begin{definition*} Let $\ext$ and $\aext$ be generalizations of
  $\preceqb_{\mbbd}$. $\aext$ is a \emph{perturbation} of $\ext$ if
  $\aextb_{\novel} = \extb_{\novel}$ and, moreover, a \emph{{{diverse}}}
  perturbation if $\countof \total (\aextb) \geq \countof \total ( \extb)$.
  % \emph{perturbation} of another, $\ext$, if $\extb_{\novel}=
  % \hextb_{\novel}$. It is \emph{{{diverse}}} if, in addition, $\countof
  % \total (\extb) \leq \countof \total ( \hextb)$.
\end{definition*}
A diverse perturbation of $\ext$ does not suppress the novel, transitive
rankings that $\ext$ generates. If we allowed for nondiverse (\ie\ dogmatic)
perturbations, the learner could continue to ``hide'' intransitive rankings.
Our main axiom is then
\begin{taggedstability}{\textsc{4stability}}\label{S} For every $Y\subseteq X$
  with cardinality $2, 3$ or $4$, every testworthy $Y$-generalization of
  $\preceqb _{\mbbd}$ has a {{diverse}} perturbation that satisfies
  \ref{T}–\ref{A}.
\end{taggedstability}

%Via the next observation, \fourdiv\ holds whenever \ref{S}\ holds vacuously.
%\begin{observation} \label{obs-testworthy} Let $\preceq_{\mbbd}$ satisfy
%  \ref{T}–\ref{A} and \ref{c2d}. For every $Y\subseteq X$ of cardinality $3$
%  or $4$, the set of testworthy $Y$-generalizations is nonempty. If, for some
%  $Y$, every testworthy $Y$-generalization is regular, then \fourdiv\ holds on
%  $Y$.
%\end{observation}
%\begin{proof}\label{proof-obs-testworthy} These statements follow via
%  \cref{prop-central-testworthy} and \cref{lem-test-empty-fourdiv}
%  respectively.
%\end{proof}
\emph{In our main theorem}\label{sec-main} we derive a real-valued kernel
function $\mathbf{v}$ on the product $X \times \mbbc$. We view $\mathbf{v}$ as
a matrix and $\mathbf{v}(x, \cdot)$ as one of its rows.  $\mathbf{v}$ is a
\emph{represents} sentiments $\preceq_{\mbbd}$ if
\begin{linenomath*} 
  \begin{equation}\notag\label{eq-rep-main} \left\{
      \begin{array}{l} \text{for every $x , y \in X$ and every $D \in
        \mbbd$,}\\
        x \preceq_{D} y \quad \text{if, and only if,} \quad \sum _ {\, c \,\in\, D}
        \mathbf{v}(x, c) \leq \sum_{\, c \,\in\, D} \mathbf{v}(y, c) .
    \end{array}\right.
  \end{equation}
\end{linenomath*}
The matrix $\mathbf{v}$ \emph{respects case equivalence} (with respect to
$\preceq_{\mbbd}$) if, for every $c,d\in \mbbc$, $c \sim^{\star} d$ if, and
only if, the columns $\mathbf{v}(\cdot, c)$ and $\mathbf{v}(\cdot, d)$ are
equal.
% whenever,
% it holds where, once more, $% I_{D}(t)$ counts the number of cases of type
% $t$ in
% $D$.  Finally, $v(x,\cdot),v(y,\cdot)$ and
% $v(z,\cdot)$ are \emph{affinely independent} if there is no $\lambda\in
% \R$ such that $v(x,\cdot)=\lambda v(y,\cdot)+(1-\lambda)
% v(z,\cdot)$. (Equivalently,
% $v(x,\cdot)-v(z,\cdot)$ and $v(y,\cdot)-v(z,\cdot)$ are noncollinear.)
%existence theorem

\begin{theorem}[Part I, Existence]\label{thm-main} Let there be given $X$,
  $\mbbcp$, $\preceqb_ \mbbd$ and generalizations, as above, such that the
  richness condition holds. Then \ref{ax-main} and \ref{wrap-main} are
  equivalent.
  \begin{enumerate}[label=\textup{(\ref{thm-main}.\roman*)}]
    \item\label{ax-main} \ref{p3d}\ and \ref{S}\ hold for $\preceq_{\mbbd}$.
    \item\label{wrap-main} There exists a matrix $\mathbf{v} : X \times \mbbc
      \rightarrow \R$ satisfying \ref{rep-main} and \ref{rows-main}$\,:$
      \begin{enumerate}[label=\textup{(\ref{thm-main}.\alph*)}]
        \item\label{rep-main} $\mathbf{v}$ is a representation of
          $\preceq_{\mbbd}$ that respects case equivalence$\,;$
        \item\label{rows-main} no row of $\mathbf{v}$ is dominated by any other
          row, and for every three distinct elements $x,y, z \in X$ and
          $\lambda \in \R$, $\mathbf{v} (x, \cdot) \neq \lambda
          \mathbf{v}(y,\cdot) + (1-\lambda)
          \mathbf{v}(z,\cdot)$\,.\footnote{The affine independence condition 
            holds if, and only if, $\mathbf{v}(x,\cdot)- \mathbf{v}(z,\cdot)$
          and $\mathbf{v}(y,\cdot)-\mathbf{v}(z,\cdot)$ are noncollinear.}
      \end{enumerate}
  \end{enumerate}
\end{theorem}
% uniqueness
Our uniqueness result is identical to that of \gsii.  \setcounter{theorem}{0}
\begin{theorem}[Part II, Uniqueness]%\label{thm-uniqueness}
  If \ref{ax-main} $[$or \ref{wrap-main}$]$ holds, then the matrix $\mathbf{v}$
  is unique in the following sense$\,:$ for every other matrix $\mathbf{u} : X
  \times \mbbc \rightarrow \R$ that represents $\preceqb_{\mbbd}$, there is a
  scalar $\lambda > 0$ and a matrix $\beta : X \times \mbbc \rightarrow \R$
  with identical rows (\ie\ with constant columns) such that $\mathbf{u} =
  \lambda \mathbf{v} + \beta$.
\end{theorem}
% We discuss the counterpart to \cref{thm-main} part II of  \gsii\ in \cref{sec-reference-free}. 

%    and a matrix
%   $\beta : X \times \mbbt \rightarrow \R$ with identical rows (\ie~constant
%   columns) such that $u = \lambda v + \beta$.

% Alternatively, if there is some eventuality $x$ that ought to be independent of or 
% ``orthogonal'' to each and every past case, then we may choose $\beta$ such
% that, for every $y \in X$, $\beta ( y , \cdot ) = - v ( x , \cdot )$. Then
% we obtain a new matrix representation $u : X \times \mbbt \rightarrow \R$ such
% that $u \defeq v + \beta$. Then every matrix representation $u '$ of
% $\preceqb _{\mbbd}$ with the property $u ' ( x , \cdot ) = 0$
% satisfies $u ' = \lambda u$ for some $\lambda > 0$.

%The proof of \cref{thm-main} appears in online appendix \ref{sec-proof-main}.
%It relies upon a translation from the abstract database/memory set up of the
%model to the setting of rational vectors similar to \gsii. We show that the
%translated \cref{thm-mainQ} (see \cref{sec-proof-main}) is equivalent to
%\cref{thm-main}. The proof of \cref{thm-mainQ} is then the subject of online
%appendix \ref{sec-proof-mainQ}.

%\emph{A variety of statistical methods} that are related to
%\cref{thm-main} are discussed in \gsii. In the next section, we provide a more
%detailed application to no-arbitrage asset pricing. This application also
%appeals to intermediate results in the proof of \cref{thm-main}, so let us
%unpack the key steps.
%
%The first step in the proof of \cref{thm-main} (see \cref{sec-proof-main} is to
%translate all the above concepts to the setting of rational vectors indexed by
%case types: as in \cref{fig-tali,fig-rates}, \ie\ vectors in $\nnreal^\mbbt$. 
%\Cref{thm-mainQ} is the corresponding theorem for rational vectors. 
%Definitions and intermediate results in the proof of \cref{thm-mainQ} are the
%subject of \cref{sec-proof-mainQ}.  For now, we maintain the discussion of
%these results in terms of samples (\ie\ databases).
%
%In \cref{sec-proof-mainQ}, we show that \ref{K}--\ref{c2d}\  alone yield a more
%general matrix representation than that of \cref{thm-main}. In particular, a
%matrix $v^{\dd} : X^2 \times \mbbc \rightarrow \R$, $x \times y \times c
%\mapsto v^{\xy}_{c}$.  For each pair $x, y \in X$, the row $v^{\xy}$ of this
%matrix is a pairwise representation provided that, for every $x,y\in X$ and $D
%\in \mbbd$, {$x \preceqb_{D} y$ if, and only if, $\sum_{c \,\in D}v^{\xy}_{c}
%\geq 0$.} (In \cref{fig-tali}, the projection of each such $v^{\xy}$ onto
%case-type space is orthogonal to the hyperplane $H^{\{x,y\}}$.)
%
%\ref{c2d}\  is characterised by the property that no row of $v^{\dd}$ dominates
%any another; nor is any pair collinear. The final key to the proof of
%\cref{thm-main}, {stability}, and the connection with no-arbitrage pricing is
%% Then, via an example, we introduce
%% some key concepts from the literature on hyperplane
%% arrangements.\footnote{A
%% more detailed introduction can be found in the classic text
%% \citet{orlik1992arrangements}, the recent \citet{dimca2017hyperplane} and the
%% lecture notes \citet{stanley2007introduction}.}
%\begin{definition*} %We will use the following notation.
%  For $Y \in 2^{X}$, $v^{\dd} : Y^{2}\times \mbbc \rightarrow \R$ satisfies the
%  \emph{groupoid property} (Jacobi identity) whenever, for every $x , y , z \in
%  Y$, $v^{\xz} = v^{\xy} + v^{\yz}$.
%\end{definition*}
%When the Jacobi identity holds on $X$, instead of the $\binom{\countof X}{2}$
%rows of $v^{\dd}$, we only need $\countof X$ vectors in $\R^\mbbc$ to summarise
%$\preceqb_\mbbd$. This is the crux of \cref{thm-main}.
%\begin{theoremEnd}[link to proof]{corollary}[a characterisation of
%  {stability}]\label{cor-foureq} Let the number of case types be finite and let
%  $\preceq_{\mbbd}$ satisfy \ref{c2d}\ . Then $\preceq_{\mbbd}$ satisfies
%  \stability\ if, and only if, $\preceq_{\mbbd}$ has a pairwise representation
%  $v^{\dd}$ that satisfies the Jacobi identity. Moreover, for every other
%  pairwise representation $u^{\dd}$, there exists $\lambda >0$, such that
%  $u^{\dd} = \lambda v^{\dd}$.
%\end{theoremEnd}
%\begin{proofEnd}%[Proof of \cref{cor-foureq}]
%  This follows from \cref{thm-foureq}, \cref{lem-induction} and the fact that,
%  via \cref{lem-test-empty-fourdiv}, \stability\ implies \ref{T}--\ref{A} when
%  $\countof{\mbbt} \leq \infty$. \label{proof-cor-foureq}
%\end{proofEnd}
%
%This suffices for us to formalise the log-accumulation process of
%\cref{eg-zeros}.

\section{Applications}\label{sec-applications}
\emph{Supervised learning.} %\label{sec-applications}
Building on \cref{eg-supervised}, the following steps show how the axioms can
be used to ``complete'' sentiments and the main theorem can be used to derive a
kernel.
\begin{enumerate}
  \item\label{eg-supervised-resample} Derive a collection $\mc D_n$ of training
    sets by resampling from $D^{\star}$. (To start with, take $n$ ``basis''
    training sets each of which contains just a single copy of some case in
    $D^{\star}$.)
  \item\label{eg-supervised-sentiments}Induce sentiments on $\mc D_n$. That is
    a plausibility ranking on $Y$ for each $D$ in $\mc D_n$.
    % , so that, given
    % $y$ and $y'$ in $Y$, $y \preceq_{D} y'$ if, and only if, $y$ is
    % less plausible than $y'$.
  \item \label{eg-supervised-axioms} Via \ref{p3d} and \stability, extend
    sentiments to the set $\mbbd_n$ of every finite resampling of $D^{\star}$.
  \item\label{eg-supervised-hyperplanes} Repeat steps
    \eqref{eg-supervised-resample}-\eqref{eg-supervised-axioms} identify the
    hyperplane arrangement $\mc H = \left\{H^{\{y,y'\}}: y, y' \in Y\right\}$
    in $\posreal^{\mbbt}$ for the set $\mbbt$ of case types (endogenous data
    bins or equivalence classes).
    %and a partition of $\mbbc_n$ %= \{c : c \in D, D \in \mbbd_n\}$
    %of copies of cases in $D^{\star}$.
  \item Derive a collection $v^{\dd}$ of vectors that are orthogonal to the
    hyperplanes of \eqref{eg-supervised-hyperplanes} satisfy the groupoid
    identity: for every $y, y',y'' \in Y$, $v^{\yypp} = v^{\yyp} + v^{\ypypp}$.
    %\item \label{eg-supervised-4div} Check whether the \fourdiv\ axiom holds.
    %  If it does, then appeal to theorem 1 of \gsii\ and proceed to
    %  \eqref{eg-supervised-estimate} of this list. 
    %\item Check if \parthreediv\ holds appeal to \cref{thm-main} proceed to
    %  step \eqref{eg-supervised-estimate}.
  \item \label{eg-supervised-estimate} Via the groupoid identity, derive a
    kernel $\mathbf{v}: Y \times \mbbt \rightarrow \R$ that satisfies
    \cref{thm-main}.
  \item Estimate $\rho_x$ using $\hat\rho: Y \times \mbbd_n \rightarrow \R$, $y
    \times D \mapsto \hat \rho_D(y) = \sum_{c \,\in D} \mathbf{v}(y, c)$, 
    %for the kernel $k = \lvert D\rvert \cdot v$ where 
    where $v$ satisfies the properties of \cref{thm-main} (or 
    %(\cf\ section 3.1 of \gsii).
\end{enumerate}
%\Cref{eg-tali} provides a particularly simple example of how to go about step
%(ii).

%-----------------------------------------------------------------------------%
\emph{Application to no-arbitrage asset pricing.} %\label{sec-applications}
%-----------------------------------------------------------------------------%
%We test these conditions
%under the null hypothesis of linear pricing kernels: allowing us to fill gaps 
%(implicit rankings) in the data. 
Fixed income securities (henceforth \emph{bonds}) are commonly traded over the
counter with certain parties (typically investment banks) acting as market
makers. Similar to bookmakers in betting markets, market makers profit from the
bid-ask spread. Unlike bookmakers, however, they do sieze on perceived
opportunities to make money by trading with other market makers. To simplify
the exposition, we assume the market makers are \emph{fair}, so that the
bid-ask spread is zero.

Recall that a (fixed income) \emph{forward} is a contract between two parties
to exchange a bond at a given date, price and amount in the future.  Building
on \cref{eg-zeros}, let $X$ denote the set of settlement/maturity dates for
forwards associated with given zero-coupon bond (a \emph{Zero}) issued by the
same entity. In particular, take $X = \{x_0, x_1, \dots\}$ such that $x_0 = 0$
and $x_k < x_{k + 1}$ for all feasible $k$. 
\begin{remark*}[on zero-coupon bonds] Although coupon-paying bonds are far more
  common, Zeros provide a natural asset for studying no-arbitrage pricing
  \citep{barillas2019speculation}. Zeros are generated by ``stripping''
  coupon-paying bonds of their coupons enroute to deriving benchmark
  zero-coupon yield curves \citep{brealey2020principles}. Since coupon
  stripping is reversible, restricting attention to Zeros is without loss of
  generality. Modulo surmountable complications relating to the uncertainty of
  the cashflows, the arguments that follow extend to dividend-paying stocks.
  % (In this case, via options markets, we may identify suitable benchmarks for
  % calculating the present value of dividends.)
\end{remark*}

Let $\past$ denote the (unique) current history of time-series data that are
relevant to this market for Zeros.  We generate the set $\mbbc$ of past cases
by taking each case $c \in \mbbc$ to consist of market-relevant data from a
given time interval (a block of time periods) in the past. Block resampling of
time series data was originally developed by
\citet{kunsch1989jackknife,politis1994stationary}. (For applications to finance
problems see \citet{white2000reality,harvey2021lucky}.) For the present
purposes, blocks need to be chosen so that, for any $k > 0$ and resampling $D =
\{c_1, \dots, c_k\}$, what matters is the number of repetitions of a given case
type, not the order of the cases that form the pseudo-time series. In other
words, $D \sim^\star D'$ for any permutation $D'$ of the cases in $D$.  It is
straightforward to verify that this is indeed the case for the stationary
bootstrap of \citet[section 2]{politis1994stationary}.

The free case $\novel$ has no additional structure beyond that of
\cref{sec-model,sec-main}.

Before turning to the interpretation of $\preceq_\mbbd$ in the present setting,
we introduce current market prices. From the market maker's perspective,
current prices reflect the rest of the market's view on yields to maturity
conditional on $\past$. Since we take current market prices (and rates) to be
fixed, we suppress reference to $\past$. The market maker observes past cases,
and current prices and forms a view about how prices might change when the
information changes.  The compound-interest formula for the \emph{market
accumulation process} (at current market rates) is then
\begin{linenomath*}
  \begin{equation*}
    a^{\xy} = \left(1+r^{\{x,y\}}\right)^{-x + y},
  \end{equation*}
\end{linenomath*}
where $r^{\{x,y\}}$ is the market's implied forward yield for the contract that
accrues interest between dates $x$ and $y$. If $x$ is later than $y$, then the
contract is to sell, and the market maker pays this yield, so that $r^{\{y,x\}}
= r^{\{x,y\}}$. For each $x \in X$, the \emph{market price} of a Zero that pays
out one-dollar at date $x$ is defined as $b(x) \defeq a^{(x, 0)}$.

Let $X \subseteq \R_{\mplus}$ index a suitable sequence of trading dates with
$0 \in X$ being the spot date.  It is well known that a Zero that pays out one
dollar at time $x > 0$ is arbitrage-free if, and only if, the log-accumulation
process satisfies
\begin{linenomath*}
  \begin{equation}\label{eq-no-arbitrage-2}
    \text{for every $x,y,z \in X$,}\quad \log a^{\xy} = \log a^{\xz} + \log
    a^{\zy} \,.\footnote{To see this, \withoutlog\ suppose that, for some $x <
      y < z$, $a^{\xz} < a^{\xy}a^{\yz}$.  The market maker would do well to sell
      the contract $\xz$ and buy the contracts $\xy$ and $\yz$. In the absence of
      counterparty risk, the difference between interest paid and received is 
    risk-free.}
  \end{equation}
\end{linenomath*}
This no-arbitrage condition is a special case of the Jacobi identity.

We now explain how a market maker might infer her own subjective accumulation
process from past cases. Our market maker thinks in terms of economic profits
(relative to the market price). For every finite resampling $D$ and every date
$x$ and $y$, let $x \preceq_{D} y$ if, and only if, at $D$, the market maker
finds $y$ (weakly) more plausible than $x$ in answer to the question
\begin{quote}
  ``Hold current market prices fixed and consider a one-dollar investment
  today. Given $D$, which maturity will yield a higher return?''
\end{quote}
% ``At $D$, buying the forward contract that settles at date $x$ and matures at
% date $y$ will yield a higher return than selling.''
% ``Would you rather buy or sell the forward that settles at date $x$ and
% matures at date $y$?

\begin{remark*}[alternative interpretation] It is also possible to interpret
  $\preceq_\mbbd$ in terms of statements of the market maker's intention to buy
  or sell forwards. Let $(x, y)$ be shorthand for the forward contract where
  the buyer accumulates interest between dates $x$ and $y$. For $x < y$, 
  %accumulating
  %interest over the interval of time $(x, y)$ means buying $(x, y)$; since
  %market
  %making is fair,
  accumulating interest over $(y, x)$ simply means selling $(x, y)$.  Holding
  current prices fixed, for each $D$ in $ \mbbd$, we have $x \preceq_{D} y$ if,
  and only if, given $D$, the market maker would buy $(x, y)$. Under this
  interpretation, if at $\past$ the market maker agrees with the market, then
  $x \simeq_{\past} y$ for every $x, y \in X$. Thus, if the market maker agrees
  with the market, then her {sentiments} are centered as in \cref{fig-tali}.
  Our point is that the market maker may not agree with the market and thus the
  rankings map may be uncentered as in \cref{fig-rates}.  To operationalise
  this interpretation, simply suppress reference to $a^{\dd}$ in
  what follows.%: that is, let $a^{\xy} = 1$ for every $x,y \in X$.
\end{remark*}

Next, we introduce the market maker's (possibly negative and subjective) markup
function. This is a markup relative to current rates $r^{\{\cdot, \cdot\}}$.  A
\emph{markup function} $\mu : X^2 \times \mbbc \rightarrow \R$, is
characterised by three conditions: for time intervals of length zero, the yield
is zero; fair pricing; and case equivalence.  These are, respectively,
formalised as follows: for every $x,y\in X$ and every $c , d\in \mbbc$,
$\mu^{\{x,x\}}_{c} = 0$; $\mu^{\{y,x\}}_{c} = \mu^{\{x,y\}}_{c}$; and $c
\sim^{\star} d$ if, and only if, $\mu^{\{x,y\}}_{c} = \mu^{\{x,y\}}_{d}$.  We
extend to $\mbbd$ by taking $\mu^{\{x, y\}}_{D}$ to be the (geometric)
\emph{mean markup conditional on $D$}
\begin{equation*} 1 + \mu^{\{x, y\}}_{D}
  = \prod_{c\,\in D} \left(1 + \mu^{\{x, y\}}_{c}\right)^{\frac{1}{\lvert D
  \rvert}}.
\end{equation*}
The market maker's subjective forward yield conditional on $D$ is then the
following modification of the market yield $r^{\{\cdot,\cdot\}}$: for every
$x,y\in X$ and $D \in \mbbd$
\begin{equation*}
  1 + \rho^{\{x,y\}}_{D} = \left(1 + r^{\{x,y\}}\right)\cdot\left(1 +
  \mu^{\{x,y\}}_{D}\right).
\end{equation*}

In turn, the market maker's subjective accumulation process ${A^{\dd}}: X^2
\times \mbbd \rightarrow \R$ modifies the market accumulation process
$a^{\dd}$. For every $x, y \in X$ and $D \in\mbbd$,
\begin{equation*}
  {A}^{\xy}_{D} := \left(1 + \rho^{\{x,y\}}_{D}\right)^{-x + y} = a^{\xy} \cdot
  \left(1 + \mu^{\{x, y\}}_{D}\right)^{-x + y}.
\end{equation*}
Note that the market maker agrees with the current market accumulation process
whenever the cases in $\past$ are such that the positive markups countervail
those that are negative. That is, for every $x, y \in X$, ${\mu}^{\xy}_{\past}
= 0$.

The market maker's (subjective) empirical bond price function $B : X \times
\mbbd \rightarrow \R$ modifies the market price $b: X \rightarrow \R$. Thus,
for a Zero with a one-dollar face value the price at time $x$, conditional on
$D$ is 
\begin{equation*} B(x,D) := {A}^{(x, 0)}_{D} =
  b(x) \cdot \left(1 + \mu^{\{0,x\}}_{D} \right)^{-x} .
\end{equation*}
This reflects the inverse relationship between bond prices and yields.  

When $D^{\star}$ belongs to $\mbbd$, the number of case types is finite, the
following corollary of \cref{thm-main} characterises \stability\ in the
Zero-market setting.
\begin{corollary}\label{cor-bond-rep} For $X$, $\mbbcp$, $\preceq_\mbbd$ and
  generalizations as in the present section, $\preceq_{\mbbd}$ satisfies
  \ref{c2d} and \stability\ if, and only if, there exists empirical implied
  yield and empirical bond price functions, such that
  \begin{linenomath*} 
    \begin{equation}\tag{$*$}\label{eq-bond-rep} \left\{
        \begin{array}{l}
          \text{for every $x , y \in X$ and every $D \in \mbbd$,}\\
          x \preceq_{D} y \quad \text{if, and only if,}\quad B(x,D) \geq B(y,D).
      \end{array}\right.
    \end{equation}
  \end{linenomath*} 
  Moreover, for every other empirical bond price function $\acute{B}$ that
  satisfies \eqref{eq-bond-rep}, there exists a scalar $\lambda > 0$ such that
  $\log \acute{B} = \lambda \log B\,;$ and, for every $D\in \mbbd$, the
  associated accumulation process ${A}^{\dd}_{D}$ is arbitrage-free.
\end{corollary}
\begin{proof}[Proof of \cref{cor-bond-rep}] Via \cref{cor-foureq},
  $\preceq_{\mbbd}$ satisfies \stability\ if, and only if, there exists a
  pairwise Jacobi representation $v^{\dd}: X^{2} \times \mbbc \rightarrow \R$.
  % For each $x, y \in X$ and $c\in
  % \mbbc$ let 
  % \begin{equation*}\log {A}^{\xy}_{c} = v^{\xy}_c.\end{equation*}
  Recalling that $0 \in X$, for every $x\in X$ and $D \in \mbbd$, let
  \begin{linenomath*}
    \begin{equation}\label{eq-bond-vxy} \textstyle B(x,D) = \exp\left(
        -\frac{1}{\lvert D \rvert} \sum_{c\,\in D}
      v^{(0, x)}_{c} \right).
    \end{equation}
  \end{linenomath*}
  For the proof of \eqref{eq-bond-rep}, suppose that \withoutlog, $x
  \preceq_{D} y$, so that $\sum_{c\,\in D} v^{\yx}_{c} \leq 0$. Via the Jacobi
  identity we have $ \sum_{c\,\in D} \left\{v^{(y,0)} + v^{(0,x)} \right\} \leq
  0$.  Via the representation property $v^{(y, y)} = 0$.  Another application
  of the Jacobi identity yields $v^{(y, 0)} = -v^{(0, y)}$. Thus, 
  \begin{linenomath*}
    \begin{equation*}
      \textstyle - \log B(x,D) + \log B(y,D) = \frac{1}{\lvert D\rvert}
      \sum_{c\,\in D} \left \{v^{(0, x)}_{c} - v^{(0, y)}_{c}\right \} \leq 0.
    \end{equation*}
  \end{linenomath*}

  We now show that the bond price is a suitable function of the empirical yield
  function.  For every $x,y \in X$ and $D \in \mbbd$, take $\log {A}^{\xy}_{D}
  = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D}v^{\xy}_{c}$.  That is,
  \begin{equation*}
    \log a^{\xy} + \tfrac{y - x}{\lvert D\rvert} \sum_{c\,\in D}\log
    \left(1+\mu^{\{x,y\}}_{c}\right) = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D}
    v^{\xy}_{c}. 
  \end{equation*}
  Take $D = \{c\}$ and note that the Jacobi identity implies $v^{\xy}_c = -
  v^{\yx}_c$. Thus
  \begin{equation*}
    \log a^{\xy} + (y - x) \log \left(1+\mu^{\{x,y\}}_{c}\right)
    = -\log a^{\yx} - (x - y) \log \left(1+\mu^{\{y,x\}}_{c}\right).
  \end{equation*}
  Moreover, note that $\log a^{\xy} = -\log a^{\yx}$, so that, by cancelling
  terms, we obtain
  \begin{equation}\label{eq-mu-sym}
    \log \left(1+\mu^{\{x,y\}}_{c}\right) = \log \left(1+\mu^{\{y,x\}}_{c}\right),
  \end{equation}
  so that $\mu^{\{x,y\}}_{c} = \mu^{\{y,x\}}_{c}$. Then $v^{\xx}_{c} = 0 = \log
  a^{\xx}_{c} $ implies that $\mu^{\{x,x\}}_{c} = 0$. Finally, note that for $c
  \sim^{\star} d$, the property $\mu^{\{x,y\}}_{c} = \mu^{\{x,y\}}_{d}$ is
  inherited from $v^{\xy}_{c} = v^{\xy}_{d}$, so that we have an empirical
  implied yield function. All of the above arguments are reversible, so that,
  given the existence of such empirical yield and bond price functions
  satisfying \eqref{eq-bond-rep}, it follows that $\preceq_{\mbbd}$ satisfies
  \stability.

  If $\acute{B}$ is another empirical bond price function that satisfies
  \eqref{eq-bond-rep}, then via \cref{eq-bond-vxy} and \cref{cor-foureq}, for
  some $\lambda >0$, $\log \acute{B} = \lambda \log B$.  The fact that, for every
  $D$, ${A}^{\dd}_{D}$ is arbitrage-free follows by virtue of the fact that the
  Jacobi identity holds element-wise for $v^{\dd}$.  In particular, since, for
  every $x, y , z \in X$ and $c \in D$, $\log {A}^{\xy}_{c} = v^{\xy}_{c}$, 
  \begin{equation*}
    \log {A}^{\xy}_{D} = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D}\log
    {A}^{\xy}_c
    %- \sum_{c\,\in
    %D} v^{\xy}_c = - \sum_{c\,\in D} \left\{v^{\xz}_c + v^{\zy}_c \right\} =
    = \tfrac{1}{\lvert D\rvert}\sum_{c\,\in D} \left\{\log {A}^{\xz}_c + \log
    {A}^{\zy}_c \right\}.
  \end{equation*} 
  Taking exponents, we obtain the no-arbitrage condition ${A}^{\xy}_{D} =
  {A}^{\xz}_{D} \cdot {A}^{\zy}_{D}$.
\end{proof}

***prop-fourdiv-empty here ***

The uniqueness result in \cref{cor-bond-rep} is stronger than in the general
setting of part II of \cref{thm-main}. This is a consequence of the fact that
our empirical bond yield satisfies the property $r^{\xx} = 0 = \mu^{\xx}$, for
every $x \in X$. 

\emph{Our diversity axiom, \ref{c2d}\ ,} has a straightforward
interpretation in the present setting. Given \stability, \twodiv\ implies that,
for every distinct $x$ and $y$, there exist $c$ and $d$ such that $v^{\xy}_{c}
< 0 < v^{\xy}_{d}$.  This, via the arguments that lead to \cref{eq-mu-sym}, is
equivalent to both $\mu^{\{x,y\}}_{d} < 0 < \mu^{\{x,y\}}_{c}$ and
$\rho^{\{x,y\}}_{c} < r^{\{x,y\}} < \rho^{\{x,y\}}_{d}$.  In words, \twodiv\
requires that the market maker's data is rich enough to contain at least one
case where her markup between date $x$ and $y$ is positive, and at least one
where it is negative.
% \footnote{The affine independence part of \ref{}translates as: there is no
% $\lambda \in \R$ such that, for every past case $c\in \mbbc$,
% $B(x, \{c\}) \neq B(y,\{c\})^{\lambda} B(z,\{c\})^{1-\lambda}$.} 
\Condtwodiv\ {generalizes} this notion to require that for every distinct $x,
y, z$, $\mu^{\{x,y\}}_{C} < 0 < \mu^{\{x,y\}}_{D}$ for some $C$ and $D$ such
that $\mu^{\{x,z\}}_{C}\cdot \mu^{\{x,z\}}_{D} >0$. An equivalent
characterisation is also available in terms of the affine independence
condition \ref{rows-main} of \cref{thm-main} for the matrix $\beta: X \times
\mbbc \rightarrow \R$, $x\times c \mapsto \beta(x, c) = B(x, \{c\})$.

The considerably stronger \condthreediv\ arises (implicitly) in
the final step of our proof of \cref{thm-foureq}. When it holds for
$\preceq_{\mbbd}$, {stability} is unnecessary (provided \ref{T}--\ref{A} hold).
In terms of mark ups, we may characterise \condthreediv\ as,
for every distinct $x, y, z$ and $w$, one of the half spaces $\{D :
\mu^{\{x,w\}}_D > 0\}$ or $\{D: \mu^{\{x,w\}}_D < 0\}$ contains $D_1, \dots,
D_6$ and permutations $\pi_1, \dots, \pi_6$ of $x, y$ and $z$ such that for
$i=1,\dots,6$, 
\begin{equation*}
  \mu^{\{\pi_i(x),0\}}_{D_i} < \mu^{\{\pi_i(y),0\}}_{D_i} <
  \mu^{\{\pi_i(z),0\}}_{D_i}.
\end{equation*}
Yet \fourdiv\ is stronger still, requiring the above to hold on both half
spaces.

This disparity between \ref{c2d}\ and \fourdiv\ reflects the value of
experience or of rich data. It also reflects the value of a prudent imagination
when the data fails to be rich.  Outside of market settings, this may be as far
as we can go, but in the present context we can say more.

\emph{Market efficiency} We begin with a proposition that justifies our
claim that when \ref{S} holds, so will the efficient markets hypothesis in
the usual \citep{fama1970efficient} sense: passive (buy-and-hold) strategies
outperform active ones.  To this end, let $\beta : X \times \mbbdp \rightarrow
\nnreal$ be an empirical bond price function such that its restriction to $X
\times \mbbd$ coincides with $B$ of \cref{cor-bond-rep} and let
\begin{equation*}
  %\frac{\lvert \Delta_c B(x, D)\rvert}{B(x, D)} 
  g_c(x, D) := \frac{\beta(x, D\cup \{c\}) - \beta(x, D)} {\beta(x, D)} 
\end{equation*}
denote the \emph{proportional price increment} for the maturity date $x$ given
the data $D$, following the arrival of a new case $c$.  By new case, we mean
that $c$ can be regular or novel.  Under stable pricing, the proportional price
increment converges to zero exponentially.
\begin{proposition}\label{prop-stability} For every $x \in X$, $D \in \mbbd$ of
  cardinality $n$ and $c \in \mbbcp \bs D$,
  \begin{equation*}
    1 + g_c(x, D) = \left(\left(1 + \rho_{c}^{\{0,x\}}\right)^{-x} / B(x,
    D)\right)^{\frac{1}{n+1}} \leq (1 + \rho)^{\frac{2 x}{n+1}}
  \end{equation*}
  where $\rho = \argmax \left\{\left(1 + \rho_d^{\{0, x\}}\right)^{-x} : d
  \in D \cup \{c\}\right\}$.% and $\xi= \max X$.
\end{proposition}
\begin{proof}
  Let $1, \dots, n$ be an enumeration of $D$ and identify $c$ with $n+1$.
  Moreover, let us suppress reference to $\{0, x\}$, so that $\rho_{n+1} =
  \rho_{c}^{\{0, x\}}$. We first derive the equality. By \cref{cor-bond-rep},
  and the fact that $\beta$ agrees with $B$ on $X \times \mbbd$, we obtain 
  \begin{equation*} \beta(x, D) = B(x, D) =
    \prod_{i}^{n} \left(1 + \rho_{i}\right)^\frac{-x}{n} .
  \end{equation*}
  and, by manipulating exponents, we obtain
  \begin{equation*}\beta(x, D \cup \{c\}) =
    \left(\prod_{i}^{n+1} \left(1 +
    \rho_{i}\right)^\frac{-x}{n}\right)^\frac{n}{n+1}.
  \end{equation*}
  Substituting for $B(x, D)$ and noting that $\frac{n}{n+1} = 1 -
  \frac{1}{n+1}$, we arrive at the expression
  \begin{equation*}
    1 + g_c(x, D) = \frac{B(x, D)^{1 - \frac{1}{n+1}} \cdot \left (1 +
    \rho_{n+1} \right)^\frac{-x}{n+1}}{B(x, D)}.  
  \end{equation*}
  For the inequality, note that $B(x, D)^{-1} = \exp(\frac{1}{n}\sum_{1}^{n}
  \nu_i)$ where $\nu_i = x\log(1+\rho_i)$. Let $\nu_{n+1} = x \log(1 +
  \rho_{n+1})$.  Then, for $\nu := \max\{\lvert \nu_i \rvert : i = 1, \dots,
  n+1\}$, we have
  \begin{equation*}
    \frac{1}{n}\sum_{1}^{n} \nu_i \leq \left \lvert \frac{1}{n}\sum_{1}^{n}
    \nu_i\right \rvert \leq \frac{1}{n} \sum_1^n \lvert \nu_i \rvert \leq \nu =
    x \log (1 + \rho).
  \end{equation*}
  Then, since $\exp$ is an increasing function,
  \begin{equation*}
    B(x, D)^{-1} \leq \exp (\nu) = (1 + \rho)^{x}
  \end{equation*}
  Mutatis mutandis, the same argument yields $(1 + \rho_{n+1})^{-x} \leq (1 +
  \rho)^{x}$. Extending this pair of bounds extend to the product brings us to
  the desired inequality. 
\end{proof}

Key to the proof of \cref{prop-stability} is that the new pricing kernel
$\beta$ coincides with the old one $B$ on $X \times \mbbd$.
\Cref{prop-stability} thus demonstrates that \stability\ implies the usual
notion of stability of statistical learning
\citep{poggio2004general,mukherjee2006learning,bousquet2002stability}. That is
to say it implies continuity of the learning map (here induced by the ranking
map) $L:\bigsqcup_{i \geq 1} Z^n \rightarrow \mc H$ from the sample space to
the hypothesis space \citep{mukherjee2006learning}.  

The following corollary of \cref{prop-stability} confirms that, for stable
pricing kernels, the impact of the arrival of a given case type is decreasing
in its frequency.  This is a decreasing marginal impact of information
property: the information carried by a common case type is already baked into
the price.  We discuss the impact of this result on market efficiency following
the statement and proof.
\begin{corollary}\label{cor-efficiency}
  For $D \in \mbbd$ of cardinality $n$, let $\mbbt_D$ be the set of case types
  that feature in $D$ and let $c, d \in \mbbc \bs D$ be cases of type $s \in
  \mbbt_D$. % such that $D\cap s \neq \emptyset$.
  Then, for every $x \in X$,
  \begin{equation*}
    \lvert g_{d}(x, D\cup \{c\}) \rvert < \lvert g_c(x, D)\rvert
  \end{equation*}
  and, moreover, if $n_s$ is the frequency of $s$ in $D$, then $g_c(x, D)$
  tends to $0$ as $n_s \rightarrow n$. 
\end{corollary}
\begin{proof}
  %\Wlog\ suppose that, for every $t \in \mbbt$, $n_t = \countof (D \cap t) >
  %0$. 
  Take $\gamma_c = g_c(x, D)$ and $\gamma_d = g_d(x, D\cup\{c\})$.  For any
  $c'$ of type $t$, let $\rho_t = \rho_{c'}^{\{0, x\}}$.  We claim that
  \begin{equation}\label{eq-ppi-types}
    1 + \gamma_c = \left(\left(1 + \rho_s\right)^{n - n_s} / \prod_{t \neq s}
    \left(1 + \rho_t\right)^{n_t} \right)^\frac{-x}{n(n+1)}.
  % \xrightarrow[n_s\to \,n]{} 1
  \end{equation}
  First note that, since, for each $t \in \mbbt_D$, $D$ contains $n_t > 0$
  cases of type $t$, we have 
  \begin{equation*}
    B(x, D) = \left(\prod_{\,\,\, t \,\in\, \mbbt_D} \left(1 +
    \rho_t\right)^{n_t}\right)^\frac{-x}{n} = \left(\prod_{t \neq s} \left(1 +
    \rho_t\right)^{n_t} \cdot \left(1 + \rho_s\right)^{n_s}\right)^\frac{-x}{n} 
  \end{equation*}
  Then a substitution for $B(x, D)$ in the expression for $1 + \gamma_c$ of
  \cref{prop-stability} followed by a straightforward manipulation of exponents
  yields \cref{eq-ppi-types}.

  Let $\theta$ denote the main ratio (inside the brackets) of
  \cref{eq-ppi-types}.  For convergence of $\gamma_c$ to $0$ note that $n_s
  \rightarrow n$ implies that, for every $t \neq s$, $n_t \rightarrow 0$. Thus
  both the numerator and the denominator of $\theta$ tend to one and
  $\gamma_c \rightarrow 0$.

  For monotonicity, since $d$ also belongs to $s$, we extend
  \cref{eq-ppi-types} to obtain
  \begin{equation}\label{eq-ppi-types-2}
    1 + \gamma_d = \left(\left(1 + \rho_s\right)^{(n + 1) - (n_s + 1)} /
    \prod_{t \neq s} \left(1 + \rho_t\right)^{n_t}
    \right)^\frac{-x}{(n+1)(n+2)}.
  \end{equation}
  Note that the exponent of the main numerator in \cref{eq-ppi-types-2}
  simplifies to $n - n_s$. Thus, the main ratio in \cref{eq-ppi-types-2} is
  also equal to $\theta$.  In both \cref{eq-ppi-types} and
  \cref{eq-ppi-types-2}, the exponent involving $x$ is negative, so that, via
  the connection with $\theta$, 
  \begin{equation}\label{eq-theta-equiv}
    \text{$\gamma_c < 0$ \quad iff \quad $\theta > 1$ \quad iff \quad
    $\gamma_d < 0$.}
  \end{equation}
 
  Combining \cref{eq-ppi-types} and \cref{eq-ppi-types-2}, substituting
  $\theta$ and simplifying the exponent:
  \begin{equation}\label{eq-ppi-ratio}
    \frac{1 + \gamma_d}{1 + \gamma_c} = \theta^{2x \cdot \frac{n+1}{k}},
  \end{equation}
  where $k = n(n+1)^2(n+2)$. Since the exponent here is positive, we arrive at
  \begin{equation}\label{eq-theta-equiv-2}
    \text{$\frac{1 + \gamma_d}{1 + \gamma_c} < 1$ \quad iff \quad $\theta <
    1$.}
  \end{equation}
  Via \cref{eq-theta-equiv}, there are two cases to consider. The most obvious
  is $\gamma_d, \gamma_c > 0$, for then $\theta < 1$ and $\gamma_d < \gamma_c$
  follows from \cref{eq-theta-equiv-2}. The remaining case is $\gamma_c,
  \gamma_d < 0$ so that, via \cref{eq-theta-equiv}, $\theta > 1$. Then, since
  $1 + \gamma_d = 1 - \lvert \gamma_d \rvert$ (and, mutatis mutandis, the same
  is true of $\gamma_c$), via \cref{eq-theta-equiv-2}, we arrive at
  \begin{equation*}
    \text{$\frac{1 - \lvert \gamma_d \rvert}{1 - \lvert \gamma_c \rvert} > 1$
    \quad iff \quad $\theta > 1$.}
  \end{equation*}
  A simple rearrangement then yields the desired inequality.
\end{proof}
In terms of market efficiency, implies that, if we assume that, more often than
not, the future resembles the past in the sense that past cases with a higher
frequency are more likely to continue to arrive more frequently,
\cref{cor-efficiency} means that the most likely cases are crowded out by the
volume other cases of the same type.  When the pricing kernel is stable, the
only way an active strategy can exploit the information in past cases is to
more accurately predict the type of the case. But, since the marginal return to
the most frequent cases is diminishing, this will be an uphill struggle.

Yet \cref{cor-efficiency} goes further. It tells us that there is value in
diversity, value to the information discovery process, for novel case types are
more rewarding. This explains why financial institutions spend vast resources
on research. When the price kernel is stable, the largest price movements come
out of ``left field''.  This frequentist idea of market efficiency allows
research and active strategies to co-exist with passive buy-and-hold. Most of
the time buy-and-hold is likely to be better, but if one can also discover the
nature of less frequent, novel cases prior to their arrival, one can improve by
actively researching the novel case generation process.  In this way, an active
imagination and hard work can help an entrepreneurial market maker to go beyond
prudence. (And also beyond the formal scope of this paper.) 

With unstable pricing kernels, the story is very different, whilst the yield
accumulation process $A = \{A^{\xy}_D : x, y \in X, D \in \mbbd\}$ is
well-defined, since the groupoid property fails to hold, the pricing kernel is
not. Whilst it is mathematically possible to derive equivalent statements to
\cref{prop-stability} and \cref{cor-efficiency} for $A$, it is
meaningless in the presence of the resulting arbitrage opportunities that are
present and likely to cause instability to $A$.  Instead, the value of $A$ is
that we can study out-of-equilibrium market activity in the absence of a
well-defined pricing kernel (in the sense of \cref{cor-bond-rep}). To our
knowledge, this feature is absent in the literature.

\Citet[p.60]{malkiel2003efficient} points out that ``Markets can be efficient
even if many market participants are quite irrational.'' Here, we can show that
markets can be efficient, in the sense that the pricing kernel is stable (and
thus acting as if it were guided by a prudent market maker), even if no agent
is prudent. Given the above discussion, this possibility should be clear when
the data is rich.  But what if the data fails to be \emph{4}-diverse?

Throughout the following, we suppose that \ref{c2d}\ holds. For in its
absence, uniqueness and existence are not guaranteed.

Recall that in our derivation above, we only assumed the market's accumulation
and bond price were available conditional on the current set $\past$. Suppose
the analyst is able to extend the market accumulation process and condition on
any finite sample and, moreover, is able to identify an arbitrage-free bond
price $b: X \times \mbbd \rightarrow \R$. (Below we provide a rudimentary
algorithm for this generalization.) Then our results say that the market bond
price induces a rankings map $\preceq_{\mbbd}$ that satisfies \stability.
Prudent pricing emerges from market activity even in the absence of prudent
agents.

This inductive notion of market efficiency differs from the more Bayesian and
forward-looking definitions of \citet{fama1970efficient} and
\citet{malkiel2003efficient}. It is not a form of weak efficiency, for we may
define cases to include more than historical data on prices. We may include
features such as firm size or price-earnings data or indeed any of the many
other factors that are extensively discussed in the literature
\citep{fama2015five,harvey2021lucky,gu2020empirical}.

%Prudent pricing in the absence of prudent market makers is on the one hand
%profound. Recall the neuroscientific evidence of a vital role for the
%imagination in prediction not only in humans (but also rats).  Prudent pricing
%represents a ``Deus ex machina'' result.  It says that markets and by
%generalization machines can produce behaviour that is indistinguishable from
%an imaginative, prudent market maker.
%

In our view, the fact that, in market settings, stable pricing can arise even
in the absence of both rich data (\fourdiv) and prudent agents is a tribute to
the power of markets: it is an instance of Adam Smith's invisible hand at its
best. While the threat of arbitrage may supplant the imagination and provide an
alternative form of external validation, stability also provides a way to
identify or ``memorize'' past cases. This is a more operational,
non-behavioural form of efficiency.

\emph{Structural breaks and second-order induction.} Of course some new
types of cases might for good reason require a re-evaluation of past ones. Such
cases lead to the formation of a new rankings map $\grave{\preceq}_{\mbbd}$. We
would expect such cases to be less common than other forms of novel case since
they represent structural breaks or regime changes.  Agents that are
sufficiently imaginative or privately informed about such cases may be able to
profit from the associated upheaval once they arrive. But novel cases that are
``independent'' of past cases (in the sense that they do not cause a
re-evaluation) should not generate arbitrage opportunities. This is the essence
of stability.

\emph{Algorithm for identifying the pricing kernel from past cases} Under
stable pricing, cases combine in a modular way, markets assimilate the
information in new types of cases without re-pricing the information of old
case. This is the role of the nonrevision property of generalizations (see
\cref{def-generalization}).  Price movements reflect the marginal information
of new cases. 

We now describe a classification method for deciding the nature of a new case
and the implications for market maker(s) who themselves act as analysts.
Consider the arrival of $j$ new cases ${c}_1, \dots, {c}_j$.  Under stable
pricing, the analyst takes new price movements to reflect the market view on
the information value of new cases.  That is, for each $i = 1,\dots, j$, the
analyst estimates the markup associated with case $c_i$ to be
$\hat{\mu}^{\{x,y\}} = b(x, \past \cup \{c_i\}) - b(x, \past) $.  This is
reasonable provided she already has used the history of prices to estimate
$b:X\times \mbbd \rightarrow \R$. Moreover suppose that, for each $i = 1,
\dots, k-1$, there exists $d\in \past$ such that for every $x,y \in X$,
$\hat{\mu}^{\{x,y\}}_{c_i} = \mu^{\{x,y\}}_{d}$.  She concludes that each of
the cases $c_1, \dots, c_{k-1}$ is a ``copy'' of some past case type. Then
$\past \cup \{c_1, \dots, c_{k-1}\}$ belongs to $\mbbd$ and the analyst's
market model is unchanged: all that has changed is the location of the current
sample $\past$ in $\mbbd$.

Case $c_{k}$ is different. The analyst finds that price movements are such
that, for every $d \in \past$ (and hence every case in $\mbbc$),
$\hat{\mu}^{\{\cdot,\cdot\}}_{c_{k}} \neq \mu^{\{\cdot,\cdot\}}_{d}$.  This
reveals $c_{k}$ to be a new type of case.  The analyst then {generalizes} her
model of the market to let $\grave{\mbbc}$ and $\grave{\mbbd}$ to respectively
be the new sets of all cases and finite resamplings that she can generate from
$\grave{D} = \past \cup \{c_{k}\}$. Assuming no-arbitrage/prudent pricing,
there no need for her to update $\mu^{\{\cdot,\cdot\}}_{d}$ for $d \in \past$.
That is, the new market pricing function $\grave{b}: X \times \grave{\mbbd}
\rightarrow \R$ satisfies $\grave{b}(\cdot, D) = b(\cdot, D)$ for every $D \in
\mbbd$. It is in this sense that markets deliver efficiency via prudent
pricing.

Now consider two possibilities for $c_{k+1},\dots,c_{j}$. The first is that the
price movements associated with all of these cases are similar to cases in
$\grave{\mbbd}$. That is, for each $i=k+1,\dots,j$, there exists $d\in
\grave{D}$ such that $\hat{\mu}^{\{\cdot,\cdot\}}_{c_{i}} =
\grave{\mu}^{\{\cdot,\cdot\}}_{d}$. This is what we would expect if new case
types arrive infrequently. It would represent a consolidation of her updated
model $\grave{b}$ and $\grave{\preceq}_{\grave{\mbbd}}$.

The second possibility is where many of the cases $c_{k},\dots,c_{j}$ turn out
to be novel.  If new case types do indeed arrive infrequently, then it is
likely to reflect a re-evaluation of cases in $\past$ and a structural break
from the past. In this scenario, it may well be worth checking to see if a
re-evaluation or even re-specification of past cases reduces the number of case
types: thus generating a more parsimonious model.
%-----------------------------------------------------------------------------%
\section{Discussion of the axioms and \cref{thm-main}}\label{sec-discussion}
%-----------------------------------------------------------------------------%
We begin by restating the existence part of the main theorem of \gsii.
%\setcounter{theorem}{-1}
\begin{theorem*}[\gsii, existence]\label{thm-gsii} Let there be given $X$,
  $\mbbc$ and $\preceqb_ \mbbd$, as above, such that the richness condition
  holds. Then \ref{ax-gsii} and \ref{wrap-gsii} are equivalent.
  \begin{enumerate}[label=\textup{(\roman*)}]
    \item\label{ax-gsii} \textup{\ref{T}--\ref{A}} and \textup{\fourdiv} hold
      for $\preceq_{\mbbd}$.
    \item\label{wrap-gsii} There exists a matrix $\mathbf{v} : X \times \mbbc
      \rightarrow \R$ satisfying \ref{rep-gsii} and \ref{rows-gsii}$\,:$
      \begin{enumerate}[label=\textup{(\alph*)}]
        \item\label{rep-gsii} $\mathbf{v}$ is a representation of
          $\preceq_{\mbbd}$ that respects case equivalence$\,;$
        \item\label{rows-gsii} For every four distinct elements $x,y,z,w \in X$
          and every $\lambda , \mu, \theta \in \R$ such that $\lambda +\mu +
          \theta = 1$, $\mathbf{v}(x,\cdot ) \not \leq \lambda
          \mathbf{v}(y,\cdot
          )+\mu
          \mathbf{v}(z,\cdot)+ \theta \mathbf{v}(w,\cdot)$.  For $\countof X <
          4$, no row is dominated by an affine combination of other rows.
      \end{enumerate} 
  \end{enumerate}
\end{theorem*}

Although diversity axioms play an important technical role, they are not
obviously behavioral. Instead, diversity axioms impose restrictions on what is
beyond the learner's control and on what is central to inductive inference:
experience. We contend that $\past$ may not be so rich as to support
$\preceqb_{\mbbd}$ satisfying \fourdiv. That is to say, there may exist $Y
\subseteq X$ such that $\countof Y = 4$, and such that the data is
insufficiently rich to support all $4 ! = 24$ strict rankings.

It is natural to ask whether \stability\ is simply requiring that, on $Y$ such
that $\preceq_{\mbbj}$ fails to satisfy \fourdiv, there exists a testworthy
$Y$-generalization that is novel and satisfies \fourdiv.  This is not the
case.  When the basic axioms hold, \fourdiv\ implies that the current ranking
map features a centered arrangement of hyperplanes (for every $Y\subseteq X$ of
cardinality four). In contrast, the axioms of \cref{thm-main} do not even imply
the existence of a centered generalization.  (See for instance
\cref{fig-hasse-centerless-Y4} of \cref{sec-online}.) It is well-known in the
literature on hyperplane arrangements that numerous complexities arise when the
arrangement is uncentered.

A casual comparison of condition \ref{rows-main} and \ref{rows-gsii} confirms
that the present framework accommodates the less experienced or equivalently
those settings where the data is not rich.  By doing so, we have identified an
important role for the imagination and in particular, a prudent imagination.
Our results show how inexperienced learners that are prudent can survive the
initial phase before going on to become experienced learners in their own
right. We have established that there is more than one kind of stable learner.
That is to say, whereas experienced (\fourdiv) learners are stable, so are
those that prudently appeal to external validation: either via their
imagination or via leave-one-type-out cross validation.

We have shown that, even in the absence of prudent learners, provided the
market structure allows agents to exploit arbitrage opportunities, market
pricing is prudent. By this we mean that it is as if a prudent market maker
were guiding the price formation process. This novel form of efficiency is
grounded in inductive inference. Moreover, it implies a modular nature to the
way information is built into market prices. It implies stability in the value
of information in past cases. 

%\emph{Comparing the complexity of \ref{c2d}\ and
%\fourdiv,} in the presence of the other axioms, provides a measure of
%the value of experience 
%As in \cref{sec-applications}, take $X$ and $\mbbt$ to be of cardinality $m$ and $n$
%respectively. 
%
%As an estimate, we compare condition \ref{rows-main}
%with \ref{rows-gsii} of Gilboa and Schmeidler's theorem.  Verifying
%\ref{rows-gsii} involves checking $n$ affine dominance constraints: one for
%each case type.  This is well-known to be equivalent to the complexity of
%linear programming with real variables \textup{\citep{DR-Linear_programming}}.
%In the absence of knowledge regarding the sparsity of $\mathbf v$, the fastest
%algorithm for achieving this is of order $n^{3}$
%\textup{\citep{LS_Linear_programming}}.  Since this holds for every subset of
%four distinct eventualities, checking \ref{rows-gsii} is of order
%$\binom{m}{4}n^{3}$.  In contrast, verifying $\mathbf{v}(x,\cdot )\not \leq
%\mathbf{v}(y,\cdot)$ takes at most $n$ steps for each of the $\binom{m}{2}$
%subsets of $2$ distinct elements. Likewise, for every three distinct elements
%$x,y,z$ in $X$, checking for noncollinearity of two vectors takes at most $n$
%steps. Thus, a na\"{i}ve algorithm for checking \ref{rows-main} is of order
%$\binom{m}{3}n$. Even for $m = 4$ and $n = 4$, the difference is stark:
%$\binom{m}{3}n = 16$ versus $\binom{m}{4} n^3 = 64$. (The threshold $4$ is
%important as we have shown in \cref{prop-fourdiv-empty} below.)
% % \footnote{In fact one way to check if \ref{rows-main}
%    % holds is to proceed as follows. Assume we have an enumeration of $\mbbt
%    %$. Let $t$ be the first member of $\mbbt$ that satisfies
%    % $\mathbf{v}(x,t)-\mathbf{v}(y,t) \neq 0$ and record this value. (If no
%% such
%    % $t$ exists, then \ref{rows-main} fails to hold.) Next, for each
%    % $z\in X\bs\{x,y\}$, solve for $\lambda\in \R$ such that
%    % $\mathbf{v}(x,t) - \mathbf{v}(y,t) =
%    % \lambda(\mathbf{v}(x,t)-\mathbf{v}(z,t))$. Next, stop the algorithm at
%% the
%    % first $t'>t$ such that either
%    % $(\mathbf{v}(x,t)-\mathbf{v}(y,t))\cdot(\mathbf{v}(x,t')-\mathbf{v}(y,t'))<
%    % 0$ or
%    % $\mathbf{v}(x,t') - \mathbf{v}(z,t')\neq \lambda
%    % (\mathbf{v}(y,t')-\mathbf{v}(z,t'))$. If, the algorithm reaches step $m$
%    % without finding such a $t'$, then \ref{rows-main} holds.  Thus,
%% algorithms
%    % for checking \ref{rows-main} are of order
%    % $O\hskip-1pt\left((m-1)\binom{m}{2} n\right)$.}
%
%\emph{In experimental settings, a robust test of {stability}} is
%available precisely when \ref{T}--\ref{c2d}\  hold and \fourdiv\ fails. For, when
%$n<\infty$, the existence of a similarity representation satisfying
%\cref{wrap-main} is equivalent to \stability. The test is robust in that,
%generically, learners that fail to check for the arrival of new cases also
%fail to satisfy \cref{wrap-main}. This is because, for every $x,y,z \in X$, the
%row differences $v^\xy := - \mathbf{v}(x,\cdot) + \mathbf{v}(y,\cdot)$ satisfy
%the Jacobi identity $v^{\xz} = v^{\xy} + v^{\yz}$. These latter three vectors
%span a linear space of dimension at most two. This in turn implies that the
%hyperplanes $H^{\{x,y\}}, H^{\{y,z\}}$ and $H^{\{x,z\}}$, to which the row
%differences are normal, are congruent. Congruence implies the hyperplanes are
%not in general position.  In other words, there is a zero (conditional)
%probability of the learner striking lucky and appearing to be prudent when
%they are in fact are not. 
%
%\emph{Novelty, memory and transitivity.} An early test and taxonomy of
%intransitive behavior is due to
%\citet{Weinstein-Intransitivity,Weinstein-Transitivity}. Weinstein points out
%that intransivity can sometimes be rational in complex situations.
%Interestingly, he shows that young are people significantly less transitive in
%their choices.  He also points out that the law itself is designed to
%accommodate irresponsible under-age decisions. In the psychology literature,
%\citet{BR-Novelty_and_intransitivity} show that presenting novel objects is
%more likely to trigger intransitive choices. 
%
%More recently, \citet{Enkavi-Hippocampal_dependence} provide evidence that
%people with a specific form of memory impairment (lesions in the hippocampus of
%the brain) are significantly more likely to violate transitivity in pairwise
%choices of chocolate bars: even though they rank numbers
%transitively.\footnote{The hippocampus is associated with learning and memory.
%\Citet{Hassabis-Hippocampal_dependence} and
%\citet{Schacter-Hippocampal_dependence} present evidence showing that the
%hippocampus plays an important role in imagining future experiences on the
%basis of past ones. \Citet{Enkavi-Hippocampal_dependence} go further by showing
%that it plays a role in the value-based decision making framework of
%\citet{Rangel-Value-based_neurobiology}. }
%
%In line with the case-based approach, the experimental evidence of
%\citet{Enkavi-Hippocampal_dependence} suggests that agents are constructing
%their preferences on the basis of past experience. Moreover, it seems natural
%to interpret agents with hippocampal impairment as inexperienced learners.
%The fact that impaired agents then make intransitive decisions is in line with
%what our model predicts as they are, in effect, facing a novel situation and
%are required to construct their preferences on the fly. It appears that
%impaired agents are also failing to be prudent, though it is not obvious that
%choices among chocolates warrant the additional neural computation that
%accompanies \stability.
%
%
%Whilst this literature does not offer
%a direct test of the present model, it supports the case-based framework of
%constructing preference as well as our premise that violations of transitivity
%by situations that require imagination or equivalently situations where the
%memory is impaired.
%  % In particular, \citet{Enkavi-Hippocampal_dependence}
%% show that people with hippocampal lesions are more likely to make
%% intransitive
%% consumption choices: even though they transitively rank numbers. For
%% instance, faced with
%% pairwise choices among chocolate bars (to be consumed at a future point in
%% time), they are more likely to choose Snickers over Mars; Mars over Bounty;
%% but
%% Bounty over Snickers.
%
%%\begin{remark*}
%%  A closer look at the relationship between the proportion $\rho$ of
%%  hippocampal impairment and the percentage $\sigma$ of intransitive choices
%%  \citep[in figure 2 of][]{Enkavi-Hippocampal_dependence} suggests another
%%  interpretation.
%%  For $\rho$ above $\frac{1}{4}$, $\sigma$ is above $20\%:$ twice as high as
%%  it is for $\rho < \frac{1}{4}$. Memories and the rankings they generate are
%%  latent variables to the observable $\rho$ and this threshold is where
%%  \ref{c2d}\ fails to hold and our model breaks down. The $16$ cases in the
%%  data are thus partitioned into three groups$:$ two that satisfy \threediv\
%%  ($\rho < 0.05$ and $\sigma< 5\%$)$;$ $12$ intermediate cases that satisfy
%%  \ref{c2d}\ ($0.05 \leq \rho \leq 0.25$ and $5\%\leq \sigma \leq 10\%$)$;$
%%  and
%%  $2$ severe cases that fail to satisfy \ref{c2d}\ ($0.25 < \rho$ and
%%  $10\% < \sigma$).
%%\end{remark*}
%
%
%\emph{The success or failure of startups. } Inexperience raises
%significant barriers to entry. Overcoming these barriers is either the result
%of making mistakes and learning by doing ``on the fly'' or the result of being
%prudent. Which form of induction bears out in practice will depend on many
%factors.  Prudent learners are more likely to survive the initial phase of
%inexperience before they acquire a rich database. It will be interesting to see
%if ChatGPT is prudent enough to survive long enough to challenge the vast
%experience of Google.
%
%%The following proposition confirms our thesis that experienced learners (\ie\
%%those that satisfy \fourdiv) have indeed encountered a high number of case
%%types. As the main theorem of \gsii\ shows, experienced learners have no need
%%for the additional structure of generalizations. Unless the prediction problem
%%changes (\eg\ new eventualities become relevant),  they have no need to engage
%%in second order induction. This saving in cognitive effort is the prize that
%%experience confers.
%%
%%\begin{theoremEnd}{proposition}[experience and case
%%  types]\label{prop-fourdiv-empty}
%%  If $\preceqb_{\mbbd}$ satisfies \ref{K}--\ref{A} and \fourdiv, then $n \geq
%%  \min \{4, m\}$, and, for every $Y \subseteq X$ of cardinality $m'$ and
%%  regular $Y$-generalization $\ext$, the number $n'$ of equivalence classes of
%%  $\sim^{\ext}$ satisfies $n' \geq \min \{4,m'\}$.
%%  \end{theoremEnd}
%%  \begin{proofEnd}%[Proof of \cref{prop-fourdiv-empty}]
%%    Via \cref{lem-axiomsQ}, \ref{K}--\ref{A} and \fourdiv\ hold for
%%    $\preceq_{\mbbd}$ if and only if \ref{KQ}--\ref{AQ} and \fourdiv\ hold for
%%    $\preceq_{\mbbj}$. Let $Y\subseteq X$ be of cardinality $m' = 1, 2, 3$ or
%%    $4$ and let $\ext$ be a regular $Y$-generalization. Via \cref{lem-insep}, there
%%    exists a pairwise representation $v^{\dd}$. For $m' = 1$, $n' = 1$ because
%%    $\ext_{J}$ is constant on $\mbbjp$.  For $m' = 2$, $n' \geq 2$, since via
%%    part \ref{D-insep} of \cref{lem-insep}, $G^{\xy}$ and $G^{\yx}$ are both
%%    nonempty.
%%
%%    By way of contradiction, first suppose $n' = 2$ and $m' \geq 3$. Via
%%    \cref{eg-zaslavski} of appendix \ref{sec-proof-mainQ},
%%    $\total(\ext) \leq 4$. In contrast, \fourdiv\ requires $\total(\ext) = 6$.
%%    The remaining case is where $n' = 3$ and $m' \geq 4$. If the rank
%%    $\mathbf r$ of $v^{\dd}$ satisfies $\mathbf r \geq 3$, then the kernel
%%    $A^{Y}$ of $v^{\dd}$ is zero-dimensional. Then $0$ is the unique element of
%%    $A^{Y}$. Thus, the positive kernel $A^{Y}_{\mpplus}$ of $v^{\dd}$ is
%%    empty. Then Zaslavski's theorem implies that $\total(\ext)< 4!$, so that
%%    \fourdiv\ fails to hold.  If $\mathbf r \leq 2$, then an application of the
%%    rank version Zaslavski's theorem (in particular \cref{eq-zaslavski-4} with
%%    $\acute{\mathbf r} = \mathbf r = 2$) yields
%%  \begin{linenomath*}
%%    \begin{equation*} \lvert \mc G_{\mpplus} \rvert \leq 1 - 6 + 15 - 20 + 15 + 6 + 1 =
%%      12. \end{equation*}
%%\end{linenomath*}
%%Thus, once again \fourdiv\ fails to hold. Thus $n' \geq \min \{4, m'\}$, as
%%required.  Finally, since $Y\subseteq X$, $m\geq m'$, and, via part
%%\ref{item-dimension} of \cref{def-generalization}, $n \geq n'$.
%%\end{proofEnd}
%%In contrast, \ref{c2d}\ implies no restrictions on the cardinality of
%%$\mbbt$ beyond $n \geq 2$ and this is also a virtue of \twodiv.
%
%
%% Related to this discussion is Quine's observation on scientific practice:
%% \begin{quote}``The experimenter settles for criteria that he can confidently
%%   adjudicate on the spot: confidently, not infallibly. He looks carefully, and
%%   if he is subsequently in doubt he repeats the experiment if he can. The finality of an
%%   experiment is historical not logical.''
%% \end{quote}
%% To which which we add that, if neither the experiment cannot be repeated and the
%% experimenter lacks confidence, then he may still look at the implications of      Thus, the present framework provides a
%% way of checking that the existing specification of the entire model (including
%% the definition of cases) does not generate violatations of the basic axioms when
%% second-order information arrives. In the proof of \cref{thm-main}, imprudent
%% learners that satisfy \ref{T}--\ref{A} and \ref{c2d}\ are represented the
%% similarity function $\sum_{c \,\in D} v^{\xy}(c)$ that is additive, but
%% inseparable in $x$ and $y$. Yet inseparability on $X$ and nonlinearity on
%% $\mbbd$ are not mutually exclusive.\footnote{With additional regularity
%%   conditions (see \citet{OCallaghan-Parametric_continuity}), such learners are
%%   also represented by a nonlinear function $u: X \times \mbbd \rightarrow \R$
%%   such that for every $D \in \mbbd$ and every $x,y \in X$, $x \preceq_{D} y$
%%   if, and only if, $u(x,D)\leq u(y,D)$.}% This
%% remains true when axioms \ref{C}, \ref{A} and \ref{c2d}\ hold. In which case
%% $\preceq_{\mbbd}$ is also represented by a . It is only when \ref{T} fails that the above $u$ will fail to exist.
%
%
% % The fact that on the model offers a way of certifying that she is not By
% %  recast the problem a the forward-looking startup understands that the domain
% %  of the similarity function will change and hence that, when a novel case
% %  arrives, the function itself will change. Unless the startup is prudent, she
% %  may end up generating intransitive (or perhaps random) SERPs. Or, worse still,
% %  in order to avoid intransitivities, the startup may become dogmatic and rule
% %  out good rankings. Both of these situations are described by a similarity
% %  function $\sum_{c \,\in D} v^{\xy}(c)
% % $ that is additive, but inseparable in $x$ and $y$.
%  
%\emph{When is {stability} worth the trouble?} The simple answer to this
%question is: when revising a model ``on the fly'', once a novel case arrives,
%is costly. We argue that in the setting of financial markets where a lack of
%{stability} can lead to arbitrage opportunities, {stability} will emerge as optimal
%behaviour with more prudent agents out-surviving others. 
%
%%\begin{example}\label{eg-arbitrage}
%% 
%%  Consider a fair market maker of \emph{Zeros} (zero-coupon treasury
%%  bonds).\footnote{Similar to a fair insurer, the fair market maker sets the
%%  market spread to zero.}
%%  The compound-interest formula for the accumulation
%%  process of such a bond is
%%  \begin{linenomath*}
%%    \begin{equation*}a^{\xy}= \left(1+r^{\{x,y\}}\right)^{-x + y}, \end{equation*}
%%  \end{linenomath*}
%%  where $r^{\{x,y\}}$ is the implied yield on a forward contract that accrues
%%  interest between dates $x$ and $y$. If $x$ is later than $y$, then the
%%  contract is to sell, and the market maker pays this yield, so that
%%  $r^{\{y,x\}} = r^{\{x,y\}}$. 
%%  Let $X \subseteq \R_{\mplus}$ index a suitable
%%  sequence of trading dates with $0 \in X$ being the spot date.  It is well
%%  known that a Zero that pays out one dollar at time $x > 0$ is arbitrage-free
%%  if, and only if, the log-accumulation process satisfies
%%  \begin{linenomath*}
%%  \begin{equation}\label{eq-no-arbitrage}
%%    \text{for every $x,y,z \in X$,}\quad   \log a^{\xy} = \log a^{\xz} +  \log a^{\zy}
%%    \,.\footnote{To see this, suppose that, for some
%%      $x< y$, she sets $a^{\xy} < a^{\xz}a^{\zy}$.  Another trader would do well
%%      to sell the forward contract $\xy$, buy the spot contract $\xz$ and sell the
%%      spot contract $\zy$.  A risk-free arbitrage opportunity is also
%%      available if the reverse inequality holds.}
%%  \end{equation}
%%\end{linenomath*}
%%This no-arbitrage condition is a special case of the Jacobi
%%identity. We now explain how a market maker might infer the
%%accumulation process from past cases.
%%
%%  \end{example}
%
%
%\emph{Discussion of second-order induction.}\vskip-8pt
%%The market maker of
%%\cref{eg-zeros,sec-applications} engages in second-order induction when she acts
%%prudently.
%%She reflects on her model by checking that the basic axioms of \gsii\ will
%%continue to hold when a novel case arrives. By way of contrast, suppose the
%%bond price of the market maker is such that $\preceq_{\mbbd}$ is consistent
%%with the basic axioms, but not \stability. Then when a novel case arrives, she
%%may be exposed to arbitrage and need to respecify her entire model ``on the
%%fly''. Such a step corresponds to the intermittent respecification of her
%%similarity weighting function $\mathbf{v}(x,c)$, as
%%\citet[p.10324]{argenziano2019second} describe. 
%In \citet{argenziano2019second}, the ``leave-one-out'' technique of
%cross-validating the model by omitting a case of each type is intuitively and
%operationally close to our inclusion of the free case $\novel$. The difference
%is that by introducing a degree of freedom, our market maker can use her
%imagination to study novel generalizations and peer into the future through
%the lens of her current model. She can exploit the intervals of time inbetween
%the arrival of novel cases by continuously engaging in second-order induction.
%
%Through an example, we now show that the present framework provides the
%  flexibility to accommodate second-order induction without sacrificing the
%  computational or normative advantages that additive similarity functions
%  provide.
%
%    % Agents may be comfortable with familiar theories and sets of empirical
%    % observations, but less so when combining their present model with a new one.
%    % (By way of analogy, in probabilistic contexts, it is often the case that
%    % joint distributions are poorly understood.\footnote{See, for instance, the
%    %   motivation to \citet{Carroll-Multidimensional_screening} in a
%    %   multidimensional-screening setting.})  The present framework is therefore
%    % geared towards settings where theories are combined over timeand the dynamics of
%    % learning matter.
%
%    % We restaurant example of second-order induction in light of
%    % the present framework.
%  \begin{example*}[second-order induction, \gsii, p.12] \label{eg-john_mary}
%    Let $c$ denote a case where Mary chooses restaurant $x$ over restaurant
%    $y$. In the absence of any further information, it is tempting to assume
%    some similarity between John and Mary. The learner then finds it plausible
%    that John prefers $x$ to $y$ given $\{c\}$. A separate database $D$
%    contains no choices between $x$ and $y$. Thus, in the absence of further
%    information, $x$ and $y$ appear equally likely based on $D$. Additivity of
%    the similarity function (or \ref{C})) implies it is plausible that John
%    prefers $x$ to $y$ given $\{c\} \cup D$. The violation of \ref{C} arises
%    when a more careful examination of the contents of $D$ reveals many choices
%    between other pairs of restaurants where John and Mary consistently differ.
%\end{example*}
%Quine's notion of perceptual similarity \citep{Quine-Roots_of_reference} offers
%a check on the learner's inference about John's choice given $\{c\}$. John and
%Mary may just as well be two drivers passing through an intersection at
%different times. Although their situations are broadly speaking very similar,
%if one faces a red light and the other a green light, their responses will
%differ. In the restaurant setting, some pivotal information is omitted from
%$c$.  Observing that the evidence in $c$ in favour of $x$ over $y$ is somewhat
%weak, a prudent learner instead recasts $\{c\}$ as a database $C$ that combines
%past observations with copies of the pivotal novel case $\novel$.  With a more
%refined model, that explicitly allows for omitted variables, the learner can
%check to see if her model {generalizes} to higher dimensions without violating
%the basic axioms.
%
%The same argument applies to nonlinear machine learning models. We recall the
%``kernel trick'' that extends the linear framework to the nonlinear by
%increasing the dimension of the problem and casting a nonlinear model as a
%linear one in the higher-dimensional space. With a suitable change of
%variables, a three-dimensional space allows for binary classification of
%vectors according to whether they lie in the unit circle using a linear kernel.
%
%%\begin{remark}
%%
%%  Predictors that satisfy \ref{T} and \ref{K}, but not \ref{C} can, with some
%%  additional regularity conditions, still be represented by a nonlinear
%%  function $u: X \times \mbbd \rightarrow \R$ such that for every $D \in \mbbd$
%%  and every $x,y \in X$, $x \preceq_{D} y$ if, and only if, $u(x,D)\leq u(y,D)$
%%  \citep[see][]{OCallaghan-Parametric_continuity}.  Similarly, learners that
%%  satisfy \ref{T}--\ref{c2d}\  but not \stability\ can also be represented by such
%%  a function: because $\preceq_{\mbbd}$ is complete and transitive for each $D
%%  \in \mbbd$. The present framework allows us to disentangle the latter kind of
%%  learner from those who, for good reason, fail to satisfy the combination
%%  axiom. (See \gsii\ for examples of such reasons.)
%%
%%\end{remark}
%% \begin{remark}(To go somewhere else)\label{rem-why-pru}
%% The main contribution of this paper is to show that, in the case where the
%% learner engages in higher-order sampling and explores novel generalizations,
%% this
%% absence of rich data does not preclude a similarity representation of the
%% consistent form that \gsii~derive. This is feasible provided the learner is
%% prudent enough to check that the arrival of novel cases will not force her
%% into the dilemma of choosing between being dogmatic and violating
%% transitivity.
%% \end{remark}
%% \begin{remark}
%% The reason we define testworthiness in terms of pairwise extremal total
%% suborders is that the alternative is less amenable to interpretation as a
%% behavioral condition. This is because the maximal novel generalization is
%% dependent on the number of case types. Defining testworthiness in terms of
%% maximal generalizations would work well in non-human agent settings.  The concept
%% of the order reversal or inverse ordering is familiar and captures the idea
%% of
%% being far from an existing order. It captures the idea that being prudent
%% requires that one should explore beyond one's comfort zone, beyond what one
%% can identify with the data. We argue that it is procedurally similar to what
%% an inexperienced agent would do in practice.  Moreover, for every quadruple
%% of
%% eventualities, maximum number of orders one has to test is at most
%% $4!/2=12$. And, in fact, if one generalization passes the test (in the sense of
%% the
%% {stability} axiom), then, as we show, so do they all.
%
%% \end{remark}
%
%% When $\current$ is sufficiently rich, that, in addition to the basic axioms,
%% $\preceqb _ \mbbd$ satisfies \fourdiv, the learner need not engage in
%% higher-order sampling or testing of novel generalizations. Testing, that is, to
%% see
%% if the arrival of novel cases will force her to either be dogmatic (and
%% exclude
%% accurate plausibility rankings/predictions) or violate transitivity. We will
%% assume the following holds for $k = 4$.
%
%
%% This resembles a robust version of two-pass methods
%% in econometrics: robust because only ordinal (ranking) information from the
%% initial step is retained when the similarity matrix is derived. In contrast
%% with
%% the standard bootstrap, since rankings are ordinal, this holds for every
%% sample
%% size.
%% Resampling in the present setting is akin to the familiar bootstrap except
%% that,
%% because ordinal rankings are primitive the sample size is not limited to that
%% of
%% $D^{\star}$. 
%% \footnote{We thank Jurgen
%% Eichberger for his question on this matter at the RUD Conference in
%% Heidelberg
%% 2018.}
%% is stored and used in the construction of the
%% similarity representation.\footnote{The standard reference for two-pass
%% methods
%% in finance is \citet{FM-Two_pass}.}
%
%
%\emph{The veracity of false news.} How should a learner check whether her
%model consistently {generalizes} to higher dimensions (when novel cases
%arrive)?  Our model and proof suggest that the most useful rankings that she
%might wish to examine are those that are far from her own. This is because
%testworthy generalizations involve assigning to the novel case $\novel$ the
%inverse of some total ranking $\preceq_{D}$.  This offers a rationale for why
%information that differs from our own is intrinsically valuable. Testworthy
%generalizations play a vital role in taming the complexity of our proof. It
%is plausible that something similar may be at play when agents encounter
%radically different information from their own on social media: \emph{even if
%it is fake}. This supports the evidence that false news is significantly more
%veracious than real news online \citep{Vosoughi-Roy-Aral-Veracity}. The fact
%that real news is typically closer to what we have observed in the past means
%that it is of less value to learners that find it costly to imagine worlds that
%are far from their own. \Cref{cor-efficiency} provides additional support for
%this argument.
%\bibliography{stable-learning-jabref.bib}
% if your bibliography is in bibtex format, uncomment commands:
\bibliographystyle{ecta-fullname} % Style BST file
\bibliography{stable-learning-jabref.bib} % Bibliography file (usually '*.bib')

\makeatletter
\def\@seccntformat#1{Appendix\,\csname the#1\endcsname.\quad}
\makeatother
\begin{appendix}
  %---------------------------------------------------------------------------%
  \section{The proof of \cref{thm-main}}\label{sec-proof-main}
  %---------------------------------------------------------------------------%
  % Steps of the proof

  % The proof proceeds as follows.  In the first step of the proof 

  % Then, by the richness condition and
  % the fact that $\sim^\star$ is an equivalence relation, there exists $% \textup {T} \in \mc T$ such that the set $\mbbc _ {/ \sim^\star}$ of
  % equivalence classes of $\sim^\star$ in $\mbbc$ satisfies $\mbbc _ {/
  % \sim^\star} = \textup{T}$.  In the second, ***


   %step-integers
  Similar to \gsii, we  translate the model into one where databases are
  represented by vectors, the dimensions of which are case types. To allow us
  to focus on aspects of the present model,  proceed directly to rational
  vectors and present the axioms  and a corresponding theorem
  (\cref{thm-mainQ}) which, as we confirm, holds if, and only if, 
  \cref{thm-main} does. The proof of \cref{thm-mainQ} can be found in
  \cref{sec-proof-mainQ}.


  \emph{Case types as dimensions.} From our definition
  of case types in  \cref{sec-model}, $\mbbt = \mbbc_{/\sim^{\star}}$ and
  $\mbbt^\novel \defeq \mbbt\cup [ \novel ]$. Let $\mbbtpp$ be a free variable
  in $\{\mbbt , \mbbtp \}$.  When no possible confusion should arise, we use
  $\novel$ as shorthand for $[ \novel ]$.  It is straightforward to show that
  the following construction would work if instead we were to work with any
  partition $\textup T$ of $\mbbc$ that is at least as fine as $\mbbt$.
  The present construction is the one with the lowest feasible number
  $\countof \mbbt$ of dimensions.

  % Let $\mbbip\upiota( \mbbt )$  and $\mbbi  ' _{\bullet}$  be the corresponding
  %subsets of $\nnint^\types$.
  %\Cref{obs-Regular} implies that the same is true of $\mbbd _{/\sim^
  %\supext}$ for every $\ext  \in \reg ( X , \preceqb _ \mbbd )$.
  %rankings counting vectors

  \emph{Translation to  counting vectors.} Let $\nnint$ denote the set of
  nonnegative integers and $\posint$ those that are (strictly) positive.  Let
  $\mbbi \subseteq \nnint^{\mbbt}$ denote the set of counting vectors $L :
  \mbbt \rightarrow \nnint$ such that $\lbc t : L ( t ) \neq 0 \rbc$ is finite
  and let $\mbbip$ denote the corresponding subset of $\nnint^{\mbbtp}$. Then
  let
  \begin{linenomath*}
    \begin{equation*}
      \text{$\mbbipp$} = \left\{
        \begin{array}{ll}
          \mbbi & \text{if, and only  if, $\mbbtpp= \mbbt$, and}\\
          \mbbip  &\text{otherwise.}
      \end{array}\right.
    \end{equation*}
  \end{linenomath*}
  Modulo notation, the following construction is identical to \gsii.  For every
  $D\in \mbbd$, let $L_{D}: \mbbt \rightarrow \nnint$ denote the function $t
  \mapsto L_{D}(t) = \countof (D \cap t )$.  For each $D \in \mbbd$, let
  $\preceqb_{L_{D}} \defeq \preceqb_{D}$.  We need to establish that $\preceqb
  _{\mbbi} \defeq \langle \preceqb _{L} : L \in \mbbi \rangle$ is well-defined.
  For every $L\in \mbbi$, the richness assumption (on $\mbbtp$) guarantees the
  existence of $D\in \mbbd$ such that $L_{D}= L$. By definition, $\sim^{\star}$
  is such that, for every $C,D \in \mbbd$, $C\sim^{\star}D$ if, and only if,
  $L_{C}= L_{D}$. Straightforward mathematical induction on the cardinality of
  $C$ shows that $C\sim^{\star} D$ implies $\preceqb_{C} = \preceqb_{D}$.  This
  construction of $\preceqb_{\mbbi}$ ensures that the same notion of
  equivalence that we introduced in \cref{obs-reg-eq} also applies here. Thus,
  $\preceqb_{\mbbi} \equiv \preceqb _{\mbbd}$.

  %rational vectors
  \emph{Translation to rational vectors.} Similarly, let $\nnrat$ denote
  the nonnegative rationals and $\posrat$ those that are (strictly) positive. 
  Take $\mbbj \subseteq \nnrat^{\mbbt}$ to be the set of rational vectors with
  $\lbc t \in \mbbt : J ( t ) \neq 0 \rbc$ finite and take $\mbbjp$ to denote
  the corresponding subset of $\nnrat^{\mbbtp}$. For each $J \in \mbbj$, by
  virtue of the fact that $\posint$ is well-ordered and $J$ has finite support,
  there exists (unique) minimal $k_{J} \in \posint$ such that $L_{J} \defeq
  k_{J} J$ belongs to $\mbbi$.  Let $\preceqb_{J}\defeq \preceqb_ {L_{J}}$.
  (This definition acquires meaning below once we translate and apply the
  combination axiom.)  In this way, $\preceqb _{\mbbj} = \langle \preceqb_ J :
  J \in \mbbj \rangle$ is well-defined, and we may introduce axioms for
  $\preceq_{\mbbj}$ directly: \ie\ without first introducing axioms for
  $\preceq_{\mbbi}$. We first demonstrate that $\preceqb_{\mbbj}$ and
  $\preceqb_{\mbbd}$ are equivalent. First note that, for every $I,J\in \mbbj$
  such that $L_{I} = L_{J}$, $\preceqb_{I}=\preceqb_{J}$. Then, let $L^{\prime}
  = L_{J}$ and take any $D$ such that $L_{D} = L^{\prime}$. Then $\preceqb_{J}
  = \preceqb_{D}$.  The reverse embedding follows by virtue of the fact that
  $\mbbi\subset \mbbj$. Thus, $\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$.

  \emph{Construction of generalizations of $\preceqb_{\mbbj}$.} We
  follow common practice by letting $2^{X}$ denote the collection of nonempty
  subsets $Y \subseteq X$. For each $Y \in 2^{X}$, we will denote the set of
  regular, novel and testworthy $Y$-generalizations (of $\preceqb_{\mbbd}$ or
  $\preceqb_{\mbbj}$) by $\reg ( Y , \cdot )$, $\nov ( Y , \cdot)$ and $\test (
  Y , \cdot)$ respectively. Recalling that every $Y$-generalization is either
  regular or novel, let $\Ext (Y,\cdot)$ denote the set of all
  $Y$-generalizations. We now clarify what it means to be a generalization
  of $\preceqb_{\mbbj}$.

  For each $t \in \mbbtpp$, we take $\delta_{t} : \mbbtpp \rightarrow \R$ to be
  the function satisfying $\delta_{t} ( s ) = 1$ if $s = t$ and $\delta_{t} (s)
  = 0$ otherwise. (When $\mbbtpp$ is finite, these are simply the basis vectors
  for $\R^{\mbbtpp}$.)  When we wish to emphasise that the vectors belong to in
  $\R^{\mbbtp}$, then, for each $\mbbtp$, we will write $\delta_{t}^{\novel}$.
  Let
  \begin{linenomath*}
    \begin{equation*}
      \text{$\mbbjpp$} = \left \{
        \begin{array}{ll}
          \mbbj & \text{if, and only if, $\mbbtpp = \mbbt$, and} \\
          \mbbjp & \text{otherwise.}
      \end{array}\right.
    \end{equation*}
  \end{linenomath*}
  For every $I \in \mbbj$ and $J \in \mbbjpp$, we write $I \equiv J$ whenever
  $I
  = J$ or $J = I\times 0$. (In the latter case, $J(t)= I(t)$ for every $t\in
  \mbbt$ and $J(\novel )= 0$.) This notion reflects the fact that, for the
  purposes of the present model, such $I$ and $J$ are equivalent.
  \begin{definition}\label{def-generalizationQ} $\extb =\langle \extb_{J}: J
    \in \mbbjpp \rangle$ is a \emph{generalization} or a
    $Y$-\emph{generalization} of $\preceqb _{\mbbj}$ if, and only if, for
    some nonempty $Y \subseteq X$ both the following hold
    \begin{enumerate}
      \item for every $J\in \mbbjpp$, $\extb_{J}\in \relations(Y)$, $\nextb_{J}
        \defeq \extb_{J} \cap \extb_{J}^{-1}$ and $\sextb_{J} \defeq \extb_{J}
        \bs \extb_{J}^{-1};$
      \item for every $J \in \mbbj$ and $L \in \mbbjpp$ such that $J \equiv L$,
        $\extb_{L} = \preceqb_{J} \cap (Y^2)$. \label{item-preservingQ}
    \end{enumerate}
    A generalization $\ext_{\mbbjpp}$ (of $\preceq_{\mbbj}$) is proper if
    $\mbbjpp= \mbbjp$ and improper otherwise.  A proper generalization is
    either regular or novel. $\ext$ is novel if, for every $s \in \mbbt$, there
    exists $I$ in $\mbbj$ such that, for $J = I \times 0$ (in $\mbbjp$), we
    have $\extb _ {J + \delta _{s}^{\novel}} \neq \extb _ {J + \delta _
    {\novel}^{\novel} }$\,.
  \end{definition} 
  For every regular $Y$-generalization $\ext$ of $\preceq_{\mbbd}$ such that
  $Y=X$, \cref{obs-reg-eq} implies $\extb\equiv \preceqb_{\mbbd}$. And, via
  $\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$ and transitivity of equivalence, we
  conclude that $\ext$ is equivalent to $\preceq_{\mbbj}$.  Two sets of
  generalizations are isomorphic if there exists a canonical isomorphism
  between equivalent generalizations. 
  \begin{theoremEnd}[no link to proof]{lemma}[proof on \cpageref{proof-nov-iso}]
    \label{lem-nov-iso} For every $Y \in 2^{X}$,
    $\reg(Y, \preceqb_{\mbbj})$ is isomorphic to $\reg(Y,\preceqb_{\mbbd})$ and
    $\nov ( Y , \preceqb_{\mbbj} )$ is isomorphic to $\nov ( Y , \preceqb _
    \mbbd )$.
  \end{theoremEnd}
  \begin{proofEnd}%[Proof of \cref{lem-nov-iso}]
    \label{proof-nov-iso} We show that there exists a canonical embedding (a
    structure preserving injection) of $\nov ( Y , \preceqb _ \mbbj )$ into
    $\nov ( Y , \preceqb _ \mbbd )$. The fact that this map is also surjective
    follows from the fact that $\nov ( Y , \preceqb _ \mbbd )$ can be embedded
    in $\nov ( Y , \preceqb _ \mbbj )$ in precisely the same way. The proof
    that the two sets of regular generalizations are isomorphic follows via a
    similar argument plus the observation that every $Y$-generalization is
    either regular or novel.

    Take $\extb \in \nov ( Y , \preceqb _ \mbbj )$ and define $\hextb = \langle
    \hextb_{C} : C \in \mbbdp \rangle$ via the property: for each $C \in
    \mbbdp$, $\hextb_{C}\defeq \extb_{J}$ if, and only if, $L_{C}= L_{J}$,
    where, as before, $t \mapsto  L_{C}(t)$ counts the number of cases of type
    $t$ in $C$ and $L_{J} = \kappa_{J} J \in \mbbip$ for some minimal
    $\kappa_{J} \in \nnint$. Now, for any $\extb ' \neq \extb$ in $\nov ( Y ,
    \preceqb _ \mbbj )$, there exists $J\in \mbbjp$ such that $\extb'_{J}\neq
    \extb_{J}$. If we define $\hext'$ analogously, so that it is equivalent to
    $\ext'$, then $\hextb ' \neq \hextb$. As a consequence, the canonical
    mapping $\extb\mapsto \hextb$ is injective. If we can show that $\hext$
    does in fact belong to $\nov ( Y , \preceqb _ \mbbd )$, then we have
    constructed the required embedding. The fact that $\hext$ satisfies
    \ref{item-binary-rel} and \ref{item-preserving} of
    \cref{def-generalization} follows immediately from
    \cref{def-generalizationQ}. The proof that \cref{item-dimension} of
    \cref{def-generalization} holds is as follows. Take any $c , c ' \in
    \mbbcp$ and $D \in \mbbdp$ such that $c \sim^\star c '$ and $c , c ' \notin
    D$. First, observe that $D \cup \{c\} \sim^\star D \cup \{c'\}$, and
    moreover, for some $t \in \mbbtp$ we have $c , c ' \in t$. Then, for every
    $t\in \mbbtp$, $\lvert D \cup \{c\} \rvert = \lvert D \cup\{c'\} \rvert =
    L$ for some $L \in \mbbip\cap \mbbjp$. Thus $\hextb _ {D \cup \{c\}} =
    \hextb _ {D \cup \{c'\}}$, as required for $\hext$ to be a generalization
    of $\preceqb _{\mbbd}$. Finally, via \cref{def-generalizationQ}, the
    definition of a novel generalization ensures that the induced equivalence
    relation $\sim^{\extb}$ on $\mbbcp$ satisfies $c \not \sim^{\extb} \novel$
    for every $c \in \mbbc$. Since $\sim^{\hextb}$ inherits this property,
    $\hext$ is novel.
  \end{proofEnd}
  \emph{Axioms and theorem.}  We restate the axioms for
  $Y$-generalizations $\ext$ of $\preceqb  _ \mbbj$.
  \begin{enumerate}[label=\textup{A\arabic*}$^\flat$] \setcounter{enumi}{-1}
    \item\label{TQ} For every $J \in \mbbjpp$, $\ext _ J$ is transitive on $Y$.
    \item\label{KQ} For every $J \in \mbbjpp$, $\ext _ J$ complete on $Y$.
    \item\label{CQ} For every $I , J \in \mbbjpp$, every $x , y \in Y$ and
      every $\lambda , \mu \in \posrat$, if $x \ext _ I y$  and $x \ext_{J} y$,
      then $x \ext_{\lambda I + \mu J} y ;$ moreover, if $x \sext_{I} y$ and $x
      \ext_{J} y$, then $x \sext_{\lambda I + \mu J}$.
    \item\label{AQ} For every $I , J \in \mbbjpp$ and every $x , y \in Y$ if $x
      \mathrel{\sext} _ J y$, then there exists $0 < \lambda < 1$ such that,
      for every $\mu \in \mbb Q \cap ( \lambda , 1 )$, $x \mathrel{\sext _{( 1
      - \mu ) I + \mu J}} y$.
  \end{enumerate}

  For $k = 2, 3, 4$, $k$-diversity is defined for generalizations of
  $\preceq_{\mbbj}$ in exactly the same way. We continue to use the term
  $k$-diversity in this setting.  The following are \ref{c2d}\ and
  \ref{p3d}\ respectively.
  \begin{condtwodivq*}
    % [label=\textup{C}\textit{2}\textup{D}$^{\natural}$]
    \label{c2dQ} For every three distinct elements $x , y , z \in Y$, one
      of the two subsets $\{J': x \prec_{J'} y \}$ and $\{J': y \prec_{J'} x\}$
      of $\mbbj$ contains both $I$ and $J$ such that $z \prec _{I} x$ and $x
      \prec _{J} z$. If $\countof Y = 2$, then \twodiv\ holds on $Y$.
  \end{condtwodivq*}
  \begin{parthreedivq*}
    \label{p3dQ} For every $Y^{\prime} \subseteq Y$ with cardinality $n =
      2$ or $3$, every $Y^{\prime}$-generalization $\ext$ of $\preceq
      _{\mbbj}$ is such that $\countof{\total(\ext)} \geq n$.
      % For every $Y \subseteq X$ such that $\countof Y = 2, 3$,
      % the regular $Y$-generalization $\ext$ of $\preceq_{\mbbj}$ is such that
      % $\countof Y\leq \countof \total ( \ext )$.
  \end{parthreedivq*}

  % def-testworthyZ and test()
  A proper generalization $\ext$ of $\preceqb _{\mbbj}$ is \emph{testworthy}
  if it satisfies \ref{KQ}--\ref{AQ} and, for some $J\in \mbbj$ such that
  $\ext_{J\times 0}$ is total, $\extb_{\novel} = \extb_{J\times 0}^{-1}$. Thus,
  for each $Y \in 2^{X}$, $\test ( Y , \preceqb _{\mbbd} ) \simeq \test ( Y ,
  \preceqb _{\mbbj })$. For any pair of generalizations $\ext$ and $\hext$,
  $\hext$ is a perturbation of $\ext$ if $\extb_{\novel} = \hextb_{\novel}$.
  Moreover, $\hext$ is a {{diverse}} perturbation if $\countof \total (\hext)
  \leq \countof \total (\ext)$.
  \begin{enumerate}[label=\textsc{iv}-\textup{S}$^{\flat}$]
    \item\label{PQ} For every $Y \subseteq X$ of cardinality $3$ or $4$, every
      testworthy $Y$-generalization of $\preceqb _{\mbbj}$ that is novel has
      a {{diverse}} perturbation that satisfies \ref{TQ}–\ref{AQ}.
  \end{enumerate}

   % lemma relating axiomsZ to axiomsQ
  The following result corresponds to claim 2 of \gsii. Its proof is a
  consequence of mathematical induction and the combination axiom.
  \vskip-15pt
  \begin{lemma}\label{lem-coneQ} If $\ext _ \mbbjpp$ and $\hext _ \mbbipp$ are
    equivalent and the latter satisfies \ref{C}, then for every $J \in \mbbjpp$
    and every rational number $q >0$, we have $\extb _{q J}  =  \extb _{J}$.
  \end{lemma}

  The fact that $\preceqb_{\mbbj}\equiv \preceqb_{\mbbd}$ immediately implies
  that $\preceqb_{\mbbj}$ satisfies \ref{TQ}, \ref{KQ} and \ref{c2dQ} if, and
  only if, the corresponding axiom holds for $\preceq_{\mbbd}$. In general, we
  have the following result, which then also yields the equivalence for the
  {stability} axiom.
  \begin{theoremEnd}[no link to proof]{lemma}[proof on
    \cpageref{proof-axiomsQ}] \label{lem-axiomsQ} For $\extb _{\mbbjpp} \equiv
    \hextb _{\dpp}$, $\ext _ {\mbbjpp}$ satisfies \ref{CQ}--\ref{AQ} if, and
    only if, $\hext_{\dpp}$ satisfies \ref{C}--\ref{A}.
  \end{theoremEnd}
  \begin{proofEnd}%[Proof of \cref{lem-axiomsQ}]
    \label{proof-axiomsQ} Fix $\extb_{\mbbjpp} \equiv \hextb_{\dpp}$ and assume
    that $\hextb_{\dpp}$ satisfies \ref{C}. We show that $\extb _ {\mbbjpp}$
    satisfies \ref{CQ}. Fix $x , y \in Y$ and $J \in \mbbjpp$ such that $x
    \ext_{J} y$ and $x \ext_{J'} y$. Fix $\lambda , \mu \in \posrat$ and let
    $\kappa$ be the smallest positive integer such that both $L \defeq \kappa
    \lambda J$ and $L ' \defeq \kappa \mu J '$ belong to $\mbbipp$.  Then, by
    \cref{lem-coneQ}, we have both $x \ext _{L} y$ and $x \ext _{L '} y$.
    Moreover, for $D, D'$ such that $L_{D}= L$ and $L_{D'}= L'$ , we have $x
    \hext _{D} y$ and $x \hext _{D '} y$ and, by \ref{C}, $x \hext _{D
    \cup D '} y$. Finally, since $L_{D} + L_{D'} = \kappa (\lambda J + \mu J
    ')$, one further application of \cref{lem-coneQ} yields $x \sext _{\lambda
    J + \mu J '} y$, as required for \ref{CQ}.

    The proof that ``\ref{C} implies \ref{CQ}'' is \emph{mutatis mutandis} a
    special case of the above argument and ommitted.  We now assume
    $\hextb_{\dpp}$ satisfies \ref{C} and \ref{A} and prove that $\extb
    _{\mbbjpp}$ satisfies \ref{AQ}.  Fix $x , y \in X$ such that $x \sext _{J}
    y$ for some $J \in \mbbj$ and take any $J ' \in \mbbjpp$.  Then, by the
    construction of $\extb _ \mbbjpp$, there exists $L, L ' \in \mbbi$ such
    that $j J = L$ and $j ' J ' = L '$ for some $j , j ' \in \posint$.  By
    \cref{lem-coneQ}, $\extb_{L} = \extb_{J}$ and $\extb _ {L '} = \extb _ {J
    '}$.  Moreover, by construction, for some $D$ and $D'$ such that $L_{D}=L$
    and $I_{D'}= L'$, $\hextb_{D} = \extb _ J$ and $\hextb _ {D'} = \extb _ {J
    '}$.  We therefore conclude that $x \hsext _{D} y$, so that \ref{A} implies
    the existence of $\kappa \in \posint$ and $\{D_{l}: D_{l}\sim^{\hextb}
      D\}_{1}^{\kappa}$ such that $x \hsext _ {D_{1}\cup\cdots \cup D_{\kappa}
      \cup D '} y$. Then, by the construction of $\extb_{\mbbjpp}$, $x
      \sext_{\kappa L_{D} + L_{D'}} y$. Let $\nu \defeq \frac {1} {\kappa j +
      j'}$ and take $\lambda = \nu j'$, so that $0 < \lambda < 0$ and
      $1-\lambda = \nu \kappa j$. By virtue of the fact that $\lambda
      \in \mbb Q$, we have $K \defeq (1-\lambda ) J +\lambda J ' \in \mbbjpp.$
      Simplifying, we obtain $K = \nu ( \kappa L + L ')$.  Since $\nu \in
      \posrat$ and $\kappa L + L ' \in \mbbjpp$, \cref{lem-coneQ} implies
      $\extb _ {K} = \extb _ {\kappa L + L '}$.  This allows us to conclude
      that $x \sext _ K y$.  Take any $\mu \in \mbb Q \cap (0,\lambda)$. From
      basic properties of the real numbers, there exists $\xi < 1$ such that
      $\mu = \xi\lambda$ and, moreover, $\xi$ is rational.  The definition of
      $K$ implies $\xi (K - J) = \xi\lambda ( J ' - J)$. Adding $J$ to each
      side of the latter and applying the definition of $\mu$ yields
          $(1 - \xi ) J +  \xi K =  ( 1 - \mu ) J + \mu  J'$.
      Then, since $x \sext _ J y$ and $x \sext _ K y$, \ref{CQ} implies $x
      \sext _ {( 1 - \mu ) J + \mu J '} y$, as required for \ref{AQ}.

    Conversely, we now assume that $\ext _{\mbbjpp}$ satisfies \ref{CQ} and
    \ref{AQ} and prove that \ref{A} holds.  Take $D , D ' \in \mbbd$ such that
    $x \hsext_{D} y$ and any other $D' \in \mbbd$. Let $L=L_{D}$ and $L' =
    L_{D'}$.  Then, by construction, $x \sext_{L} y$ and, by \ref{AQ}, there
    exists $\lambda \in \mbb Q \cap ( 0, 1)$ such that $x \sext _ {(1-\mu) L +
    \mu L'}  y$.  Then, since $\mu$ is rational, $\mu = \nicefrac {j}
    {k}$ for some $j , k \in \posint$.  Let $q : = (1 - \mu ) /\mu = ( k - j )/
    j$ and let $\kappa = j q$, so that $\kappa = k - j$.  The fact that $0 <
    \mu < 1$ ensures that $\kappa \in \posint$.  To complete the proof, we show
    that $x \sext _ {\kappa L + L'} y$, for then the existence of $D_{1},
    \dots, D_{\kappa}$ such that $x \hsext _ {D_{1}\cup \cdots \cup D_{\kappa} \cup
    D'}$ follows.  Together $x \sext _ {(1 -\mu ) L +\mu L'} y$ and
    \cref{lem-coneQ} imply $x \sext _ {q L + L '} y$.  Similarly, together $x
    \sext _{L} y$ and \cref{lem-coneQ} imply $x \sext _ {( j - 1 )q L} y$. 
    Then, since $( j - 1 )q L + ( q L + L ' ) = j q L + L '$ and $\kappa = j
    q$, an application of \ref{CQ} yields $x \sext_{\kappa L + L '} y$, as
    required.
  \end{proofEnd}

  The matrix $\mathbf{v}: X \times \mbbt \rightarrow R$ is a
  \emph{representation of $\preceq _{\mbbj}$} whenever it satisfies
  \begin{linenomath*}
    \begin{equation}\tag{$\flat$}\label{eq-rep-mainQ}
      \left\{
        \begin{array}{l}
          \text{for every $x , y \in X$ and every $J \in \mbbj$,} \\
          x \preceq_{J} y \quad \text{if, and only if,} \quad
          \sum _ {\, t \,\in\, \mbbt} \mathbf{v} (x , t) J(t)
          \leq \sum _ {\, t \,\in\, \mbbt} \mathbf{v} ( y , t ) J(t).
        \end{array}
      \right.
    \end{equation}
  \end{linenomath*}
  % By \cref{lem-as-fine-as,lem-well-defined}, we are able to modify the statement
  % so that, as in the corresponding result of \gsii, the fact that $\mbbt$
  % coincides with $\mbbc _ {/ \sim^\star}$ is given.

  %main theorem integers \setcounter{theorem}{2}
  We observe that, via the definition of case types, there exists a
  representation of $\preceq_{\mbbd}$ that respects case equivalence if, and
  only if there exists a representation of $\preceq_{\mbbj}$. The above
  translation and results imply that \cref{thm-main} is equivalent to
  \begin{theorem}\label{thm-mainQ} Let there be given $X$, $\mbbtp$,
    $\preceqb_{\mbbj}$ and associated generalizations, as above. Then
    \ref{ax-mainQ} and \ref{wrap-mainQ} are equivalent.
    \begin{enumerate}[label=\textup{(\ref{thm-mainQ}.\roman*)}]
      \item\label{ax-mainQ} \parthreedivq\ and \stabilityq\ hold for 
        $\preceqb _ {\mbbj}$ on $X$.
      \item\label{wrap-mainQ} There exists a matrix $\mathbf{v} : X \times
        \mbbt \rightarrow \R$ that satisfies both$\,:$
        \begin{enumerate}[label=\textup{(\ref{thm-mainQ}.\alph*)}]
          \item\label{rep-mainQ} $\mathbf{v}$ is a representation of $\preceq _
            {\mbbj}\,;$ and
          \item\label{rows-mainQ} no row of $\mathbf{v}$ is dominated by any
            other row, and, for every three distinct elements $x,y, z \in X$,
            $\mathbf{v}(x,\cdot)-\mathbf{v}(z,\cdot) $ and
            $\mathbf{v}(y,\cdot)-\mathbf{v}(z,\cdot)$ are noncollinear (\ie\
            linearly independent).
        \end{enumerate}
    \end{enumerate}
    Moreover, \emph{mutatis mutandis}, $\mathbf{v}$ is unique in the sense of
    \cref{thm-main} part II.
  \end{theorem}

  %-----------------------------------------------------------------------------%  
  \section{The proof of \cref{thm-mainQ}}\label{sec-proof-mainQ} 
  %-----------------------------------------------------------------------------%  
    In \cref{step-ref{c2d}} we show that
    \ref{KQ}–\ref{c2dQ} hold if, and only if, $\preceq_{\mbbj}$ has a
    conditionally $2$-diverse pairwise representation.  In
    \cref{step-stability}, we show that, when $\preceq_{\mbbj}$ satisfies
    \ref{TQ}–\ref{c2dQ}, a necessary and sufficient condition for a *** is
    \ref{SQ}. In \cref{step-induction}, via (mathematical) induction, we
    show that conditionally $2$-diverse Jacobi representation of
    $\preceq_{\mbbj}$ (on all of $X$, regardless of cardinality). The fact that
    \ref{TQ} is necessary for a Jacobi representation follows from \gsii.  As a
    consequence, \ref{TQ}–\ref{c2dQ} and \ref{SQ} are necessary and sufficient
    for a conditionally $2$-diverse Jacobi representation of $\preceq_{\mbbj}$.
    %The following argument then completes the proof of \cref{thm-mainQ}.
  
    %The present
    %proof follows a similar structure to that of \gsii. That is, we begin with the
    %proof for the case of arbitrary (nonempty) $X$ and finite $\mbbt$. We then
    %show that we can ``patch'' the proof together to account for the case where $0
    %<  \countof X < \infty = \countof \mbbt$.
    \begin{step}[characterisation of \ref{KQ}–\ref{AQ}, \twodiv\ and novel
      generalizations]\label{step-twodiv}
    For any pair of vectors $\acute v, J: \mbbtpp \rightarrow \R$ such that $J
    \in \mbbjpp$, the linear operator
    \begin{linenomath*}
      \begin{equation*}
        J \mapsto \langle \acute v, J \rangle \defeq \sum_{\{t : J(t) > 0\}} v(t)
        \cdot J(t).
      \end{equation*}
    \end{linenomath*}
    is well-defined and real-valued by virtue of the fact that $J$ has finite
    support.

    In our proof, we build on \gsii\ to directly prove all results regardless of
    the cardinality of $X$ and $\mbbt$. To facilitate this approach, we first 
    introduce the notion of an essentialization.

    Let $\R^{\oplus\mbbtpp}$ denote the vectors in $\R^{\mbbtpp}$ that have
    finite support and observe that $\mbbjpp \subseteq \R^{\oplus\mbbtpp}$ is the
    dense subset of rational vectors.  Given a vector $\acute{v}: \mbbtpp
    \rightarrow \R$, we associate the following subsets of $\R^{\oplus\mbbtpp}$:
    $\acute H = \{J : \langle \acute v , J \rangle = 0 \}$, $\acute G = \{J :
    \langle \acute v, J \rangle > 0\}$, and $\acute F = \{J : \langle \acute v,
    J \rangle \geq 0\}$. For any finite collection $\acute{\mc V} = \{\acute{v}_1,
    \dots, \acute{v}_n\}$ of such vectors, let $\acute{\mc H}$ denote the
    associated collection or \emph{arrangement} of hyperplanes in
    $\R^{\oplus\mbbtpp}$. let $\acute{S} = \spann \acute{\mc V}$ denote the
    $\acute{\mathbf{r}}$-dimensional linear span of $\acute{\mc V}$. Then
    $\acute S$ is a well-defined inner-product space in its own right and let
    $\langle \cdot , \cdot \rangle_{\acute{S}}: \acute{S}^2 \rightarrow \R$ denote
    the inner product. Let $\acute{p} \defeq p_{\acute{S}}: \R^{\oplus\mbbtpp}
    \rightarrow \acute{S}$ denote the orthogonal projection. Then observe that,
    for every $i = 1, \dots,
    n$, and $J \in \R^{\oplus\mbbtpp}$, $\langle \acute{v}_i, J \rangle = \langle
    \acute{v}_i, \acute{p}(J)\rangle$. Moreover, note that, since $\acute{v}_i \in
    S$, $\langle \acute{v}_i, J \rangle = \langle \acute{v}_i, \acute{p}(J)
    \rangle_{\acute{S}}$.  The \emph{essentialization} ${\mc H}_{\acute{S}}$ of
    the arrangement $\acute{\mc H}$ is the arrangement we obtain by orthogonally
    projecting $\acute{\mc H}$ onto $\acute{S}$.  That is, for every $H_{\acute{S}}
    \in {\mc H}_{\acute{S}}$, there exists $\acute{H} \in \acute{\mc H}$ such that
    $\acute{H} = \acute{p}^{-1}(H_{\acute{S}})$.  In the literature on
    arrangements of hyperplanes, it is common to work with the essentialization
    of an arrangement by default.  We therefore identify $\acute{\mc H}$ and
    $\mc{H}_{\acute{S}}$ and suppress reference to the latter subscript
    whenever no possible confusion may arise. The main benefit of
    essentializations is that they will allow us to work in finite dimensions
    whenever we consider a finite subset $Y\subseteq X$: regardless of the
    cardinality of $\mbbtpp$.
    %  For $n = \acute{\mathbf{r}}$, take $\mc T= \{T_{i}: i = 1, \dots , n\}$ to be
    %  an orthonormal basis for $S$. Then let $T: \R^{n} \rightarrow S$ to be the
    %  matrix with columns $\mc T$. Then, for every $J \in \R^{n}$, $T(J) = J_{1}
    %  T_{1} + \cdots + J_{n}T_{n}$, where $J_{i} \in \R$ for each $i$. Via
    %  orthonormality of $\mc T$,
    %%
    %%\begin{linenomath*}
    %%\begin{equation*}
    %%\langle T_{i} , T_{j} \rangle_{S} = \left\{
    %%\begin{array}{ll}
    %%1 & \text{if $i = j$, and}\\
    %%0 & \text{otherwise.}
    %%\end{array}
    %%\right.
    %%\end{equation*}
    %%\end{linenomath*} 
    %it then follows that, for every pair of vectors $v,J \in \R^{n}$, we have
    %\begin{linenomath*}
    %\begin{equation*} \langle T(v) , T(J) \rangle_{S} = \sum_{i,j = 1}^{n}
    %  v_{i}J_{j} \,\langle T_{i},
    %   T_{j} \rangle_{S} = \langle v, J \rangle_{\,\R^{n}}.\end{equation*}
    %\end{linenomath*}
    %
    %Let $\mathbf{\acute{u}}^{\dd}$ denote the matrix with rows
    %$\left\{\mathbf{\acute{u}}^{\xy} \defeq (T^{-1}\ccirc P)
    %\left(\acute{u}^{\xy}\right): x, y \in Y\right\}$. Then let $\mathbf{\aext}=
    %\langle \mathbf{\aext}_{J}: J \in \mbb Q_{\mplus}^{n} \rangle$ denote the
    %Y-generalization generated by $\mathbf{\acute{u}}^{\dd}$ in $\mbb
    %Q_{\mplus}^{n}$, for every $x,y\in Y$ and every $J \in \mbb Q_{\mplus}^{n}$,
    %satisfy $x \mathbin{\mathbf{\aext}}_{J} y$ if, and only if, $\langle
    %\mathbf{\acute{u}}^{\xy},J \rangle_{\,\R^{n}} \geq 0$. Then, by construction,
    %$\langle \mathbf{\acute{u}}^{\xy},J \rangle_{\,\R^{n}} \geq 0$ if, and only if
    %$\langle \acute{u}^{\xy}, T(J) \rangle_{S} \geq 0$. Thus, $x
    %\mathbin{\mathbf{\aext}}_{J} y$ if, and only if, $x \aext_{T(J)} y$.
    %
    %Next note that, with the exception of \cref{lem-induction} of
    %\cref{step-induction}, all results in the proof of \cref{thm-mainQ} involve
    %$Y$-generalizations $\ext$ of $\preceq_{\mbbj}$ for $\countof Y = 2,3$ or
    %$4$. Thus, via the above equivalence, all these results extend to arbitrary
    %$\mbbt$.  And, since \cref{step-induction} holds for arbitrary $\mbbt$
    %and $X$, the proof of \cref{thm-mainQ} is indeed complete. 

    Our domain of interest is $\nnreal^{\oplus\mbbtpp}$ and not the whole of
    $\R^{\oplus\mbbtpp}$. In order to be able to apply results from the literature
    on hyperplane arrangements without adjusting for boundaries, we find it useful
    to work within the relative interior $\acute{S}_{\mpplus}$ of
    $\acute{S}_{\mplus} = \acute{p}(\nnreal^{\oplus\mbbtpp})$.  Observe
    $\acute{S}_{\mpplus}$ is an open subset of the
    $\acute{\mathbf{r}}$-dimensional linear space $\acute{S}$.  For $\acute{H}
    \in \mc H$, take $\acute{H}_{\mpplus} = \acute{H}\cap
    \acute{S}_{\mpplus}$ to be the (strictly) \emph{positive} null-space
    of $\langle \acute v , \cdot \rangle_{\acute{S}}$ and, for $0 \not \leq
    \acute v \not \leq 0$, $\acute G_{\mpplus}$ and $\acute F_{\mpplus}$
    are, respectively, the open and closed half-spaces of $\acute{S}_{\mpplus}$
    associated with $\acute v$.  (For such $\acute v$, we also refer to $\acute
    H_{\mpplus}$ as a hyperplane in $\acute{S}_{\mpplus}$.) We refer
    to the non-negative counterpart of these sets as
    $\acute{H}_{\mplus},\acute{G}_{\mplus}$ and $\acute{F}_{\mplus}$.  For any
    finite $Y\in 2^{X}$ consider the matrix $\acute v^{\dd} : Y^{2}\times
    \mbbtpp\rightarrow \R$. For a given $x, y \in Y$ and row $\acute v^{\xy}:
    \mbbtpp \rightarrow \R$ of $\acute{v}^{\dd}$, the associated sets are
    $\acute H^{\{x,y\}}_{\mpplus}$, $\acute G^{\xy}_{\mpplus}$ and
    $\acute F^{\xy}_{\mpplus}$ respectively.

    The present proof relies heavily on the mathematics of hyperplane
    arrangements and in particular Zaslavsky's theorem. We provide a brief
    introduction and exposition of this literature in \cref{sec-online}. The
    standard references for this literature are
    \citet{orlik1992arrangements,stanley2007introduction,dimca2017hyperplane}.
    The following proposition corresponds to lemma 1 of \gsii\ and gives
    meaning to the statement ``the arrangement generated by a
    generalization''. 
   
    \begin{theoremEnd}{lemma}\label{lem-insep} For every $Y \in 2^{X}$,
      \ref{KQ}--\ref{AQ} and \twodiv\ hold for the $Y$-generalization $\aext$
      if, and only if, there exists $\acute{v}^{\dd}: Y^2 \times \mbbt
      \rightarrow \R$ such that,
      \begin{enumerate}[label=\textup{(\roman*)}]
        \item\label{lem-insep-rep} for every distinct $x, y \in Y$ and $J \in
          \mbbjpp$, $x \aext_J y$ if, and only if, $\langle \acute{v}^{\xy},
          J \rangle > 0$$\,;$ \textup{and}
        \item\label{lem-insep-2div} for every $x, y \in Y$, there exists $s, t
          \in \mbbt$ such that $\acute{v}^{\xy}(s) < 0 < \acute{v}^{\xy}(t)$.
      \end{enumerate}
      Moreover, $\acute{v}^{\xy}$ is unique upto multiplication by a positive
      scalar,  $v^{\yx} = - v^{\xy}$. Finally,  $\aext$ is novel if, and only
      if, for every $t \neq \novel$, $\acute{v}^{\dd}(t) \neq
      \acute{v}^{\dd}(\novel)$.
    \end{theoremEnd}

    \begin{proofEnd}
      Let $\mc Y = \{Y_\alpha \subseteq Y : \countof Y_\alpha = 2\}$ be the
      collection of distinct (unordered) pairs in $Y$. For every $Y_\alpha \in
      \mc Y$, \ref{T} holds simply because $Y_\alpha$ is of cardinality two;
      moreover, on $Y_\alpha$, \fourdiv\ is equivalent to \twodiv. This allows
      us to apply theorem 2 of \gsii.\footnote{\citeauthor{gilboa2003inductive}
        explicitly prove their theorem 1 holds for the case of arbitrary $X$
        and $\mbbt$. But although their theorem 2 is stated and proved in the
        first step (the case where $X, \mbbt < \infty$) of the proof of their
        theorem 1, steps 2 and 3 of that proof apply equally to their theorem
      2.}  In particular, \gsii\ yields a matrix representation $v_\alpha :
      Y_\alpha \times \mbbtpp \rightarrow R$. For $x, y \in Y_\alpha$, take
      $v^{\xy}_{\alpha} = - v_{\alpha}(x, \cdot) + v_{\alpha}(y, \cdot)$. 
      For condition \ref{lem-insep-2div}, via theorem 2 of \gsii, \twodiv\
      holds for $\aext$ on $Y_\alpha$ if, and only if, the matrix $v_\alpha$
      is \emph{2-diversified}. That is, whenever, for $x\neq y$, and every
      $\lambda \in \R$, $v_\alpha (x) \not \leq \lambda v_\alpha(y)$. Take
      $\lambda = 1$ and observe that this is equivalent to
      $v^{\xy}_{\alpha}(s) < 0 < v^{\xy}_{\alpha}(t)$ for some $s, t \in
      \mbbtpp$.
    
      To extend this to arbitrary $Y \subseteq X$, note that, for some $A'$,
      $\bigcup\{\mc Y_\alpha : \alpha \in A' \} = Y$. For an arbitrary function
      $f$, let $\graph f$ denote the graph of that function. To obtain the
      desired matrix $\acute{v}^{\dd}: Y^2 \times \mbbt \rightarrow \R$, take
      $\graph \acute{v}^{\dd} = \bigcup \{\graph v^{\dd}_{\alpha} : \alpha \in
      A'\}$.
    
      It remains for us to prove the characterisation of novel
      generalizations. Fix arbitrary $t \neq \novel$.  Then
      \cref{def-generalizationQ} implies the existence of $J \in \mbbj$ and $L
      = J \times 0 \in \mbbjp$ such that $\aextb _ {L + \delta_{t}} \neq \aextb
      _ {L + \delta_{\novel}}$.  \Wlog, consider the case where, for some $x ,
      y \in Y$, it holds that both $y \aext _ {L + \delta _ t} x$ and $x \asext
      _ {L + \delta_ \novel} y $. Equivalently, $\langle \acute{v}^{\xy}, L +
      \delta_ t \rangle \leq 0 < \langle \acute{v}^{\xy}, L + \delta_
      \novel\rangle$ which, via linearity of $\langle \acute{v}^{\xy}, \cdot
      \rangle$, we may rearrange to obtain
      \begin{linenomath*}
        \begin{equation}\label{eq-nov} \acute{v}^{\xy} ( t ) \leq -\langle
          \acute{v}^{\xy}, L \rangle <
          \acute{v}^\xy ( \novel ) .
        \end{equation}
      \end{linenomath*}
      Thus, $\acute{v}^{\xy} (t) \neq v^{\xy}(\novel)$, as required for the
      lemma.
    \end{proofEnd}
    We refer to a matrix $\acute{v}^{\dd}$ that satisfies condition
    \ref{lem-insep-2div} of \cref{lem-insep} as a \emph{$2$-diverse matrix}. We
    refer to a matrix $\acute{v}^{\dd}$ that satisfies \cref{lem-insep} as a
    $2$-diverse pairwise (matrix) representation of $\ext$.
  \end{step}
  \begin{step}[characterisations of \ref{c2dQ}]\label{step-ref{c2d}} A matrix
    $v^{\dd}$ that satisfies the conditions of the next lemma is a
    \emph{conditionally-$2$-diverse (pairwise) representation}.
  
    \begin{theoremEnd}[no link to proof]{lemma}[proof on
      \cpageref{proof-lem-c2dQ}]\label{lem-c2dQ} Let $\ext$ be a
      $Y$-generalization of $\preceq_{\mbbj}$ with $2$-diverse matrix
      representation $v^{\dd}$.  Then \ref{c2dQ} holds on $Y$ if, and only if,
      for every three distinct elements $x,y,z\in Y$, $v^{\xz}$ and $v^{\yz}$
      are noncollinear.
    \end{theoremEnd}
    \begin{proofEnd}%[Proof of \cref{lem-c2dQ}]
      \label{proof-lem-c2dQ} Let \ref{c2dQ} hold for $\ext$ on $Y$. Since
      $v^{\dd}$ is a $2$-diverse pairwise representation, $v^{\xz}, v^{\yz}\not
      \leq 0$.  By \ref{c2dQ}, \withoutlog, take $L, J \in G_{\mplus}^{\xz}$
      such that
      \begin{linenomath*}
        \begin{equation*}
          \langle v^{(y,z)} , L \rangle < 0 < \langle v^{(y,z)} , J\rangle.
        \end{equation*}
      \end{linenomath*}
      Then, since $\langle v^{\xz},\cdot \rangle$ is positive on $\{L,J\}$, for
      every $\lambda \in \R$, $v^{\xz} \neq \lambda v^{\yz}$, as required.
  
      Conversely, let $x,y,z \in Y$ be such that $v^{\xz}$ and $v^{\yz}$ are
      noncollinear. Then $H_{\mpplus}^{\{x,z\}}\neq
      H_{\mpplus}^{\{y,z\}}$, and there exists $L \in
      H_{\mpplus}^{\{x,z\}}\bs H_{\mpplus}^{\{y,z\}}$.  \Wlog,
      therefore, take $L \in H^{\{x,z\}}_{\mpplus} \cap
      G^{\yz}_{\mpplus}$ so that $L \in S_{\mpplus}$. \Wlog, take $L$
      such that $p_S^{-1}(L)$ is rational-valued and thus in $\mbbjpp$. Since
      $v^{\dd}$ is $2$-diverse, there exists $s,t\in \mbbt$ such that
      $v^{\xz}(s)<0<v^{\xz}(t)$. Let $\psi_{s}$ and $\psi_{t}$ be the convex
      paths from $L$ to $\delta_{s}$ and $\delta_{t}$ respectively. For
      sufficiently small $\lambda \in \posrat$, $\langle v^{\yz},
      \psi_{s'}(\lambda)\rangle$ remains positive for $s'= s, t$ and, moreover,
      since $L \in H^{\{x,z\}}_{\mpplus}$, as required for \ref{c2dQ}, we
      have $ \langle v^{\xz}, \psi_{s}(\lambda)\rangle<0<
          \langle v^{\xz},\psi_{t}(\lambda) \rangle.$
    \end{proofEnd}

    %The following is a translation of \cref{obs-c2d}.
    %*********************
    \begin{theoremEnd}[no link to proof]{proposition}[proof on
      \cpageref{proof-prop-c2dQ}]\label{prop-c2dQ} Let $\ext$ be a
      $Y$-generalization $\ext$ satisfying \ref{TQ}--\ref{AQ}. Then \ref{c2dQ}
      holds for $\ext$ if, and only if \ref{p3dQ} does. Morover,
      $\countof\total(\ext) \geq 4$.
    \end{theoremEnd}
    \begin{proofEnd}%[Proof of \cref{prop-c2dQ}]
      \label{proof-prop-c2dQ} When $X=2$, \ref{c2dQ} and \ref{p3dQ} are
      identical to \twodiv.  Let $Y = \{x, y, z\} \subseteq X$ and let $\ext$
      denote the improper $Y$-generalization of $\preceq_{\mbbj}$. We begin
      by assuming \ref{c2dQ} and showing that $\countof{Y} + 1 = 4 \leq
      \countof{\total(\ext)}$. Via \cref{lem-c2dQ}, there are three distinct
      hyperplanes $H_{\mpplus}^{\{x,y\}}, H_{\mpplus}^{\{y,z\}}$ and
      $H_{\mpplus}^{\{x,z\}}$ in the associated arrangement $\mc
      H_{\mpplus}$. Then, as in \cref{eg-zaslavski}, $S_{\mpplus}$ is
      the unique element of the intersection semilattice $\mc L_{\mpplus}$
      that lies below each member of $\mc{H}_{\mpplus}$. Thus, via
      \cref{eq-mobius}, $\bmu(S_{\mpplus}) = 1$ and
      $\bmu(H)=-\bmu(S_{\mpplus})$ for all three hyperplanes $H \in \mc
      H_{\mpplus}$. Thus, via Zaslavski's theorem,
      $\countof{\mc{G}_{\mpplus}} =
      \sum\{\bmu(A):{A\in\mc{L}_{\mpplus}}\}$ is bounded below by $4$. 
      Via \ref{TQ}-\ref{KQ}, $\total(\ext) \geq 4$ and \ref{p3dQ} holds for
      $\ext$.
  
      Conversely, suppose \ref{p3dQ} holds for $\ext$. Via \cref{lem-insep},
      there exists a $2$-diverse matrix representation with associated
      arrangement $\mc H_{\mpplus}$. \ref{p3dQ} implies that
      $\lvert\mc{H_{\mpplus}}\rvert > 1$. \Wlog, suppose
      $H_{\mpplus}^{\{x,y\}} \neq H_{\mpplus}^{\{y,z\}}$. \ref{TQ}
      then implies $H_{\mpplus}^{\{x,z\}} \neq H_{\mpplus}^{\{x,y\}}$
      and $H_{\mpplus}^{\{x,z\}} \neq H_{\mpplus}^{\{y,z\}}$ so that
      $v^{\xy}, v^{\yz}$ and $v^{\xz}$ are pairwise noncollinear.
      \Cref{lem-c2dQ} then yields \ref{c2dQ}.
    \end{proofEnd}
    %
  \end{step}
  
  \begin{step}[a characterisation of \stability] \label{step-stability} The
    following Jacobi identity plays a central role in the proof of \gsii.
    % Then, via an example, we introduce
    % some key concepts from the literature on hyperplane
    % arrangements.\footnote{A
    % more detailed introduction can be found in the classic text
    % \citet{orlik1992arrangements}, the recent \citet{dimca2017hyperplane} and
    % the
    % lecture notes \citet{stanley2007introduction}.}
    \begin{definition*}%\label{def-jac-rep} %We will use the following notation.
      For $Y \in 2^{X}$, the matrix $v^{\dd} : Y^{2}\times \mbbtpp \rightarrow
      \R$ satisfies the \emph{Jacobi identity} whenever, for every $x , y , z
      \in Y$, $v^{\xz} = v^{\xy} + v^{\yz}$.
    \end{definition*}
    For any given $Y$-generalization $\ext$, the \emph{Jacobi identity holds
    for $\ext$} whenever it holds for some pairwise representation $v^{\dd}$ of
    $\ext$. Moreover, in this case, $v^{\dd}$ is a \emph{Jacobi
    representation}. Finally, if the $Y$-generalization $\ext$ is improper
    and the Jacobi identity holds for $\ext$, we simply say that the
    \emph{Jacobi identity holds on $Y$}.  Consider
    \begin{k-jac*}
      For every $Y \subseteq X$ with $3 \leq \countof Y \leq k$, the Jacobi
      identity holds on $Y$.
    \end{k-jac*}
    \begin{theorem}\label{thm-foureq} For $\preceq_{\mbbj}$ satisfying
      \ref{TQ}--\ref{c2dQ}, \stability\ holds if, and only if, \fourjac\ holds.
      % (for some pairwise representation of $\preceq_{\mbbj}$).
    \end{theorem}
    %As we show in the proof of \cref{thm-foureq}, when $\mbbt$ is finite, for
    %every $Y$, the set of testworthy $Y$-generalizations that are novel is
    %nonempty. In this case, \stability\ implies \ref{TQ}--\ref{AQ} via part
    %\ref{item-preservingQ} of \cref{def-generalizationQ}. As a consequence, we
    %have
    %\begin{corollary}\label{lem-foureq} Let $\countof{\mbbt}<\infty$. For
    %  $\preceq_{\mbbj}$ satisfying \ref{c2dQ}, \stability\ holds if, and only
    %  if, \fourjac\ holds.
    %\end{corollary}
  \end{step}
  %\setcounter{step}{3}
  \begin{step}[the induction argument]\label{step-induction} The present step
    corresponds to lemma 3 and claim 9 of \gsii. There the authors establish
    that, when \fourdiv\ holds, $3$-Jac is a necessary and sufficient condition
    for the (global) Jacobi identity to hold on $X$.  \gsii\ relies on the fact
    that \fourdiv\ implies linear independence of $\{v^{\xy},v^{\yz},
    v^{\zw}\}$ for every four distinct elements $x,y,z,w \in X$. In the present
    setting, where \ref{c2d}\ only implies linear independence of pairs
    $\{v^{\xy},v^{\yz}\}$, the Jacobi identity requires \fourjac.
    % induction argument

    \begin{theoremEnd}[no link to proof]{lemma}[proof on
      \cpageref{proof-lem-induction}]\label{lem-induction} Let
      $\preceq_{\mbbj}$ have a conditionally-$2$-diverse representation
      $u^{\dd}$. Then \fourjac\ holds if, and only if, $\preceq_{\mbbj}$ has a
      Jacobi representation $v^{\dd}$.  Moreover, for every Jacobi
      representation $\mathbf v^{\dd}$ of $\preceq_{\mbbj}$ there exists
      $\lambda > 0$ satisfying $\mathbf v^{(\cdot, \cdot)} = \lambda v^{(\cdot,
      \cdot)}$.
    \end{theoremEnd}

  \begin{proofEnd}\label{proof-lem-induction}%[Proof of \cref{lem-induction}]
    Note that, when $1 \leq \lvert X \rvert \leq 2$, \fourjac\ holds vacuously
    and $\preceq_{\mbbj}$ has a Jacobi representation via \cref{lem-insep}. For
    general $X$, the fact that \fourjac\ is necessary for $\preceq_{\mbbj}$ to
    have a Jacobi representation follows simply because if the Jacobi identity
    holds on $X$, then it holds on every $Y\subseteq X$. For the sufficiency of
    \fourjac, we proceed by induction.  As in lemma 3 and claim 9 of \gsii, we
    assume that $X$ is well-ordered.  
  
    % initial step (uniqueness)
    In the case that $\lvert X \rvert \leq 4$, we only need to show that $v^{
      \dd}$ is unique. \Wlog, we take the initial step in our induction argument
    to satisfy $\lvert X \rvert = 4$.  Let $\mathbf v^{\dd}$ denote any other
    Jacobi representation of $\preceq_{\mbbj}$. By \cref{lem-insep}, for every
    distinct $x , y \in Y^{2}$, there exists $\lambda^{\{x,y\}}> 0$ such that
    $\mathbf v^{\xy} = \lambda^{\{x,y\}} v^{\xy}$. We need to show that
    $\lambda^{\{x,y\}} = \lambda$ for every distinct $x , y \in Y$.  Let $Y = \{
      x , y , z , w \}$.  By \cref{lem-c2dQ}, the set $\{v^{\xy} , v^{\xz}
    , v^{\xw} \}$ is pairwise noncollinear. Then, since the Jacobi identity
    holds for both $v^{\dd}$ and $\mathbf v^{\dd}$, we derive the equation
  \begin{linenomath*}
    \begin{equation}\label{eq-jac-unique}
      (1 - \lambda^{\{x,y\}})v^{\xy} + (1 - \lambda^{\{y,z\}})v^{\yz} =
      (1 - \lambda^{\{x,z\}}) v^{\xz}
    \end{equation}
  \end{linenomath*}
  Suppose that $1 - \lambda^{\{y,z\}} = 0$. Then, either the other coefficients
    in \cref{eq-jac-unique} are both equal to zero (and our proof is complete),
    or we obtain a contradiction of \cref{lem-c2dQ}.  Thus, $1 - \lambda
   ^{\{y,z\}}$ is nonzero and we may divide through by this term and solve for
    $v^{\yz}$. First note that, since $v^{\dd}$ is a Jacobi representation,
    $v^{\yx}+ v^{\xy} = v^{(y,y)} = 0$. Then, since $v^{\yx} = - v^{
      \xy}$, 
  \begin{linenomath*}
    \begin{equation*}
      v^{\yz} = \frac{1- \lambda^{x y}}{1 - \lambda^{y z}} v^{\yx} +
      \frac{1 - \lambda^{x y}}{1 - \lambda^{y z}} v^{\xz}.
    \end{equation*}
    \end{linenomath*}
  We then conclude that both of the coefficients in the latter equation are equal
    to one. (This follows from linear independence of $v^{\yx}$ and $v^{\xz
   }$ together with the Jacobi identity $v^{\yz} = v^{\yx} + v^{\xz}$.)
    Thus, $\lambda^{\{x,y\}} = \lambda^{\{y,z\}} = \lambda^{\{x,z\}}$, as
    required. Repeated application of the same argument to the remaining Jacobi
    identities yields the desired $\mathbf v^{\dd}= \lambda v^{\dd}$.
  
  % Jac4-inductive step
  For the inductive step, take $Y$ to be an initial segment of $X$. By the
  induction hypothesis, there exists a Jacobi representation
  $\v^{\dd}: Y^{2} \times \mbbt \rightarrow \R$ of the improper
  $Y$-generalization $\extb = \preceqb_{\mbbj} \cap Y^{2}$ that is suitably unique.
  \begin{claim}\label{claim-induction-well-defined}
    For every $w \in X \bs Y$ and $W \defeq Y \cup \{w\}$, there
  exists a Jacobi representation
  $\hat{v}^{\dd}: W^{2} \times \mbbt \rightarrow \R$ of the improper
  $W$-generalization $\hextb$.
  \end{claim}
  \begin{proof}[Proof of \cref{claim-induction-well-defined}]
    Via \cref{lem-c2dQ}, there exists a conditionally $2$-diverse pairwise
    representation $u^{\dd}$ of $\preceq_{\mbbj}$. Fix any four distinct elements
    $x, x ' , y , z$ in $Y$.  \Cref{lem-insep} implies the existence of
    $\phi, \phi' \in \posreal$ such that
    $\phi u^{\xz} = \v^{\xz}$ and $\phi' u^{\xpz} = \v^{\xpz}$.  Let
    $Z = \{x, y , z , w\}$ and $Z ' = \{x' , y , z , w\}$. Since \threejac\
    holds, there exist positive scalars
    $\alpha , \beta , \hat \beta, \gamma, \sigma$ and $\tau$ such that
  \begin{linenomath*}
  \begin{align}\label{eq-xy}
    \alpha u^{\xw} + \beta u^{\wy} &= \gamma u^{\xy},\\
     \label{eq-yz}
      \hat \beta u^{\yw} + \sigma u^{\wz} &= \tau u^{\yz}, \text{ and}\\
     \label{eq-xz}
    \gamma u^{\xy} + \tau u^{\yz} &= \v^{\xz} .
  \end{align}
  \end{linenomath*}
  \addtocounter{linenumber}{-1}
  Moreover, \fourjac\ ensures that we may take
  $\beta = \hat \beta$.  Since $u^{\dd}$ is conditionally $2$-diverse,
  $\{u^{\xy} , u^{\yz} \}$ is linearly independent, and the linear system
  \cref{eq-xz} in the unknowns $\gamma$ and $\tau$ has a unique solution.
  This, together with the induction hypothesis (which yields
  $\v^{\xy} + \v^{\yz} = \v^{\xz}$) implies that
  $\gamma u^{\xy} = \v^{\xy}$ and $\tau u^{\yz} = \v^{\yz}$.
  Similarly, for $Z '$, \fourjac\ yields
  $\alpha' , \beta' , \sigma' , \gamma' , \tau' > 0$ such that
  \begin{linenomath*}
    \begin{align}
      \label{eq-xpy}
      \alpha' u^{\xpw} + \beta' u^{\wy} &= \gamma' u^{\xpy},\\
      \label{eq-yz-xp}
      \beta' u^{\yw} + \sigma' u^{\wz}& = \tau' u^{\yz}, \text{and}\\
      \label{eq-xpz-y}
      \gamma' u^{\xpy} + \tau' u^{\yz} &= \v^{\xpz}.
   \end{align}
  \end{linenomath*}
  \addtocounter{linenumber}{-1} As in the arguments involving $\gamma$ and
  $\tau$, the induction hypothesis yields
  $\gamma ' u^{\xpy} = \v^{\xpy}$ and $\tau ' u^{\yz} = \v^{\yz}$.
  We conclude that $\tau = \tau'$.  Substituting for $\tau'$ in \cref{eq-yz-xp}
  and appealing to linear independence of $\{u^{\yw}, u^{\wz}\}$ then yields the
  desired equalities $\beta = \beta'$ and $\sigma = \sigma '$.
    
    As a consequence of the above argument, for every $y , z \in Y$, take
    $\hat{v}^{\yw}$ and $\hat{v}^{\wz}$ to be the unique vectors in $\R^{\mbbt}$
    that solve the equation $\hat{v}^{\yw} + \hat{v}^{\wz} = \v^{\yz}$.
    For every $y , z \in Y$, let $\hat{v}^{\yz} = \v^{\yz}$ and $\hat{v}^{(w,w)}
    = 0$.  Then the matrix $\hat{v}^{\dd}$ with row vectors $\left\{
      \hat{v}^{\xy}: x , y \in W \right\}$ is a Jacobi representation of $\hext$.
  \end{proof}
  Our proof of \cref{claim-induction-well-defined} shows that the
  generalization to $W$ holds for any initial subsegment of $Y$ consisting of
  four elements. Our proof thereby accounts for the case where $X$ is infinite
  and $w$ is a limit ordinal.
  \end{proofEnd}
  \end{step}
  
  \begin{step}[the concluding arguments in the proof of \cref{thm-mainQ}]
    \label{step-conc-mainQ}
    Let $v^{\dd}$ be a (conditionally $2$-diverse) Jacobi representation of
    $\preceq_{\mbbj}$ and define $\mathbf{v}: X\times \mbbt \rightarrow \R$ as
    follows. Fix arbitrary $w\in X$, and let $\mathbf{v}(w,\cdot) = 0$. Then,
    for every $x\in X$, let $\mathbf{v}(x,\cdot) = v^{\wx}$. Since $v^{\wx}=
    -v^{\xw}$ and $v^{\dd}$ satisfies the Jacobi identity, for every $x,y\in
    X$, we have $v^{\xy} =  -\mathbf{v}(x,\cdot) + \mathbf{v}(y,\cdot)$. To see
    that \ref{rep-mainQ} holds note that, for every $J\in \mbbj$, we have $x
    \preceq_{J} y$, if, and only if, $0 \leq
    \langle v^{\xy}, J \rangle$, if, and only if, $\langle v(x,\cdot),
    J \rangle \leq \langle \mathbf{v}(y,\cdot), J \rangle$.

    For \ref{rows-mainQ}, since $v^{\dd}$ is a conditionally $2$-diverse
    pairwise representation, for every distinct $x,y\in X$ we have $0 \not \leq
    v^{\xy}$ if, and only if, $\mathbf{v}(x,\cdot) \not \leq 
    \mathbf{v}(y,\cdot)$. Finally, for every $z\in X$, we have, for every
    $\lambda \in \R$, $v^{\zx} \neq \lambda v^{\zy}$ if, and only if,
    $v(x,\cdot) \neq (1-\lambda) \mathbf{v}(z,\cdot ) + \lambda
    \mathbf{v}(y,\cdot)$.

    \Cref{thm-main} part II, on uniqueness, follows from \cref{lem-induction}
    and, without modification, part 3 of the proof of theorem 2 of \gsii\ (see
    page 23).
  \end{step}
  %for lemmata/steps in the proof of thm-foureq
  %\begin{step}[the case of arbitrary $X$ and $\mbbt$]\label{step-infinite}
  %  Note that the inner product we have been working with throughout is
  %  well-defined on the infinite-dimensional, linear subspace $\R^{\oplus
  %  \mbbtpp} \subset \R^{\mbbtpp}$ of vectors with finite support. Indeed, for
  %  every $v \in \R^{\mbbtpp}$ and $J \in \R^{\oplus\mbbtpp}_{\mplus}$, the inner
  %  product is a finite sum
  %\begin{linenomath*}
  %  \begin{equation*}
  %    \langle v, J\rangle = \sum_{\{t:J(t)>0\}}v(t)J(t).
  %  \end{equation*}
  %\end{linenomath*}
  %  Next note that $\mbbjpp$ is just the set of rational-valued vectors in
  %  $\R^{\oplus \mbbtpp}$. As such, the notions of orthogonality and hyperplanes
  %  carry over to the present, infinite-dimensional, setting.  \Wlog, let $\aext$
  %  be an improper $Y$-generalization of $\preceq_{\mbbj}$ that satisfies
  %  \ref{KQ}-\ref{p3dQ}.  When $\countof \mbbt = \infty$, \cref{lem-insep}, does
  %  not hold (since the interior of $\R^{\oplus\mbbtpp}$ is empty). Nevertheless,
  %  the preceding results extend to the present setting via the notion of an
  %  \emph{essentialization}.  First note that, via theorem 2 of \gsii, there
  %  exists a representation $\acute{u}^{\dd}$ of $\aext$. The construction is the
  %  same as the one we present in proof of \cref{lem-insep}.  For every
  %  $Y\subseteq X$ of cardinality $2,3$ or $4$, the representation
  %  $\acute{u}^{\dd}$ of $\aext$ has finite rank $\acute{\mathbf r}$. The
  %  \emph{essentialization} $\ess(\acute{\mc H})$ of the associated arrangement
  %  $\acute{\mc H}$ in $\R^{\oplus \mbbt}$ is the arrangement we obtain via the
  %  by orthogonally projecting $\acute{\mc H}$ onto the ($\acute{\mathbf
  %  r}$-dimensional) span of $\acute{u}^{\dd}$. Let $\textup{p}: \R^{\oplus
  %  \mbbt}\rightarrow S = \spann\{\acute{u}^{\dd}\}$ denote this projection.
  %  Then, for every $x,y \in Y$, any $J\in \mbbjpp$, $J$ is the sum of
  %  $\textup{p}(J)$ and a term that lies in the null space of $\langle
  %  \acute{u}^{\xy}, \cdot \rangle$. Thus, for every $J \in \R^{\oplus\mbbt}$,
  %  $\langle \acute{u}^{\xy}, J \rangle = \langle \acute{u}^{\xy}, \textup{p}(J)
  %  \rangle$. As such, all the structure of $\acute{\mc H}$ in $\R^{\oplus\mbbt}$
  %  is preserved by $\ess(\acute{\mc H})$ in the finite-dimensional subspace $S$.
  %  On this space, define an inner product $\langle \cdot , \cdot \rangle_{S}$.
  
  %---------------------------------------------------------------------------%
  %proof-thm-foureq 
  \section{Restatement and proof of \cref{thm-foureq}}\label{sec-proof-foureq}
  %---------------------------------------------------------------------------%
  % Our enumeration of the results reflects this.
  %outline of proof 
  \setcounter{theorem}{2}
  \begin{theorem}\label{thm-foureq} For $\preceq_{\mbbj}$ satisfying
    \ref{TQ}--\ref{c2dQ}, \stability\ holds if, and only if, the Jacobi
    identity holds (for some pairwise representation of $\preceq_{\mbbj}$).
  \end{theorem}
  Via \cref{lem-induction}, it suffices to show that \stability\ is equivalent
  to \fourjac. Throughout the present section, take $Y\subseteq X$ to be of
  cardinality $3$ or $4$ and take $\ext$ to be the improper
  $Y$-generalization of $\preceq_{\mbbj}$. Since
  \ref{KQ}--\ref{c2dQ} hold, the pairwise representation $u^{\dd}: Y^{2}\times
  \mbbt \rightarrow \R$ of $\ext$ is conditionally $2$-diverse. 
  \Cref{lem-c2dQ} implies that $u^{\dd}$ has row rank $\mathbf r \geq 2$.

  %We now show that the set of testworthy generalizations with a central
  %arrangement is always nonempty. The trouble is that such arrangements may be
  %dogmatic (they suppress rankings).
  \begin{proposition}\label{prop-central-testworthy} For every ranking $R \in
    \total(\ext)$, there exists a (testworthy) generalization $\aext$ with
    $\aextb_\novel = R^{-1}$. Moreover, $\aext$ has a central arrangement with
    rank $\acute{\mathbf{r}} = \mathbf{r}$.  Finally, \fourjac\ holds for
    $\aext$ if, and only if, it holds for $\ext$.
  \end{proposition}
  \begin{proof} Let $J \in \mbbj$ such that $\ext_{J}$ is total. Since $Y$ is
    finite, so is the dimension of $S_{\mpplus}$.  Take $L \in
    S_{\mpplus}$ such that $\extb_{L} = \extb_{J}$ and, for some rational
    $0<\acute{\iota}<1$, let $\acute{J} = (1-\acute{\iota})J \times
    \acute{\iota}$. Now, for every $x,y$ in $Y$, let
    \begin{linenomath*}
      \begin{equation*}\acute{\eta}^{\xy} := -\frac{1-
        \acute{\iota}}{\acute{\iota}}\langle u^{\xy}, J \rangle,
      \end{equation*}
    \end{linenomath*}
    and let $\acute{u}^{\xy}\defeq u^{\xy}\times \acute \eta^{\xy}$, so that
    $\langle \acute{u}^{\xy}, \acute{J} \rangle = 0$. Let $\aext$ be the
    associated $Y$-generalization, so that by construction $\aextb_{\novel} =
    \aextb_{J}^{-1}$.  Since $\acute{J} \in \acute{H}^{\{x,y\}}_{\mpplus}$
    for every distinct $x,y\in Y$, $\acute{\mc{H}}_{\mpplus}$ is central.
    This construction together with linearity of the inner product ensures that
    the Jacobi equations \eqref{eq-xy}--\eqref{eq-xz} hold for $u^{\dd}$ if,
    and only if, they hold for $\acute{\eta}^{\dd}$ and hence
    $\acute{u}^{\dd}$. Indeed, $\mathbf r = \acute{\mathbf r}$ holds for the
    same reasons.
  \end{proof}
 
  \begin{step}[The case where \stability\ holds vacuously on $Y$] When every
    testworthy $Y$-generalization is regular, \stability\ holds vacuously on
    $Y$. Via the following lemma, \fourdiv\ holds and theorem 2 of \gsii\
    applies, so that \fourjac\ holds on $Y$.

    \begin{lemma}\label{lem-test-empty-fourdiv} If every testworthy
      $Y$-generalization of $\preceq_{\mbbj}$ is regular, then $\lvert \mbbt
      \rvert = \infty$ and \fourdiv\ holds on $Y$.
    \end{lemma}

    \begin{proof}[Proof of \cref{lem-test-empty-fourdiv}]
      \label{proof-test-empty-fourdiv} Via \cref{prop-central-testworthy}, the
      set of testworthy $Y$-generalizations is nonempty. Let $\hext$ be a
      testworthy $Y$-generalization, so that for some $J$ in $\mbbj$ that
      $\hextb_{\novel} = \hextb_{J\times 0}^{-1}$ is total. Let $P \defeq
      \hsextb_{\novel}$ so that $P \subsetneq Y^2$. Via \ref{TQ}--\ref{c2dQ},
      \cref{lem-insep} applies, let $\hat{v}^{\dd}$ be the matrix representation
      of $\hext$ and let $\hat{v}^{P}$ denote the restriction of $\hat{v}^{\dd}$
      to $P \times \mbbtp$.
      % and
      % ${v}^{P^{-1}}(t) \defeq \langle {v}^{p}(t): p$ in $P^{-1}
      % \rangle$.
      % Since $I$ in $\mbbj$, linearity
      % of $\langle {v}^{p},\cdot \rangle$ implies that, for every
      % $p$ in $P$, there exists $s^{p}$ in $\mbbt$ such that
      % ${v}^{p} ( s^{p}) > 0$. Let $S^{P}$ denote the set
      % of such $s^{p}$.
      % We will show that $\ext$ satisfies \fourdiv. Let ${v}^{\dd}$ be the
      % matrix representation of $\ext$. Suppose that, for some
      % $M$ in $\R^{Y\times Y}$ with $M<0$, we have ${v}^{\dd}(t) \neq M$ for
      % every $t$ in $\mbbt$.
      \begin{claim}
        \label{claim-test-empty} For every vector $\eta^{P} = \langle \eta^{\xy}
        \in \R_{\mpplus}: \xy \in P \rangle$, there exist $s, t \in \mbbt$ such
        that $\hat{v}^{P}(s)= \eta^{P}$ and $\hat{v}^{P}(t)= - \eta^{P}$.
        % , for every $p \in P$, ${v}^{p}(t) = \eta^{p}$.
      \end{claim}
      
      \begin{proof}[Proof of \cref{claim-test-empty}] \label{proof-test-empty} By
        way of contradiction, suppose there exists $\acute{\eta}^{P} \in
        \R^{P}_{\mpplus}$ such that, for every $s$ in $\mbbt$, $\hat{v}^{P}(s)
        \neq \acute{\eta}^{P}$. That is $\acute{\eta}^{P}$ such that, for every
        $s$ in $\mbbt$, there exists $\xy$ in $P$ such that $\hat{v}^{\xy} (s)
        \neq \acute{\eta}^{\xy}$. Now define $\acute{v}^{\dd}: X^{2} \times
        \mbbtp \rightarrow \R$. For each $\xy$ in $P$, let
        \begin{linenomath*}
          \begin{equation*}
            \acute{v}^{\xy} (s)\defeq \left\{
              \begin{array}{ll}
                \acute{\eta}^{\xy} & \text{if $s = \novel$,}\\
                \hat {v}^{\xy}(s) & \text{otherwise}.
              \end{array}
            \right.
          \end{equation*}
        \end{linenomath*}
        For every $\xy$ in $P^{-1}$, take $\acute{v}^{\xy} = - \acute{v}^{\yx}$.
        For every remaining $\xy$ in $Y^{2}$, $x = y$, so let $\acute{v}^{\xy} =
        0$.  By construction, for every $s$ in $\mbbt$, $\acute{v}^{\dd}(s) \neq
        \acute{v}^{\dd}(\novel)$. This allows us to appeal to \cref{lem-insep}
        and take $\aextb$ to be the associated novel generalization.
        Moreover, $\aext$ is testworthy by virtue of $\acute{\eta}^{P} \in
        \posreal^{P}$, so that $\asextb_{\novel} = P$ and $\aextb_{\novel} =
        \hextb_{\novel} = \hextb_{J\times 0}^{-1}$.

        Finally, for the existence of $t\in\mbbt$ such that $\hat{v}^{P}(t) =
        -\eta^{P}$, observe that \twodiv\ suffices for the existence of $L \in
        \mbbj$ such that $\hext_{L\times 0} = \hext_{J\times 0}^{-1}$.
        %\emph{Mutatis mutandis}, a repetition of the preceding argument by
        %contradiction confirms that there exists $t$ in $\mbbt$ such that
        %$\hat{v}^{P}(t) = - \eta^{P}$.
        % and the definition of $M$, In particular, note that the product of
        % $\acute{v}^{p} ( \no\acute{v}el )$ and $\acute{v}^p ( s^p )$ is
        % negative for every $s^{p}$ in $S^{P}$. This ensures that, for each
        % $t$ in $\mbbc_{/\sim^{\star}}$, we have a function
        % \begin{equation*} \rho ( \cdot ) = \acute{v}^{p} ( \cdot ) \acute{v}^{
        % p} ( s^{p
        %})\end{equation*}
        % on $\{t , \novel \}$ that is neither positive nor constant.
        %imply $\aextb _{\novel} =
        %\aextb _{J \times 0}^{- 1}$ since, for every $\xy$ in $P$, we have
        %\begin{linenomath*}
        %  \begin{equation*} \langle \acute{v}^{\xy} , J \times 0 \rangle =
        %    \langle \hat{v}^{\xy} , J \times 0 \rangle< 0 < \acute{\eta}^{\xy} =
        %  \acute{v}^{\xy} (\novel).\end{equation*}
        %\end{linenomath*}
        % \footnote{In summary,
        % as before, by setting $\acute{v}^{P} (\novel) = \eta^{P}$ and, for every
        % $t$ in $\mbbc_{/\sim^{\star}}$, $\acute{v}^{P}(t) = v^{P}(t)$, we generate a
        % novel generalization.
        % The associated generalization $\aext$ is testworthy since, for each
        % $\xy$ in $P$,
        % \begin{equation*}\langle \acute{v}^{\xy} , J\times 0 \rangle = \langle
        % {v}^{\xy}, J
        % \times 0 \rangle < 0 < \eta^{\xy} = \acute{v}^{\xy}(\novel).\end{equation*}
        %} 
        % $\R^{P^{-1}}$. Then, via the preceding argument, there exists
        % $s$ in $\mbbt$ such that ${v}^{P^{-1}}(s) = \eta^{P}$.  Now observe that
        % $(x,y)$ in $P^{-1}$ if, and only if, $(y,x)$ in $P$. \Cref{lem-insep}
        % then yields ${v}^{(x,y)}(s)= \eta^{(y,x)}>0$.  
        % We therefore conclude that, for every $\eta^{P} > 0$, there exist
        % $s, t$ in $\mbbt$ such that ${v}^{P}(s)= \eta^{P}$ and
      \end{proof}

      \Cref{claim-test-empty} implies that, when every testworthy
      $Y$-generalization is regular, the cardinality of $\mbbt$ is equal to the
      cardinality of $\R^{P}$.  We now show that \fourdiv\ holds on $Y$. Let
      ${R}$ denote an arbitrary total ordering of $Y$.  We show that, for some
      $K$ in $\mbbj$, $\langle \hat{v}^{\xy}, K \rangle \geq 0$ if, and only if,
      $\xy$ belongs to ${R}$. \Cref{claim-test-empty} ensures that we can choose
      $s$ in $\mbbt$ such that, for some $0 < \epsilon <1$
      \begin{linenomath*}
        \begin{equation*}
          \hat{v}^{\xy} ( s ) = \left \{
            \begin{array}{l l}
              1+ \epsilon & \text{if $\xy$ in ${R} \cap P $},\\
              1-\epsilon & \text{if $\xy$ in ${R}^{- 1} \cap P $}.
            \end{array}
          \right.
        \end{equation*}
      \end{linenomath*}
      % Note that, since ${R}$ is a total order and $P$ is the asymmetric part of
      % a total order, for every $p$ that does not belong to
      % ${R} \cup{R}^{- 1}) \cap P$, either $p$ in $P^{- 1}$ or $p = x \times x$.
      % By \cref{lem-insep}, for every $p$ in $P^{- 1}\cap{R} \cup{R}^{-1})$,
      % $v^{p} ( s ) = - v^{p^{- 1}} ( s )$ (and, for every
      % $p$ such that $p = x \times x$, $v^{p} ( s ) = 0$).
      Via \cref{claim-test-empty}, take $t$ in $\mbbt$ such that, for every $\xy$
      in $P$, $\hat{v}^{\xy} ( t ) = -1$.  Let $K : = \delta _{s} + \delta _{t }$
      in $\mbbjp$, so that $\langle \hat{v}^{\xy}, K \rangle = \hat{v}^{\xy}(s) +
      \hat{v}^{\xy}(t)$. By evaluating terms and observing that $\epsilon > 0$ we
      obtain
      \begin{linenomath*} 
        \begin{equation*}
          \langle \hat{v}^{\xy}, K \rangle = \left \{
            \begin{array}{l l}
              (1+ \epsilon) - 1 > 0 & \text{if $\xy$ in ${R} \cap P $},\\
              (1- \epsilon) - 1 < 0 & \text{if $\xy$ in ${R}^{-1}\cap P$}.
            \end{array}
          \right.
        \end{equation*}
      \end{linenomath*} 
      Since $\xy$ in ${R}^{-1} \cap P^{-1}$ if, and only if, $\yx$ in ${R}\cap P$
      (and, similarly, $\xy$ in ${R} \cap P^{-1}$ if, and only if $\yx$ in
      ${R}^{-1}\cap P$), we appeal to $\hat{v}^{\xy} = - \hat{v}^{\yx}$ and
      obtain
      \begin{linenomath*}
        \begin{equation*}
          \langle \hat{v}^{\xy}, K \rangle = \left \{
            \begin{array}{l l}
              -( 1 + \epsilon) + 1 < 0 & \text{if $\xy$ in ${R}^{-1} \cap P^{-1}$,}\\
              -(1- \epsilon) + 1 > 0 & \text{if $\xy$ in ${R} \cap P^{-1} $.}
            \end{array}
          \right.
        \end{equation*}
      \end{linenomath*}
      Since $P$ is the asymmetric part of a total ordering we conclude that, for
      every $x\neq y$, $\langle \hat{v}^{\xy}, K \rangle$ has the right sign.
      Finally, for $x = y$, $\langle \hat{v}^{\xy}, K \rangle = 0$.
    \end{proof}
  \end{step}
  %\emph{As a consequence of \cref{lem-test-empty-fourdiv},} for
  %the remainder of the proof of \cref{thm-foureq} we work under the assumption
  %that the set of testworthy $Y$-generalizations that are novel is nonempty. 

  \begin{step}[The case where $Y$ has cardinality $3$]
    \begin{lemma}\label{lem-Y3-r2}  If $Y = \{x,y,z\}$ and $\mathbf r = 2$, then
      \fourjac\ and \stability\ hold on $Y$.
    \end{lemma}
    \begin{proof}[Proof of \cref{lem-Y3-r2}] Fix arbitrary $J \in \mbbj$ such
      that $\ext_{J}$ is total.  We first apply Zaslavski's theorem to prove that
      the $Y$-generalization $\aext$ of \cref{prop-central-testworthy} (which,
      recall, is testworthy relative to $J$) satisfies $\lvert \acute{\mc
      G}_{\mpplus}\rvert = 6$.
       
      Via \cref{prop-central-testworthy} the arrangement
      $\acute{\mc{H}}_{\mpplus}$ is central. Thus
      $\acute{\mc{H}}_{\mpplus} \equiv \acute{\mc{H}}$ and we may drop
      reference to the subscript $\mpplus$. Via \cref{lem-c2dQ},
      $\countof{\acute{\mc{H}}} = 3$. Moroever, since every subarrangement of
      $\acute{\mc{H}}$ is central, for every $k = 0, 1, 2, 3$ there are
      $\binom{3}{k}$ ways to choose $\lvert \mc A \rvert = k$ hyperplanes from
      $\acute{\mc{H}}$. For $k < 3$, the rank of every subarrangement is $k$. For
      $k = 3$, the rank of the arrangement is $\acute{\mathbf r}$. Via
      \cref{prop-central-testworthy} $\acute{\mathbf r} = \mathbf r = 2$. Thus
      \begin{linenomath*}
        \begin{equation}\label{eq-zaslavski-3}
          \begin{aligned}
            \lvert \acute{\mc G}\rvert =& \binom{3}{3} (-1)^{3
            -\acute{\mathbf{r}}}+\binom{3}{2}(-1)^{2-2}
            +\binom{3}{1}(-1)^{1-1} + \binom{3}{0}(-1)^{0-0} = 6. %\\
            % =&(-1)^{1}+3(-1)^{0}+ 3(-1)^{0}+(-1)^{0} = 6.
          \end{aligned}
        \end{equation}
      \end{linenomath*}
      For both \fourjac\ and \stability, we require that every member of
      $\acute{\mc{G}}$ is associated with a total ordering.  \threejac\ then
      holds because we have the conditions (\ref{TQ}-\ref{AQ} and \threediv) to
      apply lemma 2 of \gsii.  \stability\ then holds since, for every testworthy
      $\hext$ that is novel and satisfies $\hextb_{\novel}=\extb_{J}^{-1}$ and
      \ref{KQ}-\ref{AQ}, $\aext$ is a {{diverse}} perturbation of $\hext$ that
      satisfies \ref{TQ}-\ref{AQ}.
       
      It remains for us to show that every member of $\acute{\mc G}$ is
      associated with a total ranking of $Y$. In particular, since every
      associated ranking is CAR (see \cref{sec-online}) it suffices to prove
      transitivity.  For this, note that, via \ref{prop-c2dQ} four of the six
      members of $\acute{\mc{G}}$ intersect $S_{\mpplus} \times 0$ and are
      therefore associated with transitive rankings. Note that, since
      $S_{\mpplus}$ is connected, the remaining two members are adjacent
      (separated by a single member of the arrangement).  Take $G$ to be one of
      the remaining members of $\acute{\mc{G}}$ and take $L \in G$. Define the
      affine path $\lambda \mapsto \phi(\lambda)= (1-\lambda)\acute{L} + \lambda
      \acute{J}$, where $\acute{J}$ belongs to the center $H^{\{x, y, z\}}$ of
      $\acute{\mc{H}}$. For $\lambda$ sufficiently close but greater than one,
      $\aextb_L = \aextb_{\phi(\lambda)}^{-1}$ because $\phi(1) = \acute{J}$
      belongs to the center. Thus, $\aext_{L}$ is transitive.
    \end{proof}

    \begin{lemma}\label{lem-Y3-r3} If $Y = \{x,y,z\}$ and $\mathbf{r} = 3$, then
      neither \stability\ nor \fourjac\ hold.
    \end{lemma}
    \begin{proof}[Proof of \cref{lem-Y3-r3}] When $\mathbf r = 3$, $u^{\xy},
      u^{\yz}$ and $u^{\xz}$ are linearly independent, so that \threejac\ fails
      to hold. We now confirm that \threepru\ also fails to hold.  Via
      \cref{prop-central-testworthy}, $\acute{\mathbf r} = \mathbf r$. We now
      apply \cref{eq-zaslavski-3} with $\mathbf{r}=3$:
      \begin{linenomath*}
        \begin{align*}
          \lvert \acute{\mc G}\rvert = (-1)^{0}+3(-1)^{0}+ 3(-1)^{0}+(-1)^{0} =
          8.
        \end{align*}
      \end{linenomath*}
      Then there are $3!=6$ members of $\total (\aext)$, and the two additional
      regions of $\acute {\mc G}$ are associated with intransitive CAR rankings.
      It remains for us to show that every $Y$-generalization $\hext$ with
      $\countof \total (\hext) = 6$ fails to satisfy \ref{TQ}.
      % This follows from the other
      % version of Zaslavski's theorem in relation to which we present the relevant
      % Hasse diagram in \cref{fig-hasse-Y3-r3}.

      % In \cref{eq-zaslavski-3}, the first term corresponds to the member of the
      % intersection semilattice with the highest rank. That is the term with the
      % lowest dimension (highest co-dimension) in $S_{\mpplus}$. This is the
      % center $H^{\{x, y, z\}}$ of the arrangement. There is just one such term. 

      Recall \cref{fig-rates-sempi}. If $\hext$ are the sentiments corresponding
      to this arrangement, then $\lvert\hat{\mc{G}}\rvert = 7$. This value is
      achieved by dropping the first term in \cref{eq-zaslavski-3}. That is,
      since $\hat{A}^{\{x,y,z\}}$ is the only member of $\hat{\mc{L}} \bs
      \hat{\mc{L}}_{\mpplus}$. We may reduce $\lvert\hat{\mc{G}}\rvert$
      further by excluding one of the intersections of two hyperplanes such as
      $\hat{H}^{\{x,y\}}\cap\hat{H}^{\{y,z\}}$. That is
      $\hat{A}^{\{x,y\},\{y,z\}}$. In terms of \cref{eq-zaslavski-3}, this
      amounts to excluding one of the $\binom{3}{2} = 3$ central subarrangements.
      This would reduce $\lvert\hat{\mc{G}}_{\mpplus}\rvert$ to $6$. To
      obtain $\hext$ satisfying \ref{TQ}, we would need to remove all
      $\binom{3}{2}$ central subarrangements of two hyperplanes. But this would
      reduce $\lvert\hat{\mc{G}}_{\mpplus}\rvert$ to $4$.
    \end{proof}
  \end{step}
  \begin{step}[The case where $Y$ has cardinality $4$]
    Note that a
    failure of \threejac\ on $Z \subset Y$ such that $\lvert Z \rvert = 3$
    implies a failure of \fourjac\ on $Y$. And since the arguments for the case
    where $\lvert Y \rvert = 3$ account for the case where \threejac\ fails, we
    henceforth assume that \threejac\ holds on $Y$. That is, our conditionally
    \emph{2}-diverse representation $u^{\dd}$ will now satisfy equations
    \eqref{eq-xy}--\eqref{eq-xz} with $\hat{\beta} = \beta$ if, and only if,
    \fourjac\ holds on $Y$.

    First some some useful results that exploit \threejac.
    \begin{proposition}\label{lem-2r3}
      If $Y = \{x, y, z, w\}$ and \threejac\ holds for $u^{\dd}$, then, for every
      $\acute{v}^{\dd} = u^{\dd} \times \acute{\eta}^{\dd}$ with rank
      $\acute{\mathbf r}$, that satisfies \threejac,
      $2\leq \acute{\mathbf r}\leq 3$.
    \end{proposition}
    \begin{proof}[Proof of \cref{lem-2r3}]
      Via \cref{prop-c2dQ} and \cref{lem-Y3-r2}, $\mathbf r \geq 2$. Indeed the
      span of $\{u^{\xw}, u^{\yw}\}$ is two. Let $S$ denote the span of
      $\{u^{\xw}, u^{\yw} ,u^{\zw}\}$.  Since $u^{\yw} = - u^{\wy}$ and $u^{\dd}$
      satisfies \threejac, equations \eqref{eq-xy}--\eqref{eq-xz} hold for
      $u^{\dd}$.  (If \fourjac\ fails to hold, then $\beta \neq \hat \beta$, but the
      equations still hold.) Thus $u^{\xy}$, $u^{\yz}$ and $u^{\xz}$ all belong to
      $S$ and $\mathbf{r} \leq 3$. Now note that above argument does not depend on
      the cardinality of $\mbbt$, thus take $\acute{\eta}^{\dd}$ to
      satisfy \threejac: indeed with the same parameters that feature in
      equations \eqref{eq-xy}--\eqref{eq-xz} for $u^{\dd}$. The preceding
      argument then {generalizes} \emph{mutatis mutandis} to $\acute{v}^{\dd}$ and
      $2 \leq \acute{\mathbf r} \leq 3$.
    \end{proof}

    \begin{proposition}\label{lem-arrangement-cardinality-rank}
      If $Y = \{x, y, z, w\}$, then $4 \leq \lvert \mc H \rvert \leq 6$ and these
      bounds are tight. If, moreover, \threejac\ holds for $u^{\dd}$ and $\lvert
      \mc H \rvert < 6$, then $\mathbf{r} = 2$.
    \end{proposition}
    \begin{proof}[Proof of \cref{lem-arrangement-cardinality-rank}]
      The upper bound $\lvert \mc H \rvert \leq 6$ follows because there are
      $\binom{4}{2} = 6$ ways to choose distinct pairs of elements from $Y$.  Via
      \cref{lem-c2dQ} at most the following equalities are feasible: $H^{\{x,y\}} =
      H^{\{z,w\}}$, $H^{\{x,z\}} = H^{\{y,w\}}$ and $H^{\{y,z\}} = H^{\{x,w\}}$.
      \emph{Ad absurdum} suppose all three equalities hold, so that
      $\lvert\mc{H}\rvert=3$. Via \twodiv, all six hyperplanes partition
      $S_{\mpplus}$, so \withoutlog\ suppose that $R=(x,y,z,w)$ and its inverse
      $R^{-1}$ both feature in $\ext$.  Consider a convex path in
      $S_{\mpplus}$ from $G^{R}$ to $G^{R^{-1}}$.  Since $H^{\{x,y\}} =
      H^{\{z,w\}}$, it follows that both support $G^{R}$.  Thus, for
      $R'=(y,x,z,w)$, $G^{R'}$ is adjacent to $G^{R}$. Continuing along the convex
      path we find that $H^{\{x,z\}}$ supports $G^{R'}$. But then
      $H^{\{x,z\}}=H^{\{y,w\}}$ yields a contradiction of \ref{TQ}.

      We now prove that \threejac\ and $H^{\{x,y\}} = H^{\{z,w\}}$ together imply
      $\mathbf r = 2$. Consider equations \eqref{eq-xy}--\eqref{eq-xz} (so
      \threejac\ holds, but \fourjac\ need not). Via \eqref{eq-xy}, $S =
      \{u^{\xw}, u^{\wy}, u^{\xy}\}$ is 2-dimensional. Since $u^{\xy}$ and
      $u^{\zw }$ are collinear, $u^{\zw}$ belongs to $S$. Finally, equations
      \eqref{eq-yz} and \eqref{eq-xz} yield $u^{yz} , u^{\xz} \in S$.
    \end{proof}
    \begin{lemma}\label{lem-r3-Y4} If $Y = \{x,y,z,w\}$, $\mathbf{r} = 3$ and
      \threejac\ holds on $Y$, then \stability\ and \fourjac\ both hold on $Y$.
    \end{lemma}
    \begin{proof}[Proof of \cref{lem-r3-Y4}] To see that \fourjac\ holds, appeal
      to the proof of lemma 3 of \gsii: if \threejac\ holds and \fourjac\ does
      not, then $\{u^{\xw}, u^{\yw}, u^{\zw}\}$ is linearly dependent. This is in
      contradiction of $\mathbf{r} = 3$.
      
      We now verify that \stability\ also holds.  Since $\mathbf{r}=3$, the
      contrapositive of \cref{lem-arrangement-cardinality-rank} implies that
      $\lvert\mc H_{\mpplus}\rvert = 6$.  Via \cref{prop-central-testworthy},
      there exists a testworthy $Y$-generalization $\aext$ with an arrangement
      $\acute{\mc{H}}_{\mpplus}$ that is central and rank
      $\acute{\mathbf{r}}=\mathbf{r}$. Since $\acute{\mc{L}}_{\mpplus} =
      \acute{\mc{L}}$, we drop reference to $\mpplus$. The rank of
      subarrangements with cardinality $4$ or more is $\acute{\mathbf{r}}$.  Let
      $\acute{\mathbf{\uptau}}$ denote the number of subarrangements $\mc A$ that
      have cardinality $3$ and rank $2$.  Each of the other
      $\binom{6}{3}-\acute{\uptau}$ subarrangements with cardinality $3$ have rank
      $\acute{\mathbf r}$. All other subarrangements have rank equal to their
      cardinality.
    \begin{linenomath*}
      \begin{equation}\label{eq-zaslavski-4}
        \begin{aligned}
          \lvert \acute{\mc G} \rvert =&\binom{6}{6} (-1)^{6 -
          \acute{\mathbf{r}}}+\binom{6}{5} (-1)^{5 - \acute{\mathbf{r}}}+
          \binom{6}{4}(-1)^{4-\acute{\mathbf{r}}}\\
          +&\binom{6}{3}(-1)^{3-\acute{\mathbf{r}}}-\,\,\,\,\acute{\mathbf{\uptau}}
          \,\,\,\,\,(-1)^{3-\acute{\mathbf{r}}} + \,\,\,\,\acute{\mathbf{\uptau}}
          \,\,\,\,\,(-1)^{3-2}\\
          +&\binom{6}{2}(-1)^{2-2}+\binom{6}{1}(-1)^{1-1} + \binom{6}{0}(-1)^{0-0}
        \end{aligned}
      \end{equation}
    \end{linenomath*} 
    We claim that $\acute{\uptau}=4$. Each of the $\binom{4}{3} = 4$ subsets of $Y$
    that have cardinality $3$ generates a subarrangement of cardinality
    $\binom{3}{2} = 3$.  (For instance, $\mc A^{\{x,y,z\}} =
    \left\{\acute{H}^{\{x,y\}},\acute{H}^{\{y,z\}},\acute{H}^{\{x,z\}}\right\}$.)
    For such subarrangements, \fourjac\ implies a rank of $2$. Arguments from the
    final step in the proof of \cref{lem-arrangement-cardinality-rank} confirm that
    every other subarrangement with cardinality $3$ has rank $3$.
    \Cref{eq-zaslavski-4} then implies
    \begin{linenomath*}
      \begin{equation*}
        \begin{aligned}
        \lvert \acute{\mc G} \rvert =
        % &\,\,\,1(-1)^{3}+6(-1)^{2}+15(-1)^{1}\\
        % +&20(-1)^{0}- 4(-1)^{0}+\,\,\,4(-1)^{1} \\
        % +& 15(-1)^{0}+6(-1)^{0}+\,\,\,1(-1)^{0} \\
        % =&
        -1+6-15+20-4-4+15+6+1=24 = 4!\hskip.5pt.
       \end{aligned}
     \end{equation*}
    \end{linenomath*}
    The fact that $\aext$ satisfies \ref{TQ} is an immediate consequence of
    \fourjac. \stability\ then follows, for, if $\cext$ is any novel, testworthy
    generalization that satisfies $\cextb_{\novel}=\extb_{J}^{-1}$, then $\aext$
    is a {{diverse}} perturbation of $\cext$ that satisfies \ref{TQ}-\ref{AQ}.
    \end{proof}

    \emph{In the remaining case, where $Y = \{x,y,z,w\}$ and $\mathbf r =
    2$,} the proof is complicated by the fact that maximally-diverse
    generalizations have centerless arrangements.  We begin by choosing
    $\acute{\eta}^{\dd}$ so as to construct $\acute{u}^{\dd} = u^{\dd} \times
    \acute{\eta}^{\dd}$ with $\acute{\mathbf r} = 3$.

    Since $\mathbf r = 2$, it follows that $u^{\xz}, u^{\yz}$ and $u^{\wz}$ form
    a linearly dependent set. Thus, for some $\pi, \rho \in \R$,
    \begin{linenomath*} 
      \begin{equation}\label{eq-iz}
        \pi u^{\xz} + \rho u^{\yz} = u^{\wz}.
      \end{equation}
    \end{linenomath*}
    Fix arbitrary $J \in \mbbj$ such that $\ext_{J}$ is total, and, as in
    \cref{prop-central-testworthy}, \withoutlog, we take $J \in
    G_{\mpplus}^{(x,y,z,w)}$.  Then, for $\acute{\iota} = \half$ and
    $\acute{J} = (1- \acute{\iota}) J \times \acute{\iota}$, let
    \begin{linenomath*}
      \begin{equation}\label{eq-xyz}
        \acute{\eta}^{\ij} = - \tfrac{1-\acute{\iota}}{\acute{\iota}} \langle
        u^{\ij}, J \rangle = - \langle u^{\ij}, J \rangle, \quad \text{for every
        $i,j\in \{x,y,z\}$.}
      \end{equation}
    \end{linenomath*}

    In the case where $\countof Y = 3$, \cref{eq-xyz} implies that the associated
    arrangement of hyperplanes $\acute{\mc H}_{\mpplus}$ is central. That
    is, recalling \cref{eg-zaslavski}, the associated positive intersection
    semilattice $\acute{\mc L}_{\mpplus}$ is isomorphic to $\acute{\mc L}$. 
    The structure of $\acute{\mc L}_{\mpplus}$ is determined by the rank
    $\acute{\mathbf{r}}$ of $\acute{\mc{U}} = \left\{\acute{u}^{\xy},
    \acute{u}^{\xz}, \acute{u}^{\yz}\right\}$.  Via \cref{lem-c2dQ}, the rank of
    $\acute{\mc{U}}$ satisfies $2 \leq \acute{\mathbf{r}} \leq 3$.

    \fourjac\ holds for the associated generalisation $\acute{\ext}$ if, and only
    if, $\acute{\mathbf{r}} = 2$. Observe that, by construction,
    $\acute{u}^{\dd}$ satisfies the Jacobi identity if, and only if, 



    such that $\acute{\mc G}$ is maximal. This is because e
    %Since $G^{(x,y,z,w)}_{\mpplus}$ is open and the inner product is
    %continuous, there exists a compact neighbourhood $N_{J} \subset
    %G^{(x,y,z,w)}_{\mpplus}$ of $J$.
    %such that, for every $L\in N_{J}$,
    %\begin{linenomath*}
    % \begin{equation*}
    % \langle u^{\yz}, L\rangle < - \langle u^{\wz}, L \rangle \quad \textup{if,
    % and only if,} \quad
    % \langle u^{\yz},J \rangle <-\langle u^{\wz},J \rangle.
    % \end{equation*}
    %\end{linenomath*}
    Let $f: G^{(x, y, z, w)}_{\mpplus} \rightarrow \acute{G}^{(x, y, z,
    w)}_{\mpplus}$ be the mapping $L \rightarrow (1 - \acute{\lambda}) L
    \times \acute{\lambda}$, where $\acute{\lambda}$ is the solution to
    $\acute{\eta}^{\yz} = -\tfrac{1 - \acute{\lambda}}{\acute{\lambda}} \langle
    u^{\yz}, L \rangle$.  In particular, substituting for $\acute{\eta}^{\yz}$
    using \cref{eq-xyz}, we obtain $\frac{1- \acute{\lambda}}{\acute{\lambda}} =
    \frac{\langle u^{\yz},J \rangle}{\langle u^{\yz}, L \rangle}$ and
    $\acute{\lambda} = \frac{\langle u^{\yz}, L\rangle}{\langle u^{\yz}, J
    \rangle + \langle u^{\yz}, L \rangle}$.  Since $J, L \in {G}^{\yz}$, all
    terms in the expression for $\acute{\lambda}$ are positive, so that $0 <
    \acute{\lambda} < 1$ and, via convexity of $\acute{G}^{(x, y, z,
    w)}_{\mpplus}$, $f$ is well-defined. As the quotient of continuous
    functions of $L$ (with the denominator $\langle u^{\yz}, J \rangle + \langle
    u^{\yz}, L \rangle$  bounded away from zero) $f$ is continuous.  Finally,
    $\lim_{L \rightarrow J}f(L) = \acute{J}$.

    Via \ref{p3dQ} and $J \in G^{(x, y, z, w)}_{\mpplus}$, $\langle u^{\yz},
    J\rangle \neq \langle u^{\zw}, J\rangle$ and both numbers are positive.  Via
    $u^{\wz} = - u^{\zw}$, it follows that $\zeta = \frac{\langle u^{\wz},
    J\rangle}{\langle u^{\yz}, J\rangle} < 0$ is the unique solution to 
    \begin{linenomath*}
      \begin{equation*}
        \langle \zeta u^{\yz} - u^{\wz}, J \rangle = 0.
      \end{equation*}
    \end{linenomath*}
    Continuity of the map $L \mapsto \frac{\langle u^{\wz}, L\rangle}{\langle
    u^{\yz}, L\rangle}$ on $G^{(x, y, z, w)}_{\mpplus}$ and the fact that
    the latter set is open suffices for the existence of a sequence $(L_n: n = 1,
    2, \dots)$, converging to $J$, such that, for every $n$, $L_n \mapsto \xi_{n}
    = \frac{\langle u^{\wz}, L_n\rangle}{\langle u^{\yz}, L_n\rangle}$ satisfies
    $\xi_n \neq \zeta$.

    Next, take $(\epsilon_n : n = 1, 2, \dots)$ be the following non-zero
    real-valued sequence that converges to zero as $L_n \rightarrow J$:
    \begin{linenomath*}
      \begin{equation} \label{eq-epsilon}
        \epsilon_n := \langle \xi_n u^{\yz} - u^{\wz}, J\rangle = \langle u^{\wz},
        \tfrac{1 - \acute{\lambda_n}}{\acute{\lambda_n}} L_n - J \rangle,
      \end{equation}
    \end{linenomath*}
    where $\frac{1 - \acute{\lambda_n}}{\acute{\lambda_n}} = \frac{\langle
    u^{\yz}, J\rangle}{\langle u^{\yz}, L_n\rangle}$ is defined as in the
    definition of $f$ above.
    %(We arrive at \ref{eq-epsilon} via bilinearity of the inner product and
    %simple substitution.)


    %\footnote{Substituting for $\frac{1- \acute{\lambda}}{\acute{\lambda}}$ in
    %the equation for $\epsilon$ in \eqref{eq-yzw} and rearranging yields a linear
    %equation
    %\begin{equation}\label{eq-k-linear}
    % k \langle u^{\yz} , L \rangle =\langle u^{\wz} , L \rangle
    % \end{equation}
    % in $L$, where $k = \tfrac{\epsilon+\langle u^{\wz},J \rangle}{\langle
    % u^{\yz},J\rangle}$. Note that, since $L \in G^{\zw}_{\mpplus}\cap
    % G^{\yz}_{\mpplus}$, $\langle u^{\wz}, L \rangle <0$ and $\langle
    % u^{\yz}, L \rangle > 0$; moreover, the same is true of $J$. Thus, $k<0$ or,
    % equivalently, $\epsilon < -\langle u^{\wz},J \rangle$.  If $\langle u^{\yz},L
    % \rangle < - \langle u^{\wz},L \rangle$, then, via \cref{eq-k-linear}, $k <
    % -1$. Moreover, our choice of $N_{J}$ is such that $\frac{\langle u^{\wz},J
    % \rangle}{\langle u^{\yz},J \rangle} < -1$. This ensures that, on $N_{J}$,
    % $\epsilon$ lies in a neighbourhood of zero, as required.}
    % %Thus, for any fixed $\epsilon \neq 0$, there is at least one
    % % solution to \cref{eq-yzw}. For even in the case where $\mbbt = \{s, t\}$
    % and % % $L = (1- \lambda)\delta_{s}+ \lambda \delta_{t}$, we have two
    %equations in two unknowns.


    For every $(i, j) \in \{y,z,w\}^2 \bs \{(y, z), (z, y)\}$, let
    $\acute{\eta}^{\ij} = - \frac{1 - \acute{\lambda}}{\acute{\lambda}} \langle
    u^{\ij}, L \rangle$.  For $(i, j) \in \{x, y , z, w\}^2 \bs \{(x, w), (w,
    x)\}$, let $\acute{u}^{\ij}:= u^{\ij} \times \acute{\eta}^{\ij}$.  To
    complete the definition of $\acute{u}^{\dd}$, we appeal to the fact that, via
    \threejac, $u^{\dd}$ satisfies equations \eqref{eq-xy}--\eqref{eq-xz}. In
    particular, from these equations, take parameters $\alpha$, $\beta$, and
    $\gamma$ and let $\acute{\eta}^{\xw}$ be the (unique) solution to the Jacobi
    identity
    \begin{linenomath*}
      \begin{equation}\label{eq-xw}
        \alpha \acute{\eta}^{\xw} = \gamma \acute{\eta}^{\xy} + \beta
        \acute{\eta}^{\yw} = -\langle \gamma u^{\xy} , J \rangle -
        \tfrac{1-\acute{\lambda}}{\acute{\lambda}}\langle\beta u^{\yw}, L
        \rangle.
      \end{equation}
    \end{linenomath*}
    For these parameter values, $\acute{u}^{\dd}$ also satisfies
    \eqref{eq-xy}--\eqref{eq-xz} of the proof of \cref{lem-induction}. That is,
    for $\{x,y,z\}$, via \cref{eq-xyz} and \eqref{eq-xz}, $\gamma
    \acute{\eta}^{\xy}+\tau \acute{\eta}^{\yz} = \phi \acute{\eta}^{\xz}$, so
    that $\acute{u}^{\dd}$ satisfies \eqref{eq-xz}. For $\{y,z,w\}$, Via
    \cref{eq-yzw} and \eqref{eq-yz}, $\hat{\beta} \acute{\eta}^{\yw}+\sigma
    \acute{\eta}^{\wz} = \tau \acute{\eta}^{\yz}$, so that $\acute{u}^{\dd}$
    satisfies \eqref{eq-yz}. For $\{x,y,w\}$, via \cref{eq-xw}, $\acute{u}^{\dd}$
    satisfies \eqref{eq-xy}.

    Now note that, for every $L \neq J$,
    \begin{linenomath*}
      \begin{equation}\label{eq-3d-epsilon}
        \pi \acute{\eta}^{\xz} + \rho \acute{\eta}^{\yz} = \acute{\eta}^{\wz} +
        \epsilon \neq \acute{\eta}^{\wz}.
      \end{equation}
    \end{linenomath*}
    Then, via \eqref{eq-3d-epsilon}, for every $\epsilon\neq 0$,
    $\{\acute{u}^{\xy}, \acute{u}^{\yz}, \acute{u}^{\wz}\}$ forms a linearly
    independent set.

    We now demonstrate that for the final triple $\{x,z,w\}$, the Jacobi identity
    holds if $\hat{\beta} = \beta$, and
    $\{\acute{u}^{\xw},\acute{u}^{\wz},\acute{u}^{\xz}\}$ has rank $3$ otherwise.

    First extract the parameters from equations \eqref{eq-xy}--\eqref{eq-xz} to
    obtain the matrix form
    \begin{linenomath*}
      \begin{equation}\label{eq-matrix-6}
        \begin{blockarray}{ccccccc}
          \text{\footnotesize$\xw$} & \text{\footnotesize$\yw$} &
          \text{\footnotesize$\xy$} &\text{\footnotesize $\wz$}&
          \text{\footnotesize $\yz$} & \text{\footnotesize $\xz$}\\
          \begin{block}{[ccc|ccc]c}
            \alpha & -\beta & -\gamma & 0 & 0 & 0 &
            \text{\footnotesize\eqref{eq-xy}} \\
            0 & \hat{\beta} & 0 & \sigma& -\tau & 0 &
            \text{\footnotesize\eqref{eq-yz}}\\
            0 & 0 & \gamma & 0 & \tau & -\phi & \text{\footnotesize\eqref{eq-xz}}\\
          \end{block}
        \end{blockarray}
      \end{equation}
    \end{linenomath*}
    Since the triple $\{\acute{u}^{(i,z)}: i = x,y,w\}$ provides a basis for
    $\spann(\acute{u}^{\dd})$, we will write all vectors in terms of this basis.
    To this end, we derive the reduced row echelon form of \cref{eq-matrix-6}. In
    particular, letting $r_{i}$ denote the rows of the matrix, we perform the
    operation $r_{1} \mapsto r_{1} + \frac{\beta}{\hat{\beta}} r_{2} + r_{3}$ to
    obtain
    \begin{linenomath*}
      \begin{equation}\label{eq-matrix-6-echelon}
        \begin{blockarray}{ccccccc}
          \text{\footnotesize$\xw$} & \text{\footnotesize$\yw$} &
          \text{\footnotesize $\xy$} & \text{\footnotesize$\wz$} &
          \text{\footnotesize $\yz$} & \text{\footnotesize $\xz$}\\
          \begin{block}{[ccc|ccc]c}
            \alpha & 0 & 0 & \frac{\beta}{\hat{\beta}} \sigma&
            (1-\frac{\beta}{\hat{\beta}}) \tau & -\phi &
            \text{\footnotesize\eqref{eq-xy}} \\
            0 & \hat{\beta} & 0 & \sigma& -\tau & 0 &
            \text{\footnotesize\eqref{eq-yz}}\\
            0 & 0 & \gamma & 0 & \tau & -\phi & \text{\footnotesize\eqref{eq-xz}}\\
          \end{block}
        \end{blockarray}.
      \end{equation}
    \end{linenomath*}
    In \cref{eq-matrix-6-echelon}, the fact that $\hat{\beta}$ (instead of
    $\beta$) that appears as a pivot in column $2$, is a consequence of the fact
    that, in this derivation, we are choosing $\acute{v}^{\yw} = \hat{\beta}
    \acute{u}^{\yw}$.
    %\emph{Mutatis mutandis}, the conclusions we will draw are the
    %same if instead we chose to define $\acute{v}^{\yw}$ using $\beta$.
    The other (relevant) rows of $\acute{v}^{\dd}: Y^{2} \times \mbbtp
    \rightarrow
    \R$ are $\acute{v}^{\xw} = \alpha \acute{u}^{\xw}$, $\acute{v}^{\xy} = \gamma
    \acute{u}^{\xy}$, $\acute{v}^{\zw}= \sigma\acute{u}^{\zw}$, $\acute{v}^{\yz} =
    \tau \acute{u}^{\yz}$ and
    $\acute{v}^{\xz} = \phi \acute{u}^{\xz}$.  % From \cref{eq-matrix-6-echelon},
    it
    % is clear that the Jacobi identity
    % $\acute{v}^{\xw} = \acute{v}^{\xz} + \acute{v}^{\zw}$ holds if, and only if,
    % $\hat{\beta}= \beta$.  We now confirm that
    % $\{\acute{v}^{\xw}, \acute{v}^{\xz}, \acute{v}^{\zw}\}$ has rank $2$ if, and
    % only if, $\hat{\beta} = \beta$.
    \begin{figure}[t]
      \begin{center}
        \begin{tikzpicture}
        \node[color=lightgray] (max) at (0,4) {\footnotesize$\{x,y,z,w\}$};
        \node (xyz) at (-6.55,2.75){\footnotesize$\{x,y,z\}$};
        \node (xyw) at (-4.66,2.75) {\footnotesize$\{x,y,w\}$};
        \node[color=lightgray] (xzlyw) at (-2.5,2.75) {\footnotesize$\{x,z\}\{y,w\}$};
        \node (xylzw) at (0,2.75) {\footnotesize$\{x,y\}\{z,w\}$};
        \node (xwlyz) at (2.5,2.75) {\footnotesize$\{x,w\}\{y,z\}$};
        \node (xzw) at (4.66,2.75) {\footnotesize$\{x,z,w\}$};
        \node (yzw) at (6.55,2.75) {\footnotesize$\{y,z,w\}$};
        \node (xy) at (-5,0.25) {\footnotesize$\{x,y\}$};
        \node (xz) at (-3,0.25) {\footnotesize$\{x,z\}$};
        \node (xw) at (3,0.25) {\footnotesize$\{x,w\}$};
        \node (yz) at (-1,0.25) {\footnotesize$\{y,z\}$};
        \node (yw) at (1,0.25) {\footnotesize$\{y,w\}$};
        \node (zw) at (5,0.25) {\footnotesize$\{z,w\}$};
        \node (min) at (0,-1) {\footnotesize$\emptyset$};
        \draw (min) -- (xy);
        \draw (min) -- (xz);
        \draw (min) -- (xw);
        \draw (min) -- (yz);
        \draw (min) -- (yw);
        \draw (min) -- (zw);

      \draw[color=lightgray] (max) -- (xyz);
        \draw[color=lightgray] (max) -- (xyw);
        \draw[color=lightgray] (max) -- (xzw);
        \draw[color=lightgray] (max) -- (yzw);
        \draw[color=lightgray] (max) -- (xylzw);
        \draw[color=lightgray] (max) -- (xzlyw);
        \draw[color=lightgray] (max) -- (xwlyz);

        \draw[color=blue](xyz)--(xy);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (xz);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (yz);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (zw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (yw);
        \draw[preaction={draw=white,-,line width=3pt},color=lightgray] (xzlyw) -- (yw);
        \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xwlyz) -- (yz);
        \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xwlyz) -- (xw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw) -- (xw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw) -- (yw);
        \draw[preaction={draw=white,-,line width=3pt},color=lightgray] (xzlyw) -- (xz);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyz) -- (yz);
        \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xylzw) -- (xy);
        \draw[preaction={draw=white,-,line width=3pt},color=patrickcolor1] (xylzw) -- (zw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyw)-- (xy);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xzw)  -- (xw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (yzw) -- (zw);
        \draw[preaction={draw=white,-,line width=3pt},color=blue] (xyz) -- (xz);
    \end{tikzpicture}
    \caption{\label{fig-hasse-centerless-Y4} The intersection semilattices
      $\acute{\mc L}$ and
      $\acute{\mc L}_{\mpplus} = \acute{\mc L} \bs \{A^{Y},
      \acute{A}^{\{x,z\}\{y,w\}}\}$ when $\countof \mbbt=2$, $\hat{\beta} = \beta$
      and $\epsilon$ is sufficiently small but distinct from zero.}
    \end{center}
    \end{figure}
    The matrix of the equation that now follows, is invertible if, and only if,
    $(1-\frac{\beta}{\hat{\beta}}) \neq 0$.
    \begin{linenomath*}
      \begin{equation}\label{eq-matrix-3}
        \begin{blockarray}{[c]}
          \acute{v}^{\xw}\\
          \acute{v}^{\xz}\\
          \acute{v}^{\zw}
        \end{blockarray}
        =
        \begin{blockarray}{[ccc]}
          -\frac{\beta}{\hat{\beta}}  & -(1-\frac{\beta}{\hat{\beta}}) & 1 \\
          0 & 0 & 1\\
          -1 & 0 & 0\\
        \end{blockarray}
        \begin{blockarray}{[c]}
          \acute{v}^{\wz}\\
          \acute{v}^{\yz}\\
          \acute{v}^{\xz}
        \end{blockarray}
      \end{equation}
    \end{linenomath*}
    Thus, unless $\hat{\beta} = \beta$, we conclude that
    $\{\acute{v}^{\xw}, \acute{v}^{\xz},\acute{v}^{\zw}\}$ has the same rank as
    $\{\acute{v}^{\wz}, \acute{v}^{\yz}, \acute{v}^{\xz}\}$ which, by construction,
    has rank $3$ for every choice of $\epsilon \neq 0$.

    Since $\hat{\beta}= \beta$ if, and only if, \fourjac\ holds for $\ext$, we
    conclude that \fourjac\ holds for $\ext$ if, and only if, it holds for
    $\aext$. It remains for us to show that, for $\epsilon$ sufficiently small
    $\acute{\mc G}$ is maximal. For if $\acute{\mc G}$ is maximal, then via
    \cref{lem-Y3-r2} and \cref{lem-Y3-r3}, \ref{TQ} holds if, and only if,
    $\rank\{\acute{v}^{\xw}, \acute{v}^{\xz},\acute{v}^{\zw}\} = 2$.

    By Zaslavski's theorem, it suffices to show that every member of the
    intersection lattice $\acute{\mc L}$, other than the center
    $\acute{A}^{\{x,y,z,w\}}$, has nonempty intersection with
    $\posreal^{\mbbtp}$. We now make explicit the dependence of the
    generalization $\aext$ on our choice of $L \in N_{J}$, though we do so
    indirectly via $\epsilon$.  Let $d^{\epsilon} = \max
    \{\diff(\acute{J},\acute{A}): \acute{A} \in \acute{\mc
    L}^{\epsilon}\}$, where $\diff(\acute{J},\acute{A})$ is the minimum
    (Euclidean) distance between $\acute{J}$ and the (closed) linear subspace
    $\acute{A}$ of
    $\R^{\mbbtp}$. Note that for $\epsilon = 0$, we obtain a central arrangement
    of the form of \cref{prop-central-testworthy} with $d^{\epsilon} = 0$.
    Moreover, since the Euclidean metric is continuous in its arguments, and, for
    every $\epsilon$, $\acute{\mc L}^{\epsilon}$ is finite, the map $\epsilon
    \mapsto d^{\epsilon}$ is continuous and $\lim_{\epsilon \rightarrow 0}
    d^{\epsilon} = 0$. Thus, for sufficiently small $\epsilon \neq 0$, every
    $\acute{A} \in \acute{\mc L}^{\epsilon}$ intersects $\posreal^{\mbbtp}$.

    \begin{remark*}
      We note that the above arguments apply without modification to the case
      where $\countof \mc H = 4,5$. Consider, for example, the Hasse diagram of
      \cref{fig-hasse-centerless-Y4}. That case arises when $u^{\xz}$ and
      $u^{\yw}$ are collinear, so that $A^{\{xz\}\{y,w\}}$ is a hyperplane of
      dimension $\countof \mbbt -1$. Assuming the same construction, with
      $\epsilon \neq 0$, so that $\acute{u}^{\dd}$ has rank $3$ and, via
      \cref{lem-arrangement-cardinality-rank}, $\acute{u}^{\xz}$ and
      $\acute{u}^{\yw}$ are linearly independent. Thus
      $\acute{A}^{\{xz\}\{y,w\}}$ is of dimension $\countof \mbbtp - 2 = \countof
      \mbbt - 1$. Thus, $\acute{A}^{\{x,z\}\{y,w\}} = A^{\{x,z\}\{y,w\}} \times
      \{0\}$ which belongs to the boundary of $\posreal^{\mbbtp}$. At $\epsilon =
      0$, $\acute{A}^{\{x,z\}\{y,w\}}$ increases by one dimension and the upper
      two levels of the Hasse diagram collapse to equal $A^{\{x,y,z,w\}}$.
    \end{remark*}
  \end{step}
  %\newpage \makeatletter
  %\def\@seccntformat#1{Proofs\,\csname the#1\endcsname.\quad}
  %\makeatother
  %---------------------------------------------------------------------------%
  \section{Proofs of lemmas, observations and propositions}
  %---------------------------------------------------------------------------%
  \printProofs
  %---------------------------------------------------------------------------%
  \section{Online Appendix}\label{sec-online}
  %---------------------------------------------------------------------------%
  \subsection{First some observations and ancillary results}
  \begin{observation}\label{obs-reg-eq}%
    Let $\ext$ and $\aext$ respectively be regular and improper
    $Y$-generalizations. For every $C \in \mbbdp$, there exists $D \in \mbbd$
    such that $C \sim^{\extb} D$ and $\extb_{C} = \aextb_{D}$.\footnote{This
      means that there is a canonical embedding of $\left\{C \times \extb_{C}: C
      \in \mbbdp\right\}$ in $\{D \times \aextb_{D}: D \in \mbbd\}$. The
      converse embedding follows from the nonrevision condition of
    \cref{def-generalization}.}
  \end{observation}
  \begin{proof}\label{proof-reg-eq} Fix $Y\subseteq X$ nonempty and $\aext$
    regular.  \Wlog, take $C \in \mbbdp \bs \mbbd$, so that $C$ contains at least
    one copy of $\novel$.  For any $c \in C \cap [ \novel ]$, the fact that
    $\aext$
    is regular implies that $c \sim^{\aextb} c _ 1$ for some $c _ 1 \in \mbbc$. 
    The richness assumption ensures that we may choose $c _ 1$ from the
    complement of $C$.  Then, since neither $c$ nor
    $c_{1}$ belong to $C _ 1 \defeq C \bs \{c\}$, $c \sim^{\aextb} c_{1}$ implies
    $\aextb_{C} = \aextb _ {C _ 1 \cup \{c_{1}\}}$.  If $c$ is the unique member
    of $C \cap [ \novel ]$, then the proof is complete.  Otherwise, using the
    fact that $C$ is finite, we may proceed by induction until we obtain a set $C
    _ n$ such that $C _ n \cap [\novel ]$ is empty and $D \defeq C _ n \cup \{c _
    1 , \dots , c _ n \}$ belongs to $\mbbd$.  Part \ref{item-preserving} of
    \cref{def-generalization} then implies $\aextb _ { D} = \preceqb _ {D} \cap
    Y^{2}$, so that, since $\ext$ is improper, $\aextb_{D}=\extb_{D}$.  Finally,
    since $C \sim^{\aextb} D$, $\aextb _ {C} = \aextb_{D}$, as required.
  \end{proof}

  \subsection{Arrangements of hyperplanes}

  From the mathematics of hyperplane arrangements, the main result to which we
  extensively appeal is Zaslavsky's theorem.  For any given generalization
  $\ext$, Zaslavsky's theorem allows us to use information about the
  intersections of hyperplanes in the arrangement to identify $\countof \total
  (\ext)$.  It does so by counting the collection $\mc G_{\mpplus}$ of
  open and connected subsets of $\R^{\mbbtpp} \bs \bigcup \{H_{\mpplus} :
  H_{\mpplus} \in \mc A \}$ are called the \emph{chambers} or
  \emph{regions} of the arrangement.  In the present setting, each chamber
  corresponds to a a complete, antisymmetric and reflexive, but possibly
  intransitive, \emph{CAR ranking} of the elements of $Y$. Every CAR ranking
  $R$ can be succinctly represented as an ordered tuple as in
  \cref{eg-tali,eg-rates}. For instance, take $Y = \{x, y , z\}$ and $x
  \mathbin{R} y \mathbin{R} z$, then the corresponding tuple
  \begin{linenomath*}
    \begin{equation*}
      l = \left \{
        \begin{array}{ll}
          (x,y,z)& \text{if $R$ is transitive}\\
          (x,y,z,x)& \text{if $R$ is intransitive.}\footnotemark
      \end{array}\right.
      % \footnotetext{Note that the intransitive list $(x,y,z,x)$ is
      % indistinguishable
      % from $(y,z,x,y)$ and $(z,x,y,z)$, but it is the inverse of $(x,z,y,x)$.} 
    \end{equation*}
  \end{linenomath*}
  The notation {generalizes} without exception to sets of cardinality $4$. For
  example $(x,y,z,x,w)$ represents the CAR ranking that is intransitive over
  $\{x,y,z\}$ and such that $w$ dominates every other member.

  \emph{The intersection semilattice} of any arrangement $\mc A$
  is the partially ordered (\emph{by reverse inclusion}) set $\mc L$ of
  intersections of members of $\mc A$. The unique minimal element is obtained
  by taking the intersection $A^{\emptyset}$ over the empty subarrangement $\mc
  A^{\emptyset}$ of $\mc A$ to obtain the ambient space itself. That is
  $A^{\emptyset}= \R^{\mbbtpp}$ or $\posreal^{\mbbtpp}$, depending on whether
  we are considering the lattice $\mc L$ or the lattice $\mc L_{\mpplus}$
  respectively. In \gsii, as a consequence of \fourdiv, $\mc H_{\mpplus}$
  is always central. In our setting, it is only $\mc H$ that is guaranteed to
  be central. In general an arrangement is central, if, and only if, its
  intersection semilattice has a unique maximal element \citep[proposition
  2.3]{stanley2007introduction}. Thus, if $\mc H_{\mpplus}$ is
  \emph{centerless}, then $\mc L_{\mpplus}$ is a meet semilattice with
  multiple maxima: as in \cref{eg-zaslavski}.  Extending our notation: if $Y =
  \{x,y,z,w\}$, then the unique intersection $A^{Y}$ is the (nonempty)
  \emph{center} of $\mc A^{Y} = \mc H$. By $A^{\{x,y,z\}}$, we mean the
  intersection over $\mc A^{\{x,y,z\}} \defeq \{H^{\{i,j\}}: \text{$i\neq j$ in
  $\{x,y,z\}$}\}$. Finally, by $A^{\{x,y\}\{z,w\}}$, we mean the intersection
  over $\mc A^{\{x,y\}\{z,w\}} \defeq \{H^{\{x,y\}},H^{\{z,w\}}\}$.

  \emph{Zaslavski's theorem} provides two distinct methods for counting
  the number of regions in an arrangement. The first states that
  \emph{$\countof{\mc{G}}$ is equal to the sum of the absolute values of the
  M\"{o}bius function $\bmu: \mc L \rightarrow \mbb Z$} which is defined
  recursively via
  \begin{linenomath*}
    \begin{equation}\label{eq-mobius}
      \bmu(A) = \left \{
        \begin{array}{ll}
          1 & \text{if $A = A^{\emptyset}$}\\
          -\sum\{\bmu(B):  A \subsetneq B\} & \text{otherwise.}%\footnotemark
      \end{array}\right.
    \end{equation}
  \end{linenomath*}
  % For $Y\subset X$, a given subarrangement $\mc A^{Y} = \{H^{\{i,j\}}:
  % \text{$i \neq j$in$Y$}\}$, we abbreviate and write
  % $\tilde\bmu(Y)\defeq \bmu( A^{Y})$. Fot the intersection
  % $A^{\{x,y\}\{z,w\}}$
  % of the subarrangement $\mc A^{\{x,y\} \{z,w\}} = \{H^{\{x,y\}},
  % H^{\{z,w\}}\}$,
  % we will write $\tilde\bmu (\{x,y\}\{z,w\}) \defeq \bmu
  % (A^{\{x,y\}\{z,w\}})$.
  The above definition of Zaslavski's theorem is explicitly provided by
  \citet{sagan1999why}. Specialised to the present setting, the more common
  \citep[see
  ][]{orlik1992arrangements,dimca2017hyperplane,stanley2007introduction}
  ``rank'' version of Zaslavski's theorem is
  \begin{linenomath*}
    \begin{equation*}\countof \mc G =\sum_{\substack{\mc A \subseteq \mc H\\
      \mc A \textup{\ central}}} (-1)^{\lvert \mc{A} \rvert - \rank(\mc{A})},
    \end{equation*}
  \end{linenomath*}
  where \emph{central} means that $\bigcap \{H: H \in \mc A\}$ is nonempty, and
  $\rank (\mc A)$ is the dimension of the space spanned by the normals to the
  hyperplanes in $\mc A$.
  %\footnote{Equivalently, $\rank(\mc A)$ is the
  %dimension of the orthogonal complement in $\R^{\mbbtpp}$ (or
  %$\R^{\mbbtpp}_{\mpplus}$) of the intersection over $\mc A$. Since the
  %intersection over the empty arrangement $\mc A^{\emptyset}$ is the ambient
  %space $\R^{\mbbtpp}$ (or $\posreal^{\mbbtpp}$), the only vector in
  %$\R^{\mbbtpp}$ that is orthogonal to the ambient space is $0$, $\rank (\mc
  %A^{\emptyset}) = 0$.}
  \begin{example}[a comparison of $\mc L$ and $\mc
    L_{\mpplus}$]\label{eg-zaslavski} Let $X = \{x , y , z \}$, $\mbbt =
    \{s,t\}$, and $u^{(x,y)} = 1 \times - 1$ and $u^{(y,z)} = 2 \times - 1$
    denote vectors in $\R^{\mbbt}$. Note that in this case $S =
    \spann\{u^{\xy}, u^{\yz}\}$ coincides with $\R^{\mbbt}$. We now apply the
    Jacobi identity and take $u^{(x,z)} = u^{(x,y)} + u^{(y,z)} = 3 \times - 2$
    and extend to the remaining pairs in $X^{2}$ using \cref{lem-insep}. Since
    these vectors are pairwise noncollinear and $\countof \mbbt = 2$, the
    associated arrangement $\mc H_{\mpplus} =
    \left\{H_{\mpplus}^{\{x,y\}},
    H_{\mpplus}^{\{y,z\}}, H_{\mpplus}^{\{x,z\}}\right\}$ consists of
    three pairwise disjoint lines that partition $\posreal^{\mbbt}$ and $\mc
    G_{\mpplus}$ has cardinality $4$.  We now confirm this using
    Zaslavski's theorem.
    \begin{figure}
      \begin{center}
        \begin{tikzpicture}
          \node (max) at (0,1.5) {\color{lightgray}$\{x,y,z\}$}; 
          \node (d) at (-2,0.25) {$\{x,y\}$};
          \node (e) at (0,0.25) {$\{y,z\}$};
          \node (f) at (2,0.25) {$\{x,z\}$};
          \node (min) at (0,-1) {$\emptyset$};
          \draw (min) -- (d);
          \draw (min) -- (e);
          \draw (min) -- (f);
          \draw[color = lightgray] (max) -- (d);
          \draw[color = lightgray] (max) -- (e);
          \draw[color = lightgray] (max) -- (f); 
          %\draw[preaction={draw=white, -,line width=6pt}] (a) -- (e) -- (c);
        \end{tikzpicture}
        \caption{\label{fig-hasse-Y3-r2} The intersection semilattice
            $\mc{L}_{\mpplus} = \mc L \bs A^{\{x,y,z\}}$.}
      \end{center}
    \end{figure}

    In the present setting, $A^{\emptyset}_{\mpplus} = \posreal^{\mbbt}$,
    and, via \cref{eq-mobius}, $\bmu(A^{\emptyset}_{\mpplus}) = 1$. Then,
    since $\posreal^{\mbbt}$ is the unique element in $\mc L_{\mpplus}$
    that (strictly) contains each hyperplane in $\mc H_{\mpplus}$,
    \cref{eq-mobius} yields $\bmu(A) = -\bmu(A^{\emptyset}_{\mpplus})$ for
    each $A \in \mc H_{\mpplus}$.  Now since the hyperplanes in
    $\mc{H}_{\mpplus}$ are pairwise disjoint, there are no further
    elements in $\mc{L}_{\mpplus}$. Thus
    \begin{linenomath*}
      \begin{equation*}
        \lvert \mc{G}_{\mpplus} \rvert= \sum_{A \in \mc{L}_{\mpplus}}
        \lvert \bmu(A) \rvert = 4.
      \end{equation*}
    \end{linenomath*}
    In contrast, although the structure of $\mc L$ is otherwise isomorphic to
    $\mc L_{\mpplus}$, since $\{0\} \subset \R^{\mbbt}$ is a subset of
    every hyperplane in $\mc H$, $\{0\}$ is the center $A^{\{x,y,z\}}$ of $\mc
    H$ and the maximal element of $\mc L$. Via \cref{eq-mobius} and the
    calculations of the previous paragraph, we obtain $\bmu(A^{\{x,y,z\}}) = -
    \left(\bmu(A^{\emptyset}) -3 \bmu(A^{\emptyset}) \right) = 2$.  Thus,
    \begin{linenomath*}
      \begin{equation*}
        \countof\mc G = \sum_{A \in \mc L} \lvert \bmu(A) \rvert = 6 = 3!.
      \end{equation*}
    \end{linenomath*}
    % First note that
    % there are four sub-arrangements $\mc A \subseteq \mc H$ that are central:
    % the
    % three singleton arrangements $\mc A^{\{x,y\}}, \mc A^{\{y,z\}}$ and
    % $\mc A^{\{x,z\}}$ that each consist of a single hyperplane with the same
    % index; and the empty arrangement (which contains no hyperplanes). Now
    % $\lvert \mbbt \rvert = 2$, and, for each singleton arrangement,
    % $\lvert \mc A^{\{i,j\}} \rvert - \rank ( \mc A^{\{i,j\}}) = 1 - 1$.
    % Whereas,
    % for the empty arrangement
    % $\lvert \mc A^{\emptyset} \rvert - \rank (\mc A^{\emptyset}) = 0 - 0$. As
    % a
    % consequence,
    % \begin{equation*}\lvert\mc G_{\mpplus}\rvert = (-1)^{4} \left(3\cdot
    % (
    % - 1 )^{1 - 1} + ( - 1 )^{0 -
    % 0} \right) = 4 < 3! = 6.\end{equation*}
  \end{example}
  \begin{remark}[The relationship between $\mc L$ and $\mc
    L_{\mpplus}$]\label{rem-lattice} Let $\aext$ be a $Y$-generalization
    with $2$-diverse representation $\acute{u}^{\dd}$. Since, for every
    distinct $x$ and $y$ in $Y$, $\acute{H}^{\{x,y\}}$ contains the origin,
    $\acute{\mc H}$ is centered. As we see in \cref{eg-zaslavski}, this is not
    the case for $\acute{\mc H}_{\mpplus}$ where $\acute{\mc
    H}_{\mpplus}$ is centerless and each of its members is maximal in
    $\acute{\mc L}_{\mpplus}$.

    In \gsii, \fourdiv\ guarantees that, for every $Y\subseteq X$ of
    cardinality $2,3$ or $4$, the improper $Y$-generalization generates a
    centered arrangement in $\posreal^{\mbbt}$. The fact that
    $\posreal^{\mbbtpp}$ is open in $\R^{\mbbtpp}$ ensures that the dimension
    of any $L \in \acute{\mc L}$ is equal to its counterpart $L _{\mpplus}
    \in \acute{\mc L}_{\mpplus}$ provided the latter exists.  Thus,
    $\acute{\mc L}_{\mpplus}$ and $\acute{\mc L}$ are isomorphic if, and
    only if, $\acute{\mc H}_{\mpplus}$ is centered. For the same reason,
    $\acute{\mc G}_{\mpplus}$ and $\acute{\mc G}$ are isomorphic if, and
    only if, $\acute{\mc H}_{\mpplus}$ is centered.
  \end{remark}

  We now abstract a useful property from \cref{eg-zaslavski}.
  \begin{proposition}\label{prop-pairwise-extremal} If
    $\preceqb_{\mbbj}$ satisfies \ref{KQ}–\ref{AQ} and \twodiv, then, for every
    $Y\subseteq X$ of cardinality $3$ or $4$, the improper $Y$-generalization
    $\extb$ is such that, for some $J,L \in\mbbj$, $\extb_{J} = \extb_{L}^{-1}
    $ belongs to $\total(\ext)$.
  \end{proposition}
  \begin{proof}%[Proof of \cref{prop-pairwise-extremal}]
    Fix $\countof Y= 3$ or $4$, via \cref{lem-insep}, let $v^{\dd}$ denote the
    $2$-diverse matrix representation of the improper $Y$-generalization $\ext$. Let
    $\mc H_{\mpplus}$ denote the associated arrangement of hyperplanes.  For
    every distinct $x,y\in X$, \cref{lem-insep} implies that
    $H^{\{x,y\}}_{\mpplus}$ intersects $\posreal^{\mbbt}$. Then, similar to
    \cref{eg-zaslavski}, the $1 \leq n \leq \binom{\countof Y}{2}$ distinct
    hyperplanes of $\mc H_{\mpplus}$ cut $\posreal^{\mbbt}$ into at least
    $n+1$ regions.  At least one pair $G$ and $G^{*}$ in $\mc G_{\mpplus}$
    are therefore separated by all $n$ distinct members of $\mc
    H_{\mpplus}$. Take $J \in G$, so that, for every distinct $x,y \in Y$,
    $\langle u^{\xy}, J \rangle \neq 0$.  Thus $\ext_{J}$ is antisymmetric,
    complete and, via \ref{TQ}, total. Next, take $L \in G^{*}$, so that since $J$
    and $L$ are separated by every hyperplane in $\mc H_{\mpplus}$,
    $\extb_{J}= \extb_{L}^{-1}$.
  \end{proof}

  % We now show that \twodiv\ is too weak for the purposes of \cref{thm-mainQ}.
  \begin{example}[insufficiency of \twodiv]\label{eg-lexicographic}
    Let $X = [0,1]^{2}$ and let $\leq^{\textup{lex}}$ denote the lexicographic
    ordering on $X$.  Let $\mbbt = \{s,t\}$, and, for each $J \in \mbbj$, let
    \begin{linenomath*}
    \begin{equation*}
    \preceqb_{J} = \left\{
      \begin{array}{ll}
        X^{2} & \text{if  $J(s) = J( t )\,;$}\\
        \leq^{\textup{lex}} & \text{if  $J(s) < J( t )\,;$}\\
        (\leq^{\textup{lex}})^{-1} & \text{otherwise.}
        \end{array}\right.
    \end{equation*}
    \end{linenomath*}
    Recall that if $\preceqb_{J} = X^{2}$, then $\preceqb_{J}$ is symmetric and
    hence equal to $\simeq_{J}$.  Thus, for every distinct $x,y\in X$,
    $H^{\{x,y\}}_{\mpplus}= \{J \in \R^{\mbbt}_{\mpplus} : J(s) =
    J(t)\}$. Via \cref{lem-insep}, $\preceqb_{\mbbj}$ has a two-diverse matrix
    representation $v^{\dd}$. But via \cref{lem-c2dQ}, below, \ref{c2d}\ fails to
    hold. The fact that $\preceqb_{\mbbj}$ fails to satisfy part \ref{rep-mainQ} of
    \cref{thm-mainQ} follows from the fact that $\preceqb_{J}$ is lexicographic for
    every $J$ outside $H$.
  \end{example}

  We now present the canonical intersection semilattices for the variety of cases
  that we consider in the proofs of the main paper.
  \begin{figure}
    \begin{center}
      \begin{tikzpicture}
        \node (max) at (0,3) {$\{x,y,z\}$};
        \node (a) at (-3,1.75)
        {$\{x,y\},\{y,z\}$};
        \node (b) at (0,1.75) {$\{x,y\},\{x,z\}$};
        \node (c) at (3,1.75) {$\{y,z\},\{x,z\}$};
        \node (d) at (-2,0.25) {$\{x,y\}$};
        \node (e) at (0,0.25) {$\{y,z\}$};
        \node (f) at (2,0.25) {$\{x,z\}$};
        \node (min) at (0,-1) {$\emptyset$};
        \draw (min) -- (d) -- (a) -- (max) -- (b) -- (f) (e)
          -- (min) -- (f) -- (c) -- (max) (d) -- (b);
        \draw[preaction={draw=white, -,line width=6pt}] (a) -- (e) -- (c);
      \end{tikzpicture}
      \caption{\label{fig-hasse-Y3-r3} The intersection semilattice of a central
          arrangement for $\countof{Y} = 3$ and ${\mathbf{r}} = 3$.}
    \end{center}
  \end{figure}

  In the intersection semilattice of \cref{fig-hasse-Y3-r3}, an increase in
  level corresponds to a decrease in dimension: since $\acute{A}^{\{x,y,z\}}$
  is nonempty, it is of dimension at least zero.  Since $\acute{J}$ belongs to
  the interior of $\posreal^{\mbbtp}$ and $\acute{A}^{\{x,y\}\{y,z\}}$ is at
  least one-dimensional, $\acute{A}^{\{x,y\} \{y,z\}}_{\mpplus}$ is
  one-dimensional.  Since $\acute{A}^{\{x,y,z\}} \subset
  \acute{A}^{\{x,y\}\{y,z\}}$, the latter set is nonempty whenever $\acute{\mc
  H}_{\mpplus}$ is central. The same, of course, applies to other members
  at the same level. Conversely, if $\acute{A}^{\{x,y\}\{y,z\}}$ is empty, then
  so is $\acute{A}^{\{x,y,z\}}$. We now use this to show that there is a unique
  form of $Y$-generalization $\hext$ such that $\countof \hat{\mc
  G}_{\mpplus} = 6$ and, moreover, that any such $\hext$ fails to satisfy
  \ref{TQ}.
  % The first lemma in the proof of \cref{thm-mainQ} builds on lemma 1 of \gsii.
  %\begin{theoremEnd}{lemma}[two-diverse pairwise representation]\label{lem-insep}
  %  Let $\aext$ be a $Y$-generalization of $\preceq_{\mbbj}$.  $\aext$
  %  satisfies \ref{KQ}--\ref{AQ} and \twodiv\ holds on $Y$, if, and only if,
  %  there exists a matrix $\acute{v}^{(\cdot,\cdot)} : Y^{2} \times \mbbtpp
  %  \rightarrow \R$ such that, for every $x,y\in Y$, row $\acute{v}^{(x,y)}:
  %  \mbbtpp \rightarrow \R$ and its associated spaces satisfy
  %  \begin{enumerate}[label=\textup{(\roman*)}]
  %     \item \label{K-insep} $\acute H^{\{x,y\}}_{\mplus} \cap \mbb Q^{\mbbtpp} =
  %       \{J: x \anext_{J} y\}$ and $\acute G^{\xy}_{\mplus} \cap \mbb
  %       Q^{\mbbtpp}= \{J: x \asext_{J} y\}$,
  %     \item\label{D-insep} $\acute{G}^{\xy}_{\mpplus}$ and
  %       $\acute{G}^{\yx}_{\mpplus}$ are both nonempty if $x \neq y$ and
  %       both empty otherwise,
  %     \item\label{skew-insep} $\acute H^{\{y,x\}}_{\mpplus} =\acute
  %       H^{\{x,y\}}_{\mpplus}$ (and in particular $\acute{v}^\yx = -
  %       \acute{v}^\xy$),
  %     \item\label{unique-insep} $\acute H^{\{x,y\}}_{\mpplus}$ is the
  %       unique hyperplane in $\posreal^{\mbbtpp}$ that separates $\{J : x
  %       \asextb_{J} y\}$ and $\{J: y \asextb_{J} x\}$.
  %  \end{enumerate}
  %
  %\end{theoremEnd}
  %\begin{proofEnd}%[Proof of \cref{lem-insep}]
  %  
  %%  In addition to \ref{KQ}–\ref{AQ}, the proof of lemma 1 of \gsii\ only appeals
  %%  to \twodiv. That lemma, like the present one, does not require \ref{TQ}, or
  %%  any diversity condition stronger than \twodiv\ since it is a result about
  %%  distinct pairs of elements $x$ and $y$ in isolation.
  %  
  %
  %  In the remainder of the proof of the present lemma, we suppress reference to
  %  the acute accent. 
  %
  %For the converse argument, fix arbitrary $t \in \mbbt$. Then
  %$\acute v^{\dd} (t) \neq \acute v^{\dd} (\novel)$ implies the existence of
  %distinct $x,y \in Y$ such that
  %$\acute v^{\xy}(t) \neq \acute v^{\xy}(\novel)$. We show that there exists
  %$J \in \mbbj$ and $L= J\times 0 \in \mbbjp$ satisfying \cref{eq-nov}. For then,
  %by retracing (in reverse order) the arguments that lead to \cref{eq-nov}, we
  %arrive at the conclusion that
  %$\extb _ {L + \delta_{t}} \neq \extb _ {L + \delta_{\novel}}$, as required
  %for $\ext$ to be novel.
  %
  %Take $\mu = v^\xy ( t )$ and $\xi = v^\xy ( \novel )$ and consider the
  %case where $\mu < \xi < 0$.  Since $\mu<0$, \twodiv\ implies that there exists
  %$s\in\mbbt$ such that $v^{\xy}(s)$ is positive.  Then, for some
  %$\lambda \in \posrat$, $- \lambda v^\xy ( s ) \in ( \mu , \xi )$.  Let
  %$L = \lambda \delta^{\novel} _ s$ and observe that
  %\begin{linenomath*}
  %  \begin{equation*} \mu < - \langle v^{\xy}, L \rangle < \xi ,\end{equation*}
  %\end{linenomath*}
  %  as required. \emph{Mutatis mutandis}, the case where both $\mu$ and $\xi$
  %  are positive is the same.  If $\mu \leq 0 \leq \xi$, then take $L = 0$, so
  %  that $\mu \neq \xi$ yields
  %  $\extb _ {L + \delta^{\novel} _ t} \neq \extb _ {L + \delta^{\novel}
  %    _ \novel}$.
  %\end{proofEnd}
  %
  % \begin{remark*}
  %   Note that although \cref{lem-insep} takes the generalization as primitive, we will
  %   also find it useful to reason in the opposite direction. For instance, let
  %   $v^{\dd}: Y^{2} \times \mbbt \rightarrow \R$ be a $2$-diverse representation
  %   of the improper $Y$-generalization $\ext$ and let
  %   $\acute{v}^{\dd}: Y^{2} \times \mbbtp \rightarrow \R$ be any matrix that
  %   agrees with $v^{\dd}$ on $Y^{2} \times \mbbt$. That is, any matrix $\acute{v}$
  %   with columns $\acute{v}^{\dd}(t) = v^{\dd}(t)$ for every $t \in \mbbt$. Now
  %   take $\aextb \in (Y\times Y)^{\mbbjp}$ such that for every $x,y\in Y$,
  %   $x \aext_{J} y$ if, and only if, $\langle \acute{v}^{\xy}, J \rangle \geq
  %   0$. Via \cref{lem-insep}, $\aext$ is a proper $Y$-generalization of
  %   $\preceq_{\mbbj}$.
  % \end{remark*}
  %\emph{An \emph{arrangement}} is a collection of hyperplanes in
  %$\posreal^{\mbbtpp}$ or $\R^{\mbbtpp}$. (A \emph{hyperplane in
  %  $\posreal^{\mbbtpp}$} is the positive kernel of some nonzero vector.)
  
      % \begin{comment
  %}   For any finite $Y\in 2^{X}$ and $Y$-generalization
  % $\ext$ with $2$-diverse representation $v^{\dd}$, consider the (finite)
  % arrangement
  % $\mc H_{\mpplus} \defeq \{H_{\mpplus}^{\{x,y\}} : x \neq y\} \subset \posreal^{\mbbtpp}$ of
  % hyperplanes generated by $\{v^{\xy}: x\neq y\}$. We will sometimes work with the
  % \emph{essentialization} $\ess(\mc H_{\mpplus})$ of $\mc H_{\mpplus}$ which has the nice
  % property that it is irreducible (in terms of dimension) whilst preserving all
  % the relevant structure of the arrangement. The usual essentialization is the
  % hyperplane arrangement that arises via the orthogonal projection
  % $\pi : \R^{\mbbtpp} \rightarrow V$, where $V$ is the row space of
  % $v^{\dd}$. ($V$ is the $\spann v^{\dd}$ of rows in $v^{\dd}$.) Since our
  % hyperplanes are positive kernels in $\posreal^{\mbbtpp}$, we modify this notion
  % to obtain a map $\pi_{\mpplus}: \posreal^{\mbbtpp} \rightarrow V_{\mpplus}$, where
  % $V_{\mpplus} = V\cap \posreal^{\mbbtpp}$. \twodiv\ then ensures that, for every
  % $x \neq y$ in $Y$, $H_{\mpplus}^{\{x,y\}}$ and
  % $H_{\mpplus}^{\{x,y\}} \defeq \pi_{\mpplus}(H_{\mpplus}^{\{x,y\}})$ are uniquely defined
  % hyperplanes in $\posreal^{\mbbtpp}$ and $V_{\mpplus}$ respectively. We will typically,
  % suppress reference to $V$ and take $H_{\mpplus}^{\xy}$ to denote a hyperplane in
  % $V_{\mpplus}$. This entails no loss of generality in so far as the structure of the
  % arrangement is concerned.
  % % \footnote{Consider the simplest possible arrangement consisting of
  %   % just one hyperplane in $\R^{n}$. In this case, the essentialization is a
  %   % central arrangement consisting of a single point at the origin and the ambient
  %   % space is some one-dimensional subspace of $\R^{n}$.}
  % \end{comment}
  
    % \begin{comment} \begin{remark}[Further notation on arrangements of
    % hyperplanes] To allow us to appeal to  the literature on arrangements of
    % hyperplanes, we recall some useful notation.  \begin{definition} Let $\mc A
    %$ be an arrangement (of hyperplanes) in a vector space $V$ and let $\mc
    % L = \mc L(\mc A)$ be the set of nonempty
    %   intersections of elements of $\mc A$.\footnote{The convention is to take
    %    $V = \bigcap \{H_{\mpplus} \in \mc A : H_{\mpplus} \in
    %   \emptyset \}$, so that $V\in \Lambda$.}  Define a partial order on $\mc L
    %  $ by reverse inclusion: \begin{equation*}\text{$B \leq B'$ if, and only if, $%   B'\subseteq B$.}\end{equation*}
    % \end{definition} Note that, for every arrangement $\mc A$, $V$ is the
    % unique minimal element of $\mc L ( \mc A )$.  $\mc A$ is a central
    % arrangement, if, and only, if $\mc L$ has a unique maximal element.  We
    % partition the proof into two parts, depending on whether there exists a
    % novel $Y$-generalization $\hext$ of
    % $\preceq_{\mbbj}$ that generates a central arrangement and that also
    % satisfies the property \begin{equation}\label{eq-maximal-ext} \text{for
    % every $Y$-generalization $\ext$,\, $\countof \total ( \ext ) \leq \countof
    % \total (\hext )$.} \end{equation} The proof is somewhat more involved in
    % the case where $\preceq_{\mbbj}$
    % has no $Y$-generalization $\hext$ that generates a central arrangement $\mc
    % A$ in addition to property \eqref{eq-maximal-ext}.  In both cases, the key
    % is to construct testworthy generalizations such that every member of $\mc L$
    % intersects the interior of $\conv( \mbbjp)$. In the absence of \fourdiv,
    % regular generalizations may fail to satisfy this property.  When
    % this property holds, we are able to appeal to Zaslavski's theorem, and thus
    % count the open and connected sets of an arrangement.  \end{remark}
    % \end{comment}https://aibe.uq.edu.au/article/2023/03/economics-of-green-hydrogen-in-australia-part-1
    %proof of thm-mainQ
\end{appendix}

%% or include bibliography directly:

% \begin{thebibliography}{}
% \bibitem{b1}
% \end{thebibliography}

\end{document}
