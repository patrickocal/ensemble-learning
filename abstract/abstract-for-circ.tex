%\documentclass{article}
%
%\begin{document}
%\title{Ensemble learning: \\ an axiomatic approach with applications to
%finance}
\begin{countabstract}
Both classical and deep ensemble learning succeed by combining weak learners to form strong ones.
We take sentiments (an ensemble of binary relations) as primitive and provide minimal requirements for the existence of a stable kernel representation.
A kernel is stable if it generalizes to novel (out-of-distribution) examples without the need to re-evaluate current weights.
In a multi-asset market, a pricing kernel is stable if, and only if, it is
everywhere arbitrage-free, so that a groupoid condition holds across assets for
every resampling.
Stable pricing kernels preserve information and support the efficient markets hypothesis in the sense that proportional price increments are submodular in the data.
US Treasury bond market data for 1961-2023 justifies our extension of the axiomatic framework of \citet{gilboa2003inductive} to the case of a partially diverse ensemble.
In general, partial diversity allows more data to be allocated for testing.
Our results shed light on the trade off between diversity and the extent to
which the ensemble generalises to novel settings.
\end{countabstract}
%data in the classical case and models in the deep case.


%
%We provide minimal diversity requirements for the existence of a stable kernel.
%
%\title{Axiomatic boosting: from weak classifiers to strong pricing kernels}
%\title{Axioms for boosting weak classifiers into stable pricing kernels}
%
%\begin{abstract}
%
%\title{Transferable Ensemble Learning: an axiomatic approach}
%Traditional machine learning assume the training and testing
%data are drawn from the same domain (and distribution).
%However, in many real-world scenarios, training data is expensive or hard to obtain.
%Learning is transferable when it generalises to new domains where the data may
%lack labels or be otherwise incomplete.
%%This gives rise to the need for learning that is (ex ante) transferable to novel domains where the data may be incomplete: labels or features may be missing in semi-supervised settings.
%It also gives rise to the need to proactively identify which novel examples are testworthy: worthy of human supervision.
%Where possible, we seek to preserve (avoid retraining) the current model when novel examples or classes arise: we seek to generalize the current model to new domains.
%Ensemble learning methods have a long history in both econometrics and machine
%learning are a standard feature of winning algorithms in machine learning competitions.
%The goal is to combine a collection of weak learners into a single strong one.
%Key conditions for successful ensemble learning are stability and diversity and the nature of the relationship between the two is an important open question.
%When stability holds, small changes in the data yield small changes in  predictions.
%When diversity holds, resampling past examples supports a diverse set
%of weak learners.
%That is, to the extent that it is possible, learning should be modular.
%
%Many successful machine learning algorithms harness ensembles of
%weak learners to form strong ones. Combining forecasters also has a rich
%tradition in financial market forecasting. The goal of ensemble learning is to
%combine weak learners to form strong ones. AdaBoost is perhaps the most famous
%yield small changes in predictions. Diversity on the other hand requires that
%members of the ensemble provide diverse predictions in response to the data.
%
%
%When the data fail to satisfy partial-4-diversity, 4-stability
% 
%requires out-of-sample validation: sentient agents may simulate or imagine the
%impact of novel data; artificial ones may engage in
%
%Our axioms, partial-3-diversity and 4-stability, are necessary and sufficient for the existence of a stable kernel.
%Partial-3-diversity is a considerable weakening of 4-diversity \citep{GS_Inductive-inference}.
%Partial-3-diversity weakens extant conditions whereas 4-stability extends
%order, linearity and continuity to novel data thus enabling
%out-of-distribution generalization 
%
%The axioms we impose on the ensemble are partial-3-diversity and 4-stability.
%
%
%
%
%  A stable learner is a prediction model that generalizes to new examples without re-evaluation of the current kernel.
%  We derive the current kernel on the basis of sentiments: an ensemble of weak
%  learners (a
%  mapping from training sets to rankings) for a broader class of stable learners.
%  This extension of Gilboa and Schmeidler (2003) is justified by US Treasury
%  bond market data for 1961-2023.  When the data is insufficiently diverse,
%  stability requires out-of-sample validation: sentient agents may simulate or
%  imagine the impact of novel data; artificial ones may engage in
%  leave-one-type-out cross-validation; in market settings, one may bypass
%  sentiments and directly rule out Dutch books. Our framework embeds current
%  sentiments in a system of potential generalizations to enable within-sample
%  testing of stability for any form of external validation.
%
  %
  %A stable learner is a prediction model that generalises to new samples
  %without re-evaluation of the current kernel.  In market settings, a learner
  %is stable if, and only if, its pricing kernel is arbitrage-free.  Stable
  %pricing kernels preserve information and support the efficient markets
  %hypothesis Our contribution is to derive the current kernel on the basis of
  %sentiments (a mapping from samples to rankings) for a broader class of
  %stable learners. This extension of Gilboa and Schmeidler (2003) is justified
  %by US Treasury bond market data for the period 1961-2023.  When the data is
  %insufficiently rich, stability requires out-of-sample validation: sentient
  %agents may simulate or imagine the impact of novel data; artificial ones may
  %engage in leave-one-type-out cross-validation; in market settings, one may
  %bypass sentiments and directly rule out Dutch books. Our framework embeds
  %current sentiments in a system of potential generalisations thus enabling
  %within-sample testing of stability for any form of external validation.
  %
  %since the marginal impact of the next observation is decreasing in the
  %relative frequency of its type.
  %
  %Our ``partial-3-diversity of data'' condition on daily rankings of annual
  %yields holds across 91\% of the sample for all 30 maturity dates. When
  %stronger diversity conditions fail,
  %
  %\end{abstract}
  %
  %\end{document}
